#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
enhanced_retriever.py - å¯åŠ¨æ—¶é¢„è®¡ç®—ç‰ˆæœ¬

ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼š
1. å¯åŠ¨æ—¶ä¸€æ¬¡æ€§é¢„è®¡ç®—æ‰€æœ‰æ–‡æ¡£embeddings
2. æŸ¥è¯¢æ—¶ç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼Œä¸å†å®æ—¶è®¡ç®—
3. æ”¯æŒå¢é‡æ›´æ–°ç¼“å­˜
"""

import os
import json
import logging
import math
import re
import time
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import numpy as np

# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# ä¾èµ–æ£€æŸ¥
# ============================================================================
try:
    from sentence_transformers import SentenceTransformer, util
    import torch
    HAVE_SEMANTIC = True
    logger.info("âœ… sentence-transformers åŠ è½½æˆåŠŸ")
except Exception as e:
    HAVE_SEMANTIC = False
    logger.warning(f"âš ï¸ sentence-transformers ä¸å¯ç”¨: {e}")

try:
    import jieba
    HAVE_JIEBA = True
    logger.info("âœ… jieba åŠ è½½æˆåŠŸ")
except Exception:
    HAVE_JIEBA = False
    logger.warning("âš ï¸ jieba ä¸å¯ç”¨ï¼Œä½¿ç”¨æ­£åˆ™åˆ†è¯")

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
class ScoringConfig:
    """è¯„åˆ†æƒé‡é…ç½®"""
    KEYWORD_IN_TITLE = 14
    EXACT_QUERY_MATCH = 36
    SYNONYM_MATCH = 28
    PROGRAM_PREFIX_MATCH = 60
    PROGRAM_CONTAINS_MATCH = 32
    FUZZY_MATCH_MULTIPLIER = 24
    DEGREE_TAG_MATCH = 12
    INTENT_HEADING_MATCH = 20
    KEYWORD_IN_HEADING = 10
    KEYWORD_IN_TEXT_PER_OCCURRENCE = 3
    MAX_KEYWORD_TEXT_SCORE = 18
    LIST_STRUCTURE_BONUS = 15
    COURSE_CODE_BONUS = 10
    LEVEL_BOOST = 12
    LEVEL_PENALTY = -8
    SEMANTIC_MULTIPLIER = 40
    SECTION_LENGTH_NORMALIZATION_BASE = 150


class CommonPhrases:
    """å¸¸è§æŸ¥è¯¢çŸ­è¯­åŠå…¶æ‰©å±•"""
    PHRASES = {
        "language requirements": ["entry requirements", "english language", "ielts", "toefl", "language proficiency"],
        "core modules": ["compulsory modules", "core courses", "required modules", "mandatory"],
        "tuition fees": ["fees", "cost", "tuition", "funding", "scholarship"],
        "entry requirements": ["admission", "qualification", "prerequisite", "grade"],
        "career prospects": ["employment", "graduate outcomes", "career", "job"],
    }


# ============================================================================
# ä¸»ç±» - å¯åŠ¨æ—¶é¢„è®¡ç®—ç‰ˆæœ¬
# ============================================================================
class EnhancedRetriever:
    """å¢å¼ºç‰ˆæ£€ç´¢å™¨ - å¯åŠ¨æ—¶é¢„è®¡ç®—ç‰ˆæœ¬"""

    def __init__(self, enable_semantic: bool = True, preload_documents: Optional[List[Dict]] = None) -> None:
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            enable_semantic: æ˜¯å¦å¯ç”¨è¯­ä¹‰æœç´¢
            preload_documents: å¯åŠ¨æ—¶é¢„åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¦‚æœæä¾›ï¼Œä¼šç«‹å³é¢„è®¡ç®—ï¼‰
        """
        self.enable_semantic = enable_semantic and HAVE_SEMANTIC
        
        # åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹
        self.semantic_model = None
        if self.enable_semantic:
            self._load_semantic_model()
        
        # ç¼“å­˜ï¼ˆæ”¹ä¸ºæŒä¹…åŒ–å­˜å‚¨ï¼‰
        self._doc_embeddings_cache: Dict[str, np.ndarray] = {}  # key: doc_url, value: embedding
        self._query_embedding_cache: Dict[str, np.ndarray] = {}
        
        # æ–‡æ¡£ç´¢å¼•ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
        self._doc_index: Dict[str, Dict] = {}  # key: doc_url, value: doc
        
        # é¢†åŸŸè¯æ±‡
        self.domain_vocab = self._build_domain_vocab()
        
        logger.info(f"ğŸš€ EnhancedRetriever åˆå§‹åŒ–å®Œæˆ (semantic={self.enable_semantic})")
        
        # ğŸ”¥ å¯åŠ¨æ—¶é¢„è®¡ç®—
        if preload_documents:
            self.precompute_all_embeddings(preload_documents)

    def _load_semantic_model(self) -> None:
        """åŠ è½½è¯­ä¹‰æ¨¡å‹"""
        if not HAVE_SEMANTIC:
            logger.warning("âš ï¸ sentence-transformers æœªå®‰è£…")
            self.enable_semantic = False
            return

        try:
            logger.info("ğŸ“¥ æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡å‹...")
            self.semantic_model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
            # é¢„çƒ­æ¨¡å‹
            _ = self.semantic_model.encode("warmup", convert_to_tensor=False, show_progress_bar=False)
            logger.info("âœ… è¯­ä¹‰æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as exc:
            logger.error(f"âŒ è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {exc}")
            self.enable_semantic = False
            self.semantic_model = None

    def precompute_all_embeddings(self, documents: List[Dict], batch_size: int = 32) -> None:
        """
        ğŸ”¥ å¯åŠ¨æ—¶ä¸€æ¬¡æ€§é¢„è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„ embeddings
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        if not self.enable_semantic or self.semantic_model is None:
            logger.warning("âš ï¸ è¯­ä¹‰æœç´¢æœªå¯ç”¨ï¼Œè·³è¿‡é¢„è®¡ç®—")
            return
        
        start_time = time.time()
        logger.info(f"ğŸ“Š å¼€å§‹é¢„è®¡ç®— {len(documents)} ä¸ªæ–‡æ¡£çš„ embeddings...")
        
        # å‡†å¤‡æ–‡æœ¬å’Œå¯¹åº”çš„ URL
        texts = []
        urls = []
        
        for doc in documents:
            if not doc or not isinstance(doc, dict):
                continue
            
            url = doc.get("url", "") or doc.get("title", "")
            if not url:
                continue
            
            # æå–æ–‡æ¡£æ–‡æœ¬
            doc_text = self._extract_doc_text(doc)
            if doc_text:
                texts.append(doc_text)
                urls.append(url)
                self._doc_index[url] = doc
        
        if not texts:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£æ–‡æœ¬ï¼Œè·³è¿‡é¢„è®¡ç®—")
            return
        
        # ğŸ”¥ æ‰¹é‡è®¡ç®— embeddingsï¼ˆå¤§å¹…æé€Ÿï¼‰
        logger.info(f"ğŸš€ æ‰¹é‡è®¡ç®— embeddings (batch_size={batch_size})...")
        try:
            # ä½¿ç”¨ show_progress_bar=True æ˜¾ç¤ºè¿›åº¦
            embeddings = self.semantic_model.encode(
                texts,
                batch_size=batch_size,
                convert_to_numpy=True,  # ç›´æ¥è½¬ä¸º numpy
                show_progress_bar=True,
                normalize_embeddings=True  # L2 å½’ä¸€åŒ–ï¼ŒåŠ é€Ÿç›¸ä¼¼åº¦è®¡ç®—
            )
            
            # å­˜å…¥ç¼“å­˜
            for url, emb in zip(urls, embeddings):
                self._doc_embeddings_cache[url] = emb
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… é¢„è®¡ç®—å®Œæˆ: {len(self._doc_embeddings_cache)} ä¸ªæ–‡æ¡£ï¼Œè€—æ—¶ {elapsed:.2f}s")
            logger.info(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {len(texts) / elapsed:.1f} docs/s")
        
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡é¢„è®¡ç®—å¤±è´¥: {e}", exc_info=True)

    def search_with_context(
        self, 
        query: str, 
        documents: List[Dict], 
        top_k: int = 5,
        intent: str = "general"
    ) -> List[Dict]:
        """
        ä¸»æœç´¢å…¥å£ - ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„ embeddings
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆé€šå¸¸ä¸éœ€è¦ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼‰
            top_k: è¿”å›ç»“æœæ•°é‡
            intent: æŸ¥è¯¢æ„å›¾
        
        Returns:
            æ’åºåçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ” å¼€å§‹æœç´¢: query='{query}', top_k={top_k}, cached_docs={len(self._doc_embeddings_cache)}")
        
        results = []
        
        # ğŸ”¥ ä¼˜å…ˆå°è¯•è¯­ä¹‰æœç´¢ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„ embeddingsï¼‰
        if self.enable_semantic and self.semantic_model is not None and self._doc_embeddings_cache:
            try:
                results = self._semantic_search_cached(query, top_k)
                if results:
                    logger.info(f"âœ… è¯­ä¹‰æœç´¢æˆåŠŸ: æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                    return results
                else:
                    logger.warning("âš ï¸ è¯­ä¹‰æœç´¢è¿”å›ç©ºç»“æœ")
            except Exception as e:
                logger.error(f"âŒ è¯­ä¹‰æœç´¢å¼‚å¸¸: {e}", exc_info=True)
        
        # ğŸ”¥ é™çº§åˆ°å…³é”®è¯æ£€ç´¢
        logger.info("ğŸ”„ é™çº§åˆ°å…³é”®è¯æ£€ç´¢...")
        # å¦‚æœ documents ä¸ºç©ºï¼Œä½¿ç”¨ç¼“å­˜çš„æ–‡æ¡£
        docs_to_search = documents if documents else list(self._doc_index.values())
        results = self._keyword_search(query, docs_to_search, top_k, intent)
        logger.info(f"âœ… å…³é”®è¯æœç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        
        return results

    def _semantic_search_cached(self, query: str, top_k: int) -> List[Dict]:
        """
        ğŸ”¥ ä½¿ç”¨é¢„è®¡ç®—çš„ embeddings è¿›è¡Œè¯­ä¹‰æœç´¢ï¼ˆæé€Ÿç‰ˆï¼‰
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self._doc_embeddings_cache:
            logger.warning("âš ï¸ ç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œè¯­ä¹‰æœç´¢")
            return []
        
        try:
            logger.info("âš¡ æ‰§è¡Œç¼“å­˜è¯­ä¹‰æœç´¢...")
            
            # 1. ç¼–ç æŸ¥è¯¢ï¼ˆå¸¦ç¼“å­˜ï¼‰
            query_embedding = self._encode_query_cached(query)
            if query_embedding is None:
                logger.error("âŒ æŸ¥è¯¢ç¼–ç å¤±è´¥")
                return []
            
            # 2. è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–è®¡ç®—ï¼Œæå¿«ï¼‰
            urls = list(self._doc_embeddings_cache.keys())
            doc_embeddings = np.array([self._doc_embeddings_cache[url] for url in urls])
            
            # ğŸ”¥ æ‰¹é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå·²å½’ä¸€åŒ–ï¼Œç›´æ¥ç‚¹ç§¯ï¼‰
            similarities = np.dot(doc_embeddings, query_embedding)
            
            # 3. è·å– top_k
            top_indices = np.argsort(similarities)[::-1][:top_k * 2]  # å– 2 å€ï¼Œè¿‡æ»¤åå†æˆªæ–­
            
            results = []
            for idx in top_indices:
                sim = float(similarities[idx])
                if sim < 0.1:  # è¿‡æ»¤ä½åˆ†
                    continue
                
                url = urls[idx]
                doc = self._doc_index.get(url)
                if not doc:
                    continue
                
                results.append({
                    'doc': doc,
                    'score': sim * 100,  # å½’ä¸€åŒ–åˆ° 0-100
                    'matched_sections': self._find_relevant_sections(doc, query)
                })
            
            logger.info(f"âœ… ç¼“å­˜è¯­ä¹‰æœç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            return results[:top_k]
        
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜è¯­ä¹‰æœç´¢å¤±è´¥: {e}", exc_info=True)
            return []

    def _encode_query_cached(self, query: str) -> Optional[np.ndarray]:
        """ç¼–ç æŸ¥è¯¢ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if query in self._query_embedding_cache:
            return self._query_embedding_cache[query]
        
        try:
            # ç›´æ¥è¿”å› numpyï¼Œä¸” L2 å½’ä¸€åŒ–
            embedding = self.semantic_model.encode(
                query,
                convert_to_numpy=True,
                normalize_embeddings=True,
                show_progress_bar=False
            )
            
            self._query_embedding_cache[query] = embedding
            return embedding
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢ç¼–ç å¤±è´¥: {e}")
            return None

    def _extract_doc_text(self, doc: Dict) -> str:
        """æå–æ–‡æ¡£æ–‡æœ¬ç”¨äº embedding"""
        text_parts = []
        
        # æ ‡é¢˜ï¼ˆæƒé‡æ›´é«˜ï¼Œé‡å¤3æ¬¡ï¼‰
        title = doc.get("title", "")
        if title:
            text_parts.append(f"{title}. {title}. {title}.")
        
        # Level/Degree
        level = doc.get("level", "")
        if level:
            text_parts.append(f"Level: {level}.")
        
        # Sectionsï¼ˆåªå–å‰ 10 ä¸ªï¼Œæ¯ä¸ªæˆªæ–­åˆ° 200 å­—ç¬¦ï¼‰
        sections = doc.get("sections", [])
        if sections and isinstance(sections, list):
            for section in sections[:10]:
                if not isinstance(section, dict):
                    continue
                heading = section.get("heading", "")
                text = section.get("text", "")
                if heading:
                    text_parts.append(f"{heading}: {text[:200]}")
                elif text:
                    text_parts.append(text[:200])
        
        return " ".join(text_parts)

    def _find_relevant_sections(self, doc: Dict, query: str) -> List[Dict]:
        """æ‰¾åˆ°ç›¸å…³çš„ sections"""
        relevant = []
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        sections = doc.get("sections", [])
        if not sections or not isinstance(sections, list):
            return []
        
        for section in sections[:20]:
            if not isinstance(section, dict):
                continue
            
            heading = section.get("heading", "").lower()
            text = section.get("text", "").lower()
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            matches = sum(1 for word in query_words if len(word) > 2 and (word in heading or word in text))
            
            if matches > 0:
                relevant.append({
                    "heading": section.get("heading", ""),
                    "text": section.get("text", "")[:500],
                    "score": matches
                })
        
        relevant.sort(key=lambda x: x["score"], reverse=True)
        return relevant[:3]

    def _keyword_search(self, query: str, documents: List[Dict], top_k: int, intent: str) -> List[Dict]:
        """å…³é”®è¯æ£€ç´¢ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        logger.info("ğŸ” æ‰§è¡Œå…³é”®è¯æ£€ç´¢...")
        
        keywords = self._extract_keywords(query)
        logger.info(f"ğŸ“ æå–å…³é”®è¯: {keywords[:10]}")
        
        if not keywords:
            logger.warning("âš ï¸ æœªæå–åˆ°æœ‰æ•ˆå…³é”®è¯")
            return []
        
        scored_docs = []
        for doc in documents:
            if not doc or not isinstance(doc, dict):
                continue
            
            score_result = self._score_document(doc, keywords, intent)
            if score_result["score"] > 0:
                scored_docs.append({
                    "doc": doc,
                    "score": score_result["score"],
                    "matched_sections": score_result.get("matched_sections", [])
                })
        
        scored_docs.sort(key=lambda x: x["score"], reverse=True)
        return scored_docs[:top_k]

    def _extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        stopwords = {
            "what", "how", "where", "when", "which", "who", "the", "a", "an",
            "is", "are", "was", "were", "do", "does", "did", "about", "for",
            "çš„", "æ˜¯", "æœ‰", "åœ¨", "å—", "å‘¢", "å•Š", "äº†",
        }
        
        try:
            query_lower = query.lower()
            
            # è‹±æ–‡å…³é”®è¯
            english_words = re.findall(r"\b[a-z]+\b", query_lower)
            english_keywords = [w for w in english_words if w not in stopwords and len(w) > 2]
            
            # ä¸­æ–‡å…³é”®è¯
            chinese_keywords = []
            chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
            for chunk in chinese_matches:
                if HAVE_JIEBA:
                    chinese_keywords.extend([
                        token for token in jieba.cut(chunk) 
                        if len(token) > 1 and token not in stopwords
                    ])
                else:
                    for i in range(len(chunk)):
                        for j in range(2, 4):
                            if i + j <= len(chunk):
                                chinese_keywords.append(chunk[i:i+j])
            
            keywords = english_keywords + chinese_keywords
            
            # æ‰©å±•å¸¸è§çŸ­è¯­
            for phrase, expansions in CommonPhrases.PHRASES.items():
                if phrase in query_lower:
                    keywords.extend(expansions)
            
            # å»é‡
            seen = set()
            unique_keywords = []
            for kw in keywords:
                if kw and kw not in seen:
                    unique_keywords.append(kw)
                    seen.add(kw)
            
            return unique_keywords
        
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
            return []

    def _score_document(self, doc: Dict, keywords: List[str], intent: str) -> Dict[str, Any]:
        """ä¸ºæ–‡æ¡£æ‰“åˆ†"""
        score = 0.0
        matched_sections = []
        
        # æ ‡é¢˜åŒ¹é…
        title = (doc.get("title") or "").lower()
        title_hits = 0
        for kw in keywords:
            if kw and kw.lower() in title:
                score += ScoringConfig.KEYWORD_IN_TITLE
                title_hits += 1
        
        # Level åŒ¹é…
        level = str(doc.get("level", "")).lower()
        if any(x in level for x in ["msc", "master", "postgraduate"]):
            score += ScoringConfig.LEVEL_BOOST
        
        # Section åŒ¹é…
        sections = doc.get("sections", [])
        if sections and isinstance(sections, list):
            intent_headings = {
                "modules": ["module", "curriculum", "syllabus", "compulsory", "optional"],
                "requirements": ["entry", "requirement", "admission", "qualification"],
                "fees": ["fee", "tuition", "cost", "scholarship"],
                "career": ["career", "employment", "prospect"],
                "services": ["service", "support", "counseling"],
            }.get(intent, [])
            
            for section in sections[:30]:
                if not isinstance(section, dict):
                    continue
                
                heading = (section.get("heading") or "").lower()
                text = (section.get("text") or "").lower()
                
                sec_score = 0.0
                
                if any(h in heading for h in intent_headings):
                    sec_score += ScoringConfig.INTENT_HEADING_MATCH
                
                matches = 0
                for kw in keywords[:20]:
                    if kw and (kw.lower() in heading or kw.lower() in text):
                        matches += 1
                
                sec_score += min(matches * ScoringConfig.KEYWORD_IN_HEADING, ScoringConfig.MAX_KEYWORD_TEXT_SCORE)
                
                if sec_score > 0:
                    matched_sections.append({
                        "heading": section.get("heading", ""),
                        "text": section.get("text", "")[:500],
                        "score": sec_score
                    })
                    score += sec_score
        
        matched_sections.sort(key=lambda x: x["score"], reverse=True)
        
        return {
            "score": score,
            "title_hits": title_hits,
            "matched_sections": matched_sections[:5]
        }

    def _build_domain_vocab(self) -> set:
        """æ„å»ºé¢†åŸŸè¯æ±‡è¡¨"""
        try:
            repo_root = Path(__file__).resolve().parents[1]
            data_paths = [
                repo_root / "public" / "data" / "ucl_programs.json",
                repo_root / "public" / "data" / "ucl_services.json",
            ]
            
            words = []
            for path in data_paths:
                if not path.exists():
                    continue
                try:
                    with path.open("r", encoding="utf-8") as fh:
                        data = json.load(fh)
                        if not data or not isinstance(data, list):
                            continue
                        for item in data:
                            if not item or not isinstance(item, dict):
                                continue
                            title = item.get("title", "")
                            if title and isinstance(title, str):
                                tokens = re.findall(r"\b\w+\b|[\u4e00-\u9fff]+", title.lower())
                                words.extend(tokens)
                except Exception as e:
                    logger.debug(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            
            counter = Counter(words)
            vocab = {token for token, freq in counter.items() if freq > 1 and len(token) > 2}
            
            logger.info(f"ğŸ“š é¢†åŸŸè¯æ±‡è¡¨æ„å»ºå®Œæˆ: {len(vocab)} ä¸ªè¯")
            return vocab
        
        except Exception as e:
            logger.warning(f"âš ï¸ é¢†åŸŸè¯æ±‡è¡¨æ„å»ºå¤±è´¥: {e}")
            return set()


# ============================================================================
# å·¥å‚å‡½æ•°
# ============================================================================
def create_retriever(enable_semantic: bool = True, preload_documents: Optional[List[Dict]] = None) -> EnhancedRetriever:
    """åˆ›å»ºæ£€ç´¢å™¨å®ä¾‹"""
    return EnhancedRetriever(enable_semantic=enable_semantic, preload_documents=preload_documents)