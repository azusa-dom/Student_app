"""å¢å¼ºç‰ˆæ£€ç´¢å™¨ - ä¿®å¤ç©ºå€¼é”™è¯¯ç‰ˆæœ¬

ä¸»è¦ä¿®å¤:
1. æ‰€æœ‰ç©ºå€¼æ£€æŸ¥å’Œå®¹é”™å¤„ç†
2. é˜²æ­¢ NoneType è¿­ä»£é”™è¯¯
3. å®Œå–„çš„å¼‚å¸¸å¤„ç†
"""

from __future__ import annotations

import json
import logging
import math
import re
from collections import Counter
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)

# ============================================================================
# ä¾èµ–æ£€æŸ¥
# ============================================================================
try:
    from sentence_transformers import SentenceTransformer, util  # type: ignore
    import torch  # type: ignore

    HAVE_SEMANTIC = True
    logger.info("âœ… sentence-transformers loaded for semantic search")
except Exception:
    HAVE_SEMANTIC = False
    logger.warning("âš ï¸ sentence-transformers not available, semantic search disabled")

try:
    import jieba  # type: ignore

    HAVE_JIEBA = True
    logger.info("âœ… jieba loaded for Chinese tokenization")
except Exception:
    HAVE_JIEBA = False
    logger.warning("âš ï¸ jieba not available, fallback to regex for Chinese")


# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
class ScoringConfig:
    """è¯„åˆ†æƒé‡é…ç½®"""
    KEYWORD_IN_TITLE = 14
    EXACT_QUERY_MATCH = 36
    SYNONYM_MATCH = 28
    PROGRAM_PREFIX_MATCH = 60
    PROGRAM_CONTAINS_MATCH = 32
    FUZZY_MATCH_MULTIPLIER = 24
    DEGREE_TAG_MATCH = 12
    INTENT_HEADING_MATCH = 20
    KEYWORD_IN_HEADING = 10
    KEYWORD_IN_TEXT_PER_OCCURRENCE = 3
    MAX_KEYWORD_TEXT_SCORE = 18
    LIST_STRUCTURE_BONUS = 15
    COURSE_CODE_BONUS = 10
    LEVEL_BOOST = 12
    LEVEL_PENALTY = -8
    SEMANTIC_MULTIPLIER = 40
    SECTION_LENGTH_NORMALIZATION_BASE = 150


class CommonPhrases:
    """å¸¸è§æŸ¥è¯¢çŸ­è¯­åŠå…¶æ‰©å±•"""
    PHRASES = {
        "language requirements": ["entry requirements", "english language", "ielts", "toefl", "language proficiency"],
        "core modules": ["compulsory modules", "core courses", "required modules", "mandatory"],
        "tuition fees": ["fees", "cost", "tuition", "funding", "scholarship"],
        "entry requirements": ["admission", "qualification", "prerequisite", "grade"],
        "career prospects": ["employment", "graduate outcomes", "career", "job"],
    }


# ============================================================================
# ä¸»ç±»
# ============================================================================

class EnhancedRetriever:
    """å¢å¼ºç‰ˆæ£€ç´¢å™¨ - å®Œå…¨ä¿®å¤ç©ºå€¼å¤„ç†"""

    def __init__(self, enable_semantic: bool = True, cache_embeddings: bool = True) -> None:
        self.enable_semantic = enable_semantic and HAVE_SEMANTIC
        self.cache_embeddings = cache_embeddings

        # ğŸ”¥ ç¡®ä¿æ‰€æœ‰å…³é”®è¯åˆ—è¡¨éƒ½ä¸ä¸º None
        self.intent_keywords: Dict[str, List[str]] = {
            "modules": ["module", "course", "subject", "curriculum", "syllabus", "teaching", "learn", "è¯¾ç¨‹", "æ¨¡å—"],
            "requirements": ["requirement", "prerequisite", "entry", "admission", "qualification", "ielts", "toefl", "è¦æ±‚", "ç”³è¯·"],
            "career": ["career", "job", "employment", "graduate", "prospect", "å°±ä¸š", "èŒä¸š"],
            "fees": ["fee", "tuition", "cost", "funding", "scholarship", "å­¦è´¹", "è´¹ç”¨"],
        }

        self.intent_headings: Dict[str, List[str]] = {
            "modules": [
                "module", "curriculum", "syllabus", "teaching", "what you will learn",
                "course structure", "compulsory", "optional", "placement", "supervisor"
            ],
            "requirements": ["entry", "requirement", "admission", "english", "qualification", "ielts", "toefl", "language"],
            "career": ["career", "employment", "graduate", "prospects", "outcomes"],
            "fees": ["fee", "tuition", "cost", "funding", "scholarship"],
        }

        self.custom_synonyms: Dict[str, List[str]] = {
            "data science": ["data science", "æ•°æ®ç§‘å­¦", "analytics", "data"],
            "computer science": ["computer science", "computing", "computer"],
            "business analytics": ["business analytics", "å•†ä¸šåˆ†æ"],
            "management": ["management", "ç®¡ç†", "business"],
        }

        self.domain_vocab = self._build_domain_vocab()
        self.semantic_model: Optional[Any] = None
        self._section_embeddings_cache: Dict[int, Dict[int, Any]] = {}

        if self.enable_semantic:
            self._load_semantic_model()

    def detect_intent(self, query: str) -> str:
        """ğŸ”¥ æ£€æµ‹æŸ¥è¯¢æ„å›¾ - å®Œå…¨é˜²æŠ¤ç‰ˆæœ¬"""
        if not query:
            return "general"
        
        try:
            query_lower = query.lower()
        except (AttributeError, TypeError):
            return "general"
        
        for intent, keywords in self.intent_keywords.items():
            # ğŸ”¥ å¤šé‡ç©ºå€¼æ£€æŸ¥
            if not keywords:
                continue
            try:
                # ç¡®ä¿ keywords æ˜¯å¯è¿­ä»£çš„ä¸”æ¯ä¸ª kw éƒ½æ˜¯å­—ç¬¦ä¸²
                for kw in keywords:
                    if kw and isinstance(kw, str) and kw in query_lower:
                        return intent
            except (TypeError, AttributeError):
                continue
        
        return "general"

    def _detect_intent(self, query: str, keywords: List[str]) -> str:
        """ğŸ”¥ æ£€æµ‹æŸ¥è¯¢æ„å›¾ï¼ˆå¸¦å…³é”®è¯åˆ—è¡¨ï¼‰- å®Œå…¨é˜²æŠ¤ç‰ˆæœ¬"""
        if not query:
            return "general"
        
        try:
            query_lower = query.lower()
        except (AttributeError, TypeError):
            return "general"
        
        # ğŸ”¥ ç¡®ä¿ keywords ä¸ä¸º None
        if keywords is None:
            keywords = []
        
        # æ„å›¾åŒ¹é…è§„åˆ™
        intent_patterns = {
            "modules": [
                "module", "course", "curriculum", "subject", "teaching",
                "æ¨¡å—", "è¯¾ç¨‹", "ç§‘ç›®", "æ•™å­¦"
            ],
            "requirements": [
                "requirement", "entry", "admission", "prerequisite", "qualify",
                "è¦æ±‚", "å…¥å­¦", "æ¡ä»¶", "èµ„æ ¼", "ç”³è¯·"
            ],
            "fees": [
                "fee", "tuition", "cost", "price", "funding", "scholarship",
                "å­¦è´¹", "è´¹ç”¨", "ä»·æ ¼", "å¥–å­¦é‡‘", "èµ„åŠ©"
            ],
            "career": [
                "career", "job", "employment", "graduate", "outcome",
                "èŒä¸š", "å°±ä¸š", "å·¥ä½œ", "å‰æ™¯", "å‘å±•"
            ],
            "services": [
                "service", "support", "help", "counseling", "wellbeing",
                "æœåŠ¡", "æ”¯æŒ", "å¸®åŠ©", "å’¨è¯¢", "è¾…å¯¼"
            ]
        }
        
        # æ£€æµ‹æ„å›¾
        for intent, patterns in intent_patterns.items():
            if not patterns:
                continue
            try:
                for pattern in patterns:
                    if not pattern or not isinstance(pattern, str):
                        continue
                    # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«æ¨¡å¼
                    if pattern in query_lower:
                        return intent
                    # æ£€æŸ¥å…³é”®è¯åˆ—è¡¨
                    if keywords:
                        for kw in keywords:
                            if kw and isinstance(kw, str) and pattern in kw:
                                return intent
            except (TypeError, AttributeError):
                continue
        
        return "general"

    def search_with_context(self, query: str, documents: List[dict], top_k: int = 8) -> List[Dict]:
        """ğŸ”¥ ä¿®å¤åçš„ä¸»æ£€ç´¢æ–¹æ³• - å®Œå…¨é˜²æŠ¤ç‰ˆæœ¬"""
        
        # ğŸ”¥ ç©ºå€¼æ£€æŸ¥
        if not query:
            logger.warning("âš ï¸  Empty query")
            return []
        
        if not documents:
            logger.warning("âš ï¸  Empty documents")
            return []

        try:
            # ğŸ”¥ æå–å…³é”®è¯æ—¶æ·»åŠ ç©ºå€¼æ£€æŸ¥
            keywords = self._extract_smart_keywords(query)
            if not keywords:
                logger.warning("âš ï¸  No keywords extracted, using simple split")
                keywords = [w for w in query.lower().split() if len(w) > 2]
        except Exception as e:
            logger.error(f"âŒ Keyword extraction failed: {e}")
            keywords = [w for w in query.lower().split() if len(w) > 2]

        # æ£€æµ‹æ„å›¾
        try:
            intent = self._detect_intent(query, keywords)
        except Exception as e:
            logger.error(f"âŒ Intent detection failed: {e}")
            intent = "general"
        
        query_lower = query.lower()

        logger.info(f"ğŸ” [æ£€ç´¢] æŸ¥è¯¢: {query[:100]}")
        logger.info(f"ğŸ¯ [æ£€ç´¢] æ„å›¾: {intent}")
        logger.info(f"ğŸ”‘ [æ£€ç´¢] å…³é”®è¯: {keywords[:10]}")

        # é¢„è®¡ç®— embeddings
        query_embedding = None
        if self.enable_semantic:
            try:
                query_embedding = self._encode_query(query)
            except Exception as e:
                logger.warning(f"âš ï¸  Query encoding failed: {e}")

        if self.enable_semantic and self.cache_embeddings and not self._section_embeddings_cache:
            try:
                logger.info("ğŸ”„ [ç¼“å­˜] é¢„è®¡ç®—æ–‡æ¡£ embeddings...")
                self._precompute_embeddings(documents)
            except Exception as e:
                logger.warning(f"âš ï¸  Embedding precompute failed: {e}")

        results: List[Dict] = []

        for doc_idx, doc in enumerate(documents):
            try:
                # ğŸ”¥ æ·»åŠ ç©ºå€¼æ£€æŸ¥
                title_raw = doc.get("title")
                if not title_raw or not isinstance(title_raw, str):
                    continue
                
                title_lower = title_raw.lower()

                # æ ‡é¢˜è¯„åˆ†
                title_score, title_hits = self._score_title_similarity(title_lower, keywords, query_lower)
                
                # å­¦ä½çº§åˆ«è¯„åˆ†
                level_score = self._score_level(doc, query_lower)
                
                # ç« èŠ‚è¯„åˆ†
                sections = doc.get("sections")
                if not sections or not isinstance(sections, list):
                    sections = []
                
                sections_score, matched_sections = self._score_sections(sections, keywords, intent)

                # ğŸ”¥ è¯­ä¹‰è¯„åˆ†æ—¶æ·»åŠ ç©ºå€¼æ£€æŸ¥
                semantic_score = 0.0
                if self.enable_semantic and query_embedding is not None:
                    try:
                        semantic_score = self._semantic_boost(doc_idx, sections, query_embedding)
                    except Exception as e:
                        logger.debug(f"âš ï¸  Semantic scoring failed for doc {doc_idx}: {e}")

                base_score = title_score + level_score + sections_score + semantic_score

                # è¿‡æ»¤ä½åˆ†ç»“æœ
                if base_score <= 0 or (not matched_sections and title_score < 25):
                    continue

                results.append({
                    "doc": doc,
                    "score": base_score,
                    "title_score": title_score,
                    "sections_score": sections_score,
                    "semantic_score": semantic_score,
                    "level_score": level_score,
                    "matched_sections": matched_sections,
                    "title_matches": title_hits,
                    "intent": intent,
                })
            except Exception as e:
                logger.debug(f"âš ï¸  Error processing doc {doc_idx}: {e}")
                continue

        # æ’åº
        results.sort(key=lambda item: item["score"], reverse=True)

        logger.info(f"ğŸ“Š [æ£€ç´¢] æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè¿”å› top {top_k}")

        return results[:top_k]

    def _score_level(self, doc: dict, query_lower: str) -> float:
        """è¯„åˆ†ï¼šå­¦ä½çº§åˆ«åŒ¹é…"""
        if not doc or not isinstance(doc, dict):
            return 0.0
        
        level = str(doc.get("level", "")).lower()
        if not level:
            return 0.0

        score = 0.0
        postgraduate_markers = ("postgraduate", "msc", "master", "mres", "ma", "phd")
        undergraduate_markers = ("undergraduate", "ba", "bsc")

        try:
            if any(marker in query_lower for marker in ("msc", "master", "postgraduate", "graduate", "ç¡•å£«", "ç ”ç©¶ç”Ÿ")):
                if any(marker in level for marker in postgraduate_markers):
                    score += ScoringConfig.LEVEL_BOOST
                if any(marker in level for marker in undergraduate_markers):
                    score += ScoringConfig.LEVEL_PENALTY

            if any(marker in query_lower for marker in ("bsc", "ba", "undergraduate", "bachelor", "æœ¬ç§‘")):
                if any(marker in level for marker in undergraduate_markers):
                    score += ScoringConfig.LEVEL_BOOST
                if any(marker in level for marker in postgraduate_markers):
                    score += ScoringConfig.LEVEL_PENALTY
        except (TypeError, AttributeError):
            pass

        return score

    def _score_title_similarity(self, title_lower: str, keywords: List[str], query_lower: str) -> Tuple[float, List[str]]:
        """è¯„åˆ†ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦"""
        score = 0.0
        matches: List[str] = []

        if not title_lower or not isinstance(title_lower, str):
            return score, matches
        
        if not keywords:
            keywords = []

        try:
            main_title = title_lower.split("|")[0].strip()

            # å…³é”®è¯åŒ¹é…
            for kw in keywords:
                if not kw or not isinstance(kw, str) or len(kw) <= 2:
                    continue
                if kw in title_lower:
                    score += ScoringConfig.KEYWORD_IN_TITLE
                    if kw not in matches:
                        matches.append(kw)

            # å®Œå…¨æŸ¥è¯¢åŒ¹é…
            normalized_query = re.sub(r"\s+", " ", query_lower.strip())
            if normalized_query and normalized_query in title_lower:
                score += ScoringConfig.EXACT_QUERY_MATCH

            # åŒä¹‰è¯åŒ¹é…
            for phrase, synonyms in self.custom_synonyms.items():
                if not synonyms:
                    continue
                if any(syn and syn in query_lower for syn in synonyms) and phrase in title_lower:
                    score += ScoringConfig.SYNONYM_MATCH
                    if phrase not in matches:
                        matches.append(phrase)

            # æ¨¡ç³ŠåŒ¹é…
            ratio = SequenceMatcher(None, query_lower, title_lower).ratio()
            if ratio > 0.42:
                score += ratio * ScoringConfig.FUZZY_MATCH_MULTIPLIER

            # å­¦ä½æ ‡ç­¾åŒ¹é…
            for tag in ("msc", "mres", "ma", "pgdip", "bsc", "ba", "phd"):
                if tag in query_lower and tag in title_lower:
                    score += ScoringConfig.DEGREE_TAG_MATCH
                    if tag not in matches:
                        matches.append(tag)

        except Exception as e:
            logger.debug(f"âš ï¸  Title scoring error: {e}")

        return score, matches

    def _score_sections(self, sections: List[dict], keywords: List[str], intent: str) -> Tuple[float, List[dict]]:
        """è¯„åˆ†ï¼šç« èŠ‚å†…å®¹"""
        
        # ğŸ”¥ æ·»åŠ ç©ºå€¼æ£€æŸ¥
        if not sections or not isinstance(sections, list):
            return 0.0, []
        
        if not keywords:
            keywords = []
        
        score = 0.0
        relevant_sections: List[dict] = []
        target_headings = self.intent_headings.get(intent, [])

        for section in sections:
            if not section or not isinstance(section, dict):
                continue
            
            try:
                heading = section.get("heading") or ""
                text = section.get("text") or ""
                
                if not isinstance(heading, str):
                    heading = ""
                if not isinstance(text, str):
                    text = ""
                
                if not heading and not text:
                    continue

                heading_lower = heading.lower()
                text_lower = text.lower()
                section_score = 0.0
                reasons: List[str] = []

                # æ„å›¾ç›¸å…³æ ‡é¢˜åŒ¹é…
                for th in target_headings:
                    if th and isinstance(th, str) and th in heading_lower:
                        section_score += ScoringConfig.INTENT_HEADING_MATCH
                        reasons.append(f"intent_heading:{th}")

                # æ ‡é¢˜å…³é”®è¯åŒ¹é…
                for kw in keywords:
                    if kw and isinstance(kw, str) and kw in heading_lower:
                        section_score += ScoringConfig.KEYWORD_IN_HEADING
                        reasons.append(f"heading_kw:{kw}")

                # æ–‡æœ¬å…³é”®è¯åŒ¹é…
                for kw in keywords:
                    if not kw or not isinstance(kw, str):
                        continue
                    occurrences = text_lower.count(kw)
                    if occurrences:
                        kw_score = min(
                            occurrences * ScoringConfig.KEYWORD_IN_TEXT_PER_OCCURRENCE,
                            ScoringConfig.MAX_KEYWORD_TEXT_SCORE
                        )
                        section_score += kw_score
                        reasons.append(f"text_kw:{kw}x{occurrences}")

                # æ¨¡å—æ„å›¾ç‰¹æ®Šå¤„ç†
                if intent == "modules":
                    if "â€¢" in text or "\n-" in text or text.count("\n") > 5:
                        section_score += ScoringConfig.LIST_STRUCTURE_BONUS
                        reasons.append("list_structure")
                    if re.search(r"\b[A-Z]{4}\d{4}\b", text):
                        section_score += ScoringConfig.COURSE_CODE_BONUS
                        reasons.append("course_code")

                # é•¿åº¦å½’ä¸€åŒ–
                text_len = max(len(text), 1)
                normalization_factor = 1 + math.log10(text_len / ScoringConfig.SECTION_LENGTH_NORMALIZATION_BASE + 1)
                section_score /= normalization_factor

                if section_score > 0:
                    snippet = text.strip().replace("\u00a0", " ")[:1000]
                    relevant_sections.append({
                        "heading": heading,
                        "text": snippet,
                        "score": section_score,
                        "reasons": reasons,
                    })
                    score += section_score

            except Exception as e:
                logger.debug(f"âš ï¸  Section scoring error: {e}")
                continue

        relevant_sections.sort(key=lambda item: item["score"], reverse=True)
        return score, relevant_sections[:5]

    def _extract_smart_keywords(self, query: str) -> List[str]:
        """æ™ºèƒ½æå–æŸ¥è¯¢å…³é”®è¯"""
        if not query or not isinstance(query, str):
            return []
        
        stopwords = {
            "what", "how", "where", "when", "which", "who", "the", "a", "an",
            "is", "are", "was", "were", "do", "does", "did", "about", "for",
            "çš„", "æ˜¯", "æœ‰", "åœ¨", "å—", "å‘¢", "å•Š", "äº†",
        }

        try:
            query_lower = query.lower()

            # æå–è‹±æ–‡å•è¯
            english_words = re.findall(r"\b[a-z]+\b", query_lower)
            english_keywords = [word for word in english_words if word not in stopwords and len(word) > 2]

            # æå–ä¸­æ–‡å…³é”®è¯
            chinese_keywords: List[str] = []
            chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
            for chunk in chinese_matches:
                if HAVE_JIEBA:
                    chinese_keywords.extend([
                        token for token in jieba.cut(chunk) 
                        if len(token) > 1 and token not in stopwords
                    ])
                else:
                    chinese_keywords.extend([
                        chunk[i : i + j] 
                        for i in range(len(chunk)) 
                        for j in range(2, 5) 
                        if i + j <= len(chunk)
                    ])

            keywords: List[str] = english_keywords + chinese_keywords

            # æ‰©å±•å¸¸è§çŸ­è¯­
            for phrase, expansions in CommonPhrases.PHRASES.items():
                if phrase in query_lower:
                    keywords.extend(expansions)

            # å»é‡
            expanded: List[str] = []
            seen = set()
            for kw in keywords:
                if not kw or not isinstance(kw, str) or kw in seen:
                    continue
                expanded.append(kw)
                seen.add(kw)

            # è¿‡æ»¤é¢†åŸŸè¯æ±‡
            domain_filtered = [kw for kw in expanded if len(kw) > 3 or kw in self.domain_vocab]

            return domain_filtered or expanded

        except Exception as e:
            logger.error(f"âŒ Keyword extraction error: {e}")
            return []

    def _load_semantic_model(self) -> None:
        """åŠ è½½è¯­ä¹‰æ¨¡å‹"""
        if not HAVE_SEMANTIC:
            return

        try:
            self.semantic_model = SentenceTransformer("all-MiniLM-L6-v2")
            logger.info("âœ… è¯­ä¹‰æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as exc:
            logger.warning(f"âš ï¸  è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {exc}")
            self.enable_semantic = False

    def _precompute_embeddings(self, documents: List[dict]) -> None:
        """é¢„è®¡ç®—æ–‡æ¡£ embeddings"""
        if not self.enable_semantic or self.semantic_model is None:
            return

        for doc_idx, doc in enumerate(documents):
            if not doc or not isinstance(doc, dict):
                continue
            
            if doc_idx in self._section_embeddings_cache:
                continue

            self._section_embeddings_cache[doc_idx] = {}
            sections = doc.get("sections", [])
            if not sections or not isinstance(sections, list):
                continue
            
            for sec_idx, section in enumerate(sections):
                if not section or not isinstance(section, dict):
                    continue
                text = section.get("text") or ""
                if text and isinstance(text, str) and len(text) > 50:
                    try:
                        emb = self.semantic_model.encode(text, convert_to_tensor=True)
                        self._section_embeddings_cache[doc_idx][sec_idx] = emb
                    except Exception as exc:
                        logger.debug(f"âš ï¸  è®¡ç®— embedding å¤±è´¥: {exc}")

    def _encode_query(self, query: str) -> Optional[Any]:
        """ç¼–ç æŸ¥è¯¢"""
        if not self.enable_semantic or self.semantic_model is None:
            return None

        try:
            return self.semantic_model.encode(query, convert_to_tensor=True)
        except Exception as exc:
            logger.warning(f"âš ï¸  æŸ¥è¯¢ç¼–ç å¤±è´¥: {exc}")
            return None

    def _semantic_boost(self, doc_idx: int, sections: List[dict], query_emb: Any) -> float:
        """è¯­ä¹‰ç›¸ä¼¼åº¦åŠ æˆ"""
        if not sections or query_emb is None:
            return 0.0

        best_sim = 0.0

        if doc_idx in self._section_embeddings_cache:
            for sec_idx, section_emb in self._section_embeddings_cache[doc_idx].items():
                try:
                    similarity = util.pytorch_cos_sim(query_emb, section_emb).item()
                    best_sim = max(best_sim, similarity)
                except Exception:
                    continue

        return best_sim * ScoringConfig.SEMANTIC_MULTIPLIER

    def _build_domain_vocab(self) -> set:
        """æ„å»ºé¢†åŸŸè¯æ±‡è¡¨"""
        repo_root = Path(__file__).resolve().parents[1]
        data_paths = [
            repo_root / "public" / "data" / "ucl_programs.json",
            repo_root / "public" / "data" / "ucl_services.json",
        ]

        words: List[str] = []
        for path in data_paths:
            if not path.exists():
                continue
            try:
                with path.open("r", encoding="utf-8") as fh:
                    data = json.load(fh)
                    if not data or not isinstance(data, list):
                        continue
                    for item in data:
                        if not item or not isinstance(item, dict):
                            continue
                        title = item.get("title", "")
                        if not title or not isinstance(title, str):
                            continue
                        tokens = re.findall(r"\b\w+\b|[\u4e00-\u9fff]+", title.lower())
                        words.extend(tokens)
            except Exception as exc:
                logger.warning(f"âš ï¸  è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {exc}")
                continue

        counter = Counter(words)
        vocab = {token for token, freq in counter.items() if freq > 1 and len(token) > 2}

        logger.info(f"ğŸ“š Built domain vocab with {len(vocab)} words")
        return vocab


def create_retriever(enable_semantic: bool = True, cache_embeddings: bool = True) -> EnhancedRetriever:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ£€ç´¢å™¨"""
    return EnhancedRetriever(enable_semantic=enable_semantic, cache_embeddings=cache_embeddings)