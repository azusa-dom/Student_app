#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›ç‰ˆ QA Enhanced Wrapper
- ä¿®å¤ LLM è°ƒç”¨é€»è¾‘
- æ”¹è¿› prompt è®¾è®¡
- ä¼˜åŒ–ç­”æ¡ˆæ ¼å¼
- å¢åŠ è¯¦ç»†æ—¥å¿—
"""
import os
import json
import sys
import traceback
from pathlib import Path
from typing import Dict, List, Tuple
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åŠ¨æ€åŠ å…¥é¡¹ç›®æ ¹è·¯å¾„
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "scripts"))

# ==================== LLM å®¢æˆ·ç«¯é…ç½® ====================
try:
    from scripts.llm_client import chat_completion, LLMUnavailable, OLLAMA_MODEL, is_configured
    HAVE_LLM = True
    logger.info(f"âœ… LLM å®¢æˆ·ç«¯åŠ è½½æˆåŠŸï¼Œæ¨¡å‹: {OLLAMA_MODEL}")
    
    # æ£€æŸ¥ LLM æœåŠ¡æ˜¯å¦å¯ç”¨
    if not is_configured():
        logger.warning("âš ï¸  Ollama æœåŠ¡æœªè¿è¡Œæˆ–ä¸å¯è®¿é—®")
        HAVE_LLM = False
    else:
        logger.info("âœ… Ollama æœåŠ¡è¿è¡Œä¸­")
        
except Exception as e:
    logger.error(f"âŒ LLM å®¢æˆ·ç«¯åŠ è½½å¤±è´¥: {e}")
    HAVE_LLM = False
    
    class LLMUnavailable(Exception): 
        pass
    
    def chat_completion(*_args, **_kwargs): 
        raise LLMUnavailable("LLM æœªåˆå§‹åŒ–")
    
    OLLAMA_MODEL = "unknown"

# ==================== åŠ è½½æ–‡æ¡£æ•°æ® ====================
REAL_DOCS: List[Dict[str, str]] = []

def load_documents_from_file(path: Path) -> List[Dict[str, str]]:
    """ä» JSON æ–‡ä»¶åŠ è½½æ–‡æ¡£"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        docs = []
        for item in data:
            title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
            url = item.get("url", "#")
            sections = item.get("sections", [])
            
            # æå–æ‰€æœ‰ section çš„æ–‡æœ¬
            text_parts = []
            for s in sections:
                if isinstance(s, dict):
                    heading = s.get("heading", "")
                    text = s.get("text", "")
                    if heading:
                        text_parts.append(f"## {heading}\n{text}")
                    else:
                        text_parts.append(text)
            
            full_text = "\n\n".join(text_parts).strip()
            
            if full_text:
                docs.append({
                    "title": title.strip(),
                    "url": url,
                    "text": full_text
                })
        
        return docs
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {path}: {e}")
        return []

def load_all_documents():
    """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    global REAL_DOCS
    program_path = ROOT / "public/data/ucl_programs.json"
    service_path = ROOT / "public/data/ucl_services.json"
    
    programs = load_documents_from_file(program_path)
    services = load_documents_from_file(service_path)
    
    REAL_DOCS = programs + services
    logger.info(f"âœ… æˆåŠŸåŠ è½½æ–‡æ¡£: {len(programs)} ä¸ªè¯¾ç¨‹, {len(services)} ä¸ªæœåŠ¡ï¼Œæ€»è®¡ {len(REAL_DOCS)} ä¸ª")

# åˆå§‹åŒ–åŠ è½½
load_all_documents()

# ==================== æŸ¥è¯¢å¤„ç† ====================
def _detect_language_and_rewrite(query: str) -> Tuple[str, List[str]]:
    """æ£€æµ‹è¯­è¨€å¹¶ç”ŸæˆæŸ¥è¯¢æ”¹å†™"""
    q = (query or "").strip()
    if not q:
        return "zh", [""]
    
    has_zh = any("\u4e00" <= ch <= "\u9fff" for ch in q)
    lang = "zh" if has_zh else "en"
    
    rewrites = [q]
    
    # æ·»åŠ ç«™ç‚¹é™å®š
    if lang == "zh":
        rewrites.append(f"{q} UCL")
        rewrites.append(f"{q} ä¼¦æ•¦å¤§å­¦å­¦é™¢")
    else:
        rewrites.append(f"{q} UCL")
        rewrites.append(f"{q} University College London")
    
    logger.debug(f"æŸ¥è¯¢æ”¹å†™: {rewrites}")
    return lang, rewrites

# ==================== æ–‡æ¡£æ£€ç´¢ ====================
def _retrieve_documents(query: str, top_k: int = 5) -> List[Dict[str, str]]:
    """æ”¹è¿›çš„æ–‡æ¡£æ£€ç´¢ï¼Œæ”¯æŒä¸­è‹±æ–‡åˆ†è¯å’Œè¯­ä¹‰åŒ¹é…"""
    import re
    
    q = query.lower()
    
    # æå–å…³é”®è¯
    keywords = []
    
    # è‹±æ–‡å…³é”®è¯ï¼ˆ2å­—ç¬¦ä»¥ä¸Šï¼‰
    english_words = re.findall(r'[a-zA-Z]{2,}', q)
    keywords.extend([w.lower() for w in english_words])
    
    # ä¸­æ–‡å…³é”®è¯ï¼ˆ2-4å­—ï¼‰
    chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', q)
    keywords.extend(chinese_words)
    
    # ç‰¹æ®Šå…³é”®è¯è¯†åˆ«
    special_keywords = {
        "è¯­è¨€": ["language", "english", "ielts", "toefl"],
        "è¦æ±‚": ["requirement", "entry", "admission"],
        "ç¡•å£«": ["master", "msc", "postgraduate"],
        "å¿ƒç†": ["psychology", "mental", "wellbeing"],
        "å’¨è¯¢": ["counselling", "counseling", "support"]
    }
    
    for zh_key, en_keys in special_keywords.items():
        if zh_key in query:
            keywords.extend(en_keys)
    
    logger.info(f"æå–å…³é”®è¯: {keywords}")
    
    if not keywords:
        keywords = q.split()
    
    # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§å¾—åˆ†
    scored_docs = []
    
    for doc in REAL_DOCS:
        title = doc.get("title", "").lower()
        text = doc.get("text", "").lower()
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        title_matches = sum(5 for kw in keywords if kw in title)  # æ ‡é¢˜æƒé‡æ›´é«˜
        text_matches = sum(1 for kw in keywords if kw in text)
        
        # ç²¾ç¡®çŸ­è¯­åŒ¹é…åŠ åˆ†
        if q in title:
            title_matches += 10
        if q in text:
            text_matches += 3
        
        total_score = title_matches + text_matches
        
        if total_score > 0:
            scored_docs.append((total_score, doc))
    
    # æ’åºå¹¶è¿”å›
    scored_docs.sort(key=lambda x: x[0], reverse=True)
    
    result = [doc for score, doc in scored_docs[:top_k]]
    
    logger.info(f"æ£€ç´¢åˆ° {len(result)} ä¸ªç›¸å…³æ–‡æ¡£")
    for i, doc in enumerate(result[:3], 1):
        logger.debug(f"  {i}. {doc['title']}")
    
    return result

# ==================== ç­”æ¡ˆç”Ÿæˆ ====================
def _generate_local_summary(query: str, docs: List[Dict[str, str]]) -> str:
    """æ”¹è¿›çš„æœ¬åœ°æ‘˜è¦ç”Ÿæˆï¼ˆå½“ LLM ä¸å¯ç”¨æ—¶ï¼‰"""
    if not docs:
        return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•ï¼š\n1. æ¢ä¸ªé—®æ³•\n2. è®¿é—® UCL å®˜ç½‘æŸ¥è¯¢\n3. è”ç³» UCL æ‹›ç”ŸåŠ"
    
    # æå–æœ€ç›¸å…³çš„ä¿¡æ¯
    summary_parts = [f"æ ¹æ® UCL å®˜æ–¹èµ„æ–™ï¼Œå…³äºã€Œ{query}ã€ï¼š\n"]
    
    for idx, doc in enumerate(docs[:3], start=1):
        title = doc.get('title', 'æœªçŸ¥æ¥æº')
        text = doc.get('text', '')
        
        # æ™ºèƒ½æˆªå–ç›¸å…³æ®µè½
        lines = text.split('\n')
        relevant_lines = []
        
        # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ®µè½
        query_lower = query.lower()
        for line in lines:
            if any(keyword in line.lower() for keyword in query_lower.split()):
                relevant_lines.append(line.strip())
                if len(relevant_lines) >= 3:  # æœ€å¤š3ä¸ªæ®µè½
                    break
        
        if relevant_lines:
            snippet = '\n'.join(relevant_lines)
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…æ®µè½ï¼Œå–å¼€å¤´
            snippet = text[:300].strip()
        
        if len(snippet) > 300:
            snippet = snippet[:300] + "..."
        
        summary_parts.append(f"\nã€{idx}ã€‘{title}\n{snippet}")
    
    summary_parts.append("\n\nğŸ’¡ å»ºè®®æŸ¥çœ‹ä¸‹æ–¹å‚è€ƒæ¥æºè·å–å®Œæ•´ä¿¡æ¯")
    
    return "".join(summary_parts)

def _build_improved_prompt(query: str, docs: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """æ„å»ºæ”¹è¿›çš„ prompt"""
    
    # æå–å‰3ä¸ªæœ€ç›¸å…³æ–‡æ¡£çš„å†…å®¹
    context_parts = []
    for idx, doc in enumerate(docs[:3], start=1):
        title = doc.get('title', 'æœªçŸ¥æ–‡æ¡£')
        text = doc.get('text', '')[:800]  # é™åˆ¶é•¿åº¦
        context_parts.append(f"ã€æ–‡æ¡£{idx}ã€‘{title}\n{text}")
    
    context = "\n\n".join(context_parts)
    
    # æ”¹è¿›çš„ç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯ UCLï¼ˆä¼¦æ•¦å¤§å­¦å­¦é™¢ï¼‰çš„å®˜æ–¹æ™ºèƒ½åŠ©æ‰‹ã€‚

ä½ çš„èŒè´£ï¼š
1. **ä¸¥æ ¼åŸºäºæä¾›çš„èµ„æ–™å›ç­”**ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœèµ„æ–™ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„èµ„æ–™æ— æ³•å®Œæ•´å›ç­”"
3. ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„ä¸­æ–‡å›ç­”
4. å›ç­”è¦ç»“æ„åŒ–ã€æ˜“è¯»

å›ç­”æ ¼å¼è¦æ±‚ï¼š
- å¼€å¤´ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜
- ä½¿ç”¨è¦ç‚¹åˆ—è¡¨ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
- ç®€æ´æ˜äº†ï¼Œé¿å…å†—ä½™
- é•¿åº¦æ§åˆ¶åœ¨150å­—ä»¥å†…"""

    # ç”¨æˆ·æç¤º
    user_prompt = f"""å‚è€ƒèµ„æ–™ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä»¥ä¸Šèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜ã€‚"""

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

def answer_enhanced(query: str, top_k: int = 5) -> Dict[str, object]:
    """
    å¢å¼ºç‰ˆé—®ç­”ä¸»å‡½æ•°
    
    Args:
        query: ç”¨æˆ·é—®é¢˜
        top_k: è¿”å›æ–‡æ¡£æ•°é‡
    
    Returns:
        åŒ…å«ç­”æ¡ˆã€å¼•ç”¨ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    logger.info(f"\n{'='*60}\næ”¶åˆ°æŸ¥è¯¢: {query}\n{'='*60}")
    
    # 1. æŸ¥è¯¢å¤„ç†
    lang, rewrites = _detect_language_and_rewrite(query)
    logger.info(f"æ£€æµ‹è¯­è¨€: {lang}")
    
    # 2. æ–‡æ¡£æ£€ç´¢
    docs = _retrieve_documents(query, top_k)
    
    if not docs:
        logger.warning("âš ï¸  æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
        return {
            "intent": lang,
            "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªé—®æ³•æˆ–è®¿é—® UCL å®˜ç½‘ã€‚",
            "citations": [],
            "rewritten_queries": rewrites,
            "reranked": [],
        }
    
    # 3. ç”Ÿæˆç­”æ¡ˆ
    answer = None
    
    # å°è¯•ä½¿ç”¨ LLM
    if HAVE_LLM:
        try:
            messages = _build_improved_prompt(query, docs)
            logger.info(f"ğŸ“¤ è°ƒç”¨ LLM ({OLLAMA_MODEL})...")
            
            answer = chat_completion(
                messages, 
                temperature=0.3,  # é™ä½æ¸©åº¦æé«˜å‡†ç¡®æ€§
                timeout=60
            )
            
            if answer:
                logger.info(f"âœ… LLM ç”Ÿæˆç­”æ¡ˆæˆåŠŸ (é•¿åº¦: {len(answer)})")
            else:
                raise LLMUnavailable("LLM è¿”å›ç©ºç­”æ¡ˆ")
                
        except LLMUnavailable as e:
            logger.warning(f"âš ï¸  LLM ä¸å¯ç”¨: {e}ï¼Œé™çº§åˆ°æœ¬åœ°æ‘˜è¦")
            answer = None
        except Exception as e:
            logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            traceback.print_exc()
            answer = None
    else:
        logger.info("â„¹ï¸  LLM æœªé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°æ‘˜è¦æ¨¡å¼")
    
    # é™çº§åˆ°æœ¬åœ°æ‘˜è¦
    if not answer:
        logger.info("ğŸ“ ä½¿ç”¨æœ¬åœ°æ‘˜è¦ç”Ÿæˆç­”æ¡ˆ")
        answer = _generate_local_summary(query, docs)
    
    # 4. æ„å»ºå¼•ç”¨åˆ—è¡¨
    citations = [
        {
            "title": doc.get("title", "æœªçŸ¥æ¥æº"),
            "url": doc.get("url", "#")
        }
        for doc in docs[:5]
    ]
    
    result = {
        "intent": lang,
        "answer": answer or "æ— æ³•ç”Ÿæˆç­”æ¡ˆ",
        "citations": citations,
        "rewritten_queries": rewrites,
        "reranked": docs,
    }
    
    logger.info(f"âœ… é—®ç­”å®Œæˆ\n{'='*60}\n")
    
    return result


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    # æµ‹è¯•é—®é¢˜
    test_queries = [
        "è®¡ç®—æœºç§‘å­¦ç¡•å£«çš„è¯­è¨€è¦æ±‚",
        "å¦‚ä½•é¢„çº¦å¿ƒç†å’¨è¯¢",
        "å•†ç§‘ç¡•å£«å…¥å­¦è¦æ±‚"
    ]
    
    print("\n" + "="*60)
    print("QA ç³»ç»Ÿæµ‹è¯•")
    print("="*60)
    
    for q in test_queries:
        print(f"\né—®é¢˜: {q}")
        result = answer_enhanced(q, top_k=3)
        print(f"\nç­”æ¡ˆ:\n{result['answer']}")
        print(f"\nå¼•ç”¨æ–‡æ¡£: {len(result['citations'])} ä¸ª")
        print("-"*60)