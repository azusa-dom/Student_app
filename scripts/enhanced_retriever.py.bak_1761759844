#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import logging
logger = logging.getLogger(__name__)

"""enhanced_retriever.py - å¢å¼ºç‰ˆæ£€ç´¢å™¨ï¼ˆæ”¯æŒè¯­ä¹‰æœç´¢ï¼‰"""

import os
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
from functools import lru_cache

# æ–°å¢è¯­ä¹‰æœç´¢ä¾èµ–
try:
    from sentence_transformers import SentenceTransformer
    HAVE_SEMANTIC = True
    logger.info("âœ… Sentence transformers available")
except ImportError:
    HAVE_SEMANTIC = False
    logger.warning("âš ï¸ Sentence transformers not available, semantic search disabled")

# ... å…¶ä½™å¯¼å…¥ ...

# åœ¨ EnhancedRetriever ç±»ä¸­æ·»åŠ è¯­ä¹‰æœç´¢
class EnhancedRetriever:
    """å¢å¼ºç‰ˆæ£€ç´¢å™¨ - æ”¯æŒè¯­ä¹‰æœç´¢"""
    
    def __init__(self, enable_semantic=True, cache_embeddings=True):
        self.enable_semantic = enable_semantic and HAVE_SEMANTIC
        self.cache_embeddings = cache_embeddings
        
        # åˆå§‹åŒ–è¯­ä¹‰æœç´¢æ¨¡å‹
        if self.enable_semantic:
            try:
                self.model = SentenceTransformer('all-MiniLM-L6-v2', device="cpu")  # è½»é‡çº§æ¨¡å‹
                logger.info("âœ… Semantic search model loaded")
            except Exception as e:
                logger.error(f"âŒ Failed to load semantic model: {e}")
                self.enable_semantic = False
        else:
            self.model = None
        
        # ç¼“å­˜åµŒå…¥å‘é‡
        self.embedding_cache = {} if cache_embeddings else None
    
    def semantic_search(self, query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """è¯­ä¹‰æœç´¢å®ç°"""
        if not self.enable_semantic or not self.model:
            logger.info("âœ… Semantic retrieval used"); return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.model.encode(query, convert_to_tensor=True)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            results = []
            for doc in documents:
                # æå–æ–‡æ¡£æ–‡æœ¬
                text = self._extract_doc_text(doc)
                if not text:
                    continue
                
                # ç”Ÿæˆæˆ–è·å–æ–‡æ¡£åµŒå…¥
                doc_embedding = self._get_doc_embedding(text)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )
                
                results.append({
                    'doc': doc,
                    'score': float(similarity),
                    'matched_sections': self._find_relevant_sections(doc, query)
                })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ Semantic search failed: {e}")
            return []
    
    def _extract_doc_text(self, doc: Dict) -> str:
        """æå–æ–‡æ¡£æ–‡æœ¬ç”¨äºåµŒå…¥"""
        text_parts = []
        
        # æ ‡é¢˜
        if 'title' in doc:
            text_parts.append(doc['title'])
        
        # ä¸»è¦å†…å®¹
        if 'sections' in doc:
            for section in doc['sections']:
                if isinstance(section, dict):
                    heading = section.get('heading', '')
                    text = section.get('text', '')
                    if heading:
                        text_parts.append(f"{heading}: {text}")
                    else:
                        text_parts.append(text)
        
        return ' '.join(text_parts)
    
    def _get_doc_embedding(self, text: str):
        """è·å–æˆ–è®¡ç®—æ–‡æ¡£åµŒå…¥"""
        if self.embedding_cache is not None:
            cache_key = hash(text)
            if cache_key in self.embedding_cache:
                return self.embedding_cache[cache_key]
        
        embedding = self.model.encode(text, convert_to_tensor=True)
        
        if self.embedding_cache is not None:
            cache_key = hash(text)
            self.embedding_cache[cache_key] = embedding
        
        return embedding
    
    def _find_relevant_sections(self, doc: Dict, query: str) -> List[Dict]:
        """æ‰¾åˆ°ç›¸å…³çš„ç« èŠ‚"""
        relevant = []
        query_lower = query.lower()
        
        if 'sections' in doc:
            for section in doc['sections']:
                if isinstance(section, dict):
                    heading = section.get('heading', '').lower()
                    text = section.get('text', '').lower()
                    
                    # ç®€å•å…³é”®è¯åŒ¹é…
                    if any(word in heading or word in text 
                          for word in query_lower.split() if len(word) > 2):
                        relevant.append(section)
        
        return relevant[:3]  # æœ€å¤š3ä¸ªç›¸å…³ç« èŠ‚

# ... å…¶ä½™ä»£ç  ...

"""å¢å¼ºç‰ˆæ£€ç´¢å™¨ - ä¿®å¤ç©ºå€¼é”™è¯¯ç‰ˆæœ¬

ä¸»è¦ä¿®å¤:
1. æ‰€æœ‰ç©ºå€¼æ£€æŸ¥å’Œå®¹é”™å¤„ç†
2. é˜²æ­¢ NoneType è¿­ä»£é”™è¯¯
3. å®Œå–„çš„å¼‚å¸¸å¤„ç†
"""


import json
import math
import re
from collections import Counter
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ ç½‘ç»œæœç´¢å¯¼å…¥
import requests
from bs4 import BeautifulSoup
import time


# ============================================================================
# ä¾èµ–æ£€æŸ¥
# ============================================================================
try:
    from sentence_transformers import SentenceTransformer, util  # type: ignore
    import torch  # type: ignore

    HAVE_SEMANTIC = True
    logger.info("âœ… sentence-transformers loaded for semantic search")
except Exception:
    HAVE_SEMANTIC = False
    logger.warning("âš ï¸ sentence-transformers not available, semantic search disabled")

try:
    import jieba  # type: ignore

    HAVE_JIEBA = True
    logger.info("âœ… jieba loaded for Chinese tokenization")
except Exception:
    HAVE_JIEBA = False
    logger.warning("âš ï¸ jieba not available, fallback to regex for Chinese")


# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
class ScoringConfig:
    """è¯„åˆ†æƒé‡é…ç½®"""
    KEYWORD_IN_TITLE = 14
    EXACT_QUERY_MATCH = 36
    SYNONYM_MATCH = 28
    PROGRAM_PREFIX_MATCH = 60
    PROGRAM_CONTAINS_MATCH = 32
    FUZZY_MATCH_MULTIPLIER = 24
    DEGREE_TAG_MATCH = 12
    INTENT_HEADING_MATCH = 20
    KEYWORD_IN_HEADING = 10
    KEYWORD_IN_TEXT_PER_OCCURRENCE = 3
    MAX_KEYWORD_TEXT_SCORE = 18
    LIST_STRUCTURE_BONUS = 15
    COURSE_CODE_BONUS = 10
    LEVEL_BOOST = 12
    LEVEL_PENALTY = -8
    SEMANTIC_MULTIPLIER = 40
    SECTION_LENGTH_NORMALIZATION_BASE = 150


class CommonPhrases:
    """å¸¸è§æŸ¥è¯¢çŸ­è¯­åŠå…¶æ‰©å±•"""
    PHRASES = {
        "language requirements": ["entry requirements", "english language", "ielts", "toefl", "language proficiency"],
        "core modules": ["compulsory modules", "core courses", "required modules", "mandatory"],
        "tuition fees": ["fees", "cost", "tuition", "funding", "scholarship"],
        "entry requirements": ["admission", "qualification", "prerequisite", "grade"],
        "career prospects": ["employment", "graduate outcomes", "career", "job"],
    }


# ============================================================================
# ä¸»ç±»
# ============================================================================

class EnhancedRetriever:
    """å¢å¼ºç‰ˆæ£€ç´¢å™¨ - å®Œå…¨ä¿®å¤ç©ºå€¼å¤„ç†"""

    def __init__(self, enable_semantic: bool = True, cache_embeddings: bool = True) -> None:
        self.enable_semantic = enable_semantic and HAVE_SEMANTIC
        self.cache_embeddings = cache_embeddings

        # ğŸ”¥ ç¡®ä¿æ‰€æœ‰å…³é”®è¯åˆ—è¡¨éƒ½ä¸ä¸º None
        self.intent_keywords: Dict[str, List[str]] = {
            "modules": ["module", "course", "subject", "curriculum", "syllabus", "teaching", "learn", "è¯¾ç¨‹", "æ¨¡å—"],
            "requirements": ["requirement", "prerequisite", "entry", "admission", "qualification", "ielts", "toefl", "è¦æ±‚", "ç”³è¯·"],
            "career": ["career", "job", "employment", "graduate", "prospect", "å°±ä¸š", "èŒä¸š"],
            "fees": ["fee", "tuition", "cost", "funding", "scholarship", "å­¦è´¹", "è´¹ç”¨"],
        }

        self.intent_headings: Dict[str, List[str]] = {
            "modules": [
                "module", "curriculum", "syllabus", "teaching", "what you will learn",
                "course structure", "compulsory", "optional", "placement", "supervisor"
            ],
            "requirements": ["entry", "requirement", "admission", "english", "qualification", "ielts", "toefl", "language"],
            "career": ["career", "employment", "graduate", "prospects", "outcomes"],
            "fees": ["fee", "tuition", "cost", "funding", "scholarship"],
        }

        self.custom_synonyms: Dict[str, List[str]] = {
            "data science": ["data science", "æ•°æ®ç§‘å­¦", "analytics", "data"],
            "computer science": ["computer science", "computing", "computer"],
            "business analytics": ["business analytics", "å•†ä¸šåˆ†æ"],
            "management": ["management", "ç®¡ç†", "business"],
        }

        self.domain_vocab = self._build_domain_vocab()
        self.semantic_model: Optional[Any] = None
        self._section_embeddings_cache: Dict[int, Dict[int, Any]] = {}

        if self.enable_semantic:
            self._load_semantic_model()

    def detect_intent(self, query: str) -> str:
        """ğŸ”¥ æ£€æµ‹æŸ¥è¯¢æ„å›¾ - å®Œå…¨é˜²æŠ¤ç‰ˆæœ¬"""
        if not query:
            return "general"
        
        try:
            query_lower = query.lower()
        except (AttributeError, TypeError):
            return "general"
        
        for intent, keywords in self.intent_keywords.items():
            # ğŸ”¥ å¤šé‡ç©ºå€¼æ£€æŸ¥
            if not keywords:
                continue
            try:
                # ç¡®ä¿ keywords æ˜¯å¯è¿­ä»£çš„ä¸”æ¯ä¸ª kw éƒ½æ˜¯å­—ç¬¦ä¸²
                for kw in keywords:
                    if kw and isinstance(kw, str) and kw in query_lower:
                        return intent
            except (TypeError, AttributeError):
                continue
        
        return "general"

    def _detect_intent(self, query: str, keywords: List[str]) -> str:
        """ğŸ”¥ æ£€æµ‹æŸ¥è¯¢æ„å›¾ï¼ˆå¸¦å…³é”®è¯åˆ—è¡¨ï¼‰- å®Œå…¨é˜²æŠ¤ç‰ˆæœ¬"""
        if not query:
            return "general"
        
        try:
            query_lower = query.lower()
        except (AttributeError, TypeError):
            return "general"
        
        # ğŸ”¥ ç¡®ä¿ keywords ä¸ä¸º None
        if keywords is None:
            keywords = []
        
        # æ„å›¾åŒ¹é…è§„åˆ™
        intent_patterns = {
            "modules": [
                "module", "course", "curriculum", "subject", "teaching",
                "æ¨¡å—", "è¯¾ç¨‹", "ç§‘ç›®", "æ•™å­¦"
            ],
            "requirements": [
                "requirement", "entry", "admission", "prerequisite", "qualify",
                "è¦æ±‚", "å…¥å­¦", "æ¡ä»¶", "èµ„æ ¼", "ç”³è¯·"
            ],
            "fees": [
                "fee", "tuition", "cost", "price", "funding", "scholarship",
                "å­¦è´¹", "è´¹ç”¨", "ä»·æ ¼", "å¥–å­¦é‡‘", "èµ„åŠ©"
            ],
            "career": [
                "career", "job", "employment", "graduate", "outcome",
                "èŒä¸š", "å°±ä¸š", "å·¥ä½œ", "å‰æ™¯", "å‘å±•"
            ],
            "services": [
                "service", "support", "help", "counseling", "wellbeing",
                "æœåŠ¡", "æ”¯æŒ", "å¸®åŠ©", "å’¨è¯¢", "è¾…å¯¼"
            ]
        }
        
        # æ£€æµ‹æ„å›¾
        for intent, patterns in intent_patterns.items():
            if not patterns:
                continue
            try:
                for pattern in patterns:
                    if not pattern or not isinstance(pattern, str):
                        continue
                    # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«æ¨¡å¼
                    if pattern in query_lower:
                        return intent
                    # æ£€æŸ¥å…³é”®è¯åˆ—è¡¨
                    if keywords:
                        for kw in keywords:
                            if kw and isinstance(kw, str) and pattern in kw:
                                return intent
            except (TypeError, AttributeError):
                continue
        
        return "general"

    def search_with_context(self, query: str, documents: List[dict], top_k: int = 8) -> List[Dict]:
        """ğŸ”¥ ä¿®å¤åçš„ä¸»æ£€ç´¢æ–¹æ³• - å®Œå…¨é˜²æŠ¤ç‰ˆæœ¬"""
        
        # ğŸ”¥ ç©ºå€¼æ£€æŸ¥
        if not query:
            logger.warning("âš ï¸  Empty query")
            return []
        
        if not documents:
            logger.warning("âš ï¸  Empty documents")
            return []

        try:
            # ğŸ”¥ æå–å…³é”®è¯æ—¶æ·»åŠ ç©ºå€¼æ£€æŸ¥
            keywords = self._extract_smart_keywords(query)
            if not keywords:
                logger.warning("âš ï¸  No keywords extracted, using simple split")
                keywords = [w for w in query.lower().split() if len(w) > 2]
        except Exception as e:
            logger.error(f"âŒ Keyword extraction failed: {e}")
            keywords = [w for w in query.lower().split() if len(w) > 2]

        # æ£€æµ‹æ„å›¾
        try:
            intent = self._detect_intent(query, keywords)
        except Exception as e:
            logger.error(f"âŒ Intent detection failed: {e}")
            intent = "general"
        
        query_lower = query.lower()

        logger.info(f"ğŸ” [æ£€ç´¢] æŸ¥è¯¢: {query[:100]}")
        logger.info(f"ğŸ¯ [æ£€ç´¢] æ„å›¾: {intent}")
        logger.info(f"ğŸ”‘ [æ£€ç´¢] å…³é”®è¯: {keywords[:10]}")

        # é¢„è®¡ç®— embeddings
        query_embedding = None
        if self.enable_semantic:
            try:
                query_embedding = self._encode_query(query)
            except Exception as e:
                logger.warning(f"âš ï¸  Query encoding failed: {e}")

        if self.enable_semantic and self.cache_embeddings and not self._section_embeddings_cache:
            try:
                logger.info("ğŸ”„ [ç¼“å­˜] é¢„è®¡ç®—æ–‡æ¡£ embeddings...")
                self._precompute_embeddings(documents)
            except Exception as e:
                logger.warning(f"âš ï¸  Embedding precompute failed: {e}")

        results: List[Dict] = []

        for doc_idx, doc in enumerate(documents):
            try:
                # ğŸ”¥ æ·»åŠ ç©ºå€¼æ£€æŸ¥
                title_raw = doc.get("title")
                if not title_raw or not isinstance(title_raw, str):
                    continue
                
                title_lower = title_raw.lower()

                # æ ‡é¢˜è¯„åˆ†
                title_score, title_hits = self._score_title_similarity(title_lower, keywords, query_lower)
                
                # å­¦ä½çº§åˆ«è¯„åˆ†
                level_score = self._score_level(doc, query_lower)
                
                # ç« èŠ‚è¯„åˆ†
                sections = doc.get("sections")
                if not sections or not isinstance(sections, list):
                    sections = []
                
                sections_score, matched_sections = self._score_sections(sections, keywords, intent)

                # ğŸ”¥ è¯­ä¹‰è¯„åˆ†æ—¶æ·»åŠ ç©ºå€¼æ£€æŸ¥
                semantic_score = 0.0
                if self.enable_semantic and query_embedding is not None:
                    try:
                        semantic_score = self._semantic_boost(doc_idx, sections, query_embedding)
                    except Exception as e:
                        logger.debug(f"âš ï¸  Semantic scoring failed for doc {doc_idx}: {e}")

                base_score = title_score + level_score + sections_score + semantic_score

                # è¿‡æ»¤ä½åˆ†ç»“æœ
                if base_score <= 0 or (not matched_sections and title_score < 25):
                    continue

                results.append({
                    "doc": doc,
                    "score": base_score,
                    "title_score": title_score,
                    "sections_score": sections_score,
                    "semantic_score": semantic_score,
                    "level_score": level_score,
                    "matched_sections": matched_sections,
                    "title_matches": title_hits,
                    "intent": intent,
                })
            except Exception as e:
                logger.debug(f"âš ï¸  Error processing doc {doc_idx}: {e}")
                continue

        # æ’åº
        results.sort(key=lambda item: item["score"], reverse=True)

        logger.info(f"ğŸ“Š [æ£€ç´¢] æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè¿”å› top {top_k}")

        return results[:top_k]

    def _score_level(self, doc: dict, query_lower: str) -> float:
        """è¯„åˆ†ï¼šå­¦ä½çº§åˆ«åŒ¹é…"""
        if not doc or not isinstance(doc, dict):
            return 0.0
        
        level = str(doc.get("level", "")).lower()
        if not level:
            return 0.0

        score = 0.0
        postgraduate_markers = ("postgraduate", "msc", "master", "mres", "ma", "phd")
        undergraduate_markers = ("undergraduate", "ba", "bsc")

        try:
            if any(marker in query_lower for marker in ("msc", "master", "postgraduate", "graduate", "ç¡•å£«", "ç ”ç©¶ç”Ÿ")):
                if any(marker in level for marker in postgraduate_markers):
                    score += ScoringConfig.LEVEL_BOOST
                if any(marker in level for marker in undergraduate_markers):
                    score += ScoringConfig.LEVEL_PENALTY

            if any(marker in query_lower for marker in ("bsc", "ba", "undergraduate", "bachelor", "æœ¬ç§‘")):
                if any(marker in level for marker in undergraduate_markers):
                    score += ScoringConfig.LEVEL_BOOST
                if any(marker in level for marker in postgraduate_markers):
                    score += ScoringConfig.LEVEL_PENALTY
        except (TypeError, AttributeError):
            pass

        return score

    def _score_title_similarity(self, title_lower: str, keywords: List[str], query_lower: str) -> Tuple[float, List[str]]:
        """è¯„åˆ†ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦"""
        score = 0.0
        matches: List[str] = []

        if not title_lower or not isinstance(title_lower, str):
            return score, matches
        
        if not keywords:
            keywords = []

        try:
            main_title = title_lower.split("|")[0].strip()

            # å…³é”®è¯åŒ¹é…
            for kw in keywords:
                if not kw or not isinstance(kw, str) or len(kw) <= 2:
                    continue
                if kw in title_lower:
                    score += ScoringConfig.KEYWORD_IN_TITLE
                    if kw not in matches:
                        matches.append(kw)

            # å®Œå…¨æŸ¥è¯¢åŒ¹é…
            normalized_query = re.sub(r"\s+", " ", query_lower.strip())
            if normalized_query and normalized_query in title_lower:
                score += ScoringConfig.EXACT_QUERY_MATCH

            # åŒä¹‰è¯åŒ¹é…
            for phrase, synonyms in self.custom_synonyms.items():
                if not synonyms:
                    continue
                if any(syn and syn in query_lower for syn in synonyms) and phrase in title_lower:
                    score += ScoringConfig.SYNONYM_MATCH
                    if phrase not in matches:
                        matches.append(phrase)

            # æ¨¡ç³ŠåŒ¹é…
            ratio = SequenceMatcher(None, query_lower, title_lower).ratio()
            if ratio > 0.42:
                score += ratio * ScoringConfig.FUZZY_MATCH_MULTIPLIER

            # å­¦ä½æ ‡ç­¾åŒ¹é…
            for tag in ("msc", "mres", "ma", "pgdip", "bsc", "ba", "phd"):
                if tag in query_lower and tag in title_lower:
                    score += ScoringConfig.DEGREE_TAG_MATCH
                    if tag not in matches:
                        matches.append(tag)

        except Exception as e:
            logger.debug(f"âš ï¸  Title scoring error: {e}")

        return score, matches

    def _score_sections(self, sections: List[dict], keywords: List[str], intent: str) -> Tuple[float, List[dict]]:
        """è¯„åˆ†ï¼šç« èŠ‚å†…å®¹"""
        
        # ğŸ”¥ æ·»åŠ ç©ºå€¼æ£€æŸ¥
        if not sections or not isinstance(sections, list):
            return 0.0, []
        
        if not keywords:
            keywords = []
        
        score = 0.0
        relevant_sections: List[dict] = []
        target_headings = self.intent_headings.get(intent, [])

        for section in sections:
            if not section or not isinstance(section, dict):
                continue
            
            try:
                heading = section.get("heading") or ""
                text = section.get("text") or ""
                
                if not isinstance(heading, str):
                    heading = ""
                if not isinstance(text, str):
                    text = ""
                
                if not heading and not text:
                    continue

                heading_lower = heading.lower()
                text_lower = text.lower()
                section_score = 0.0
                reasons: List[str] = []

                # æ„å›¾ç›¸å…³æ ‡é¢˜åŒ¹é…
                for th in target_headings:
                    if th and isinstance(th, str) and th in heading_lower:
                        section_score += ScoringConfig.INTENT_HEADING_MATCH
                        reasons.append(f"intent_heading:{th}")

                # æ ‡é¢˜å…³é”®è¯åŒ¹é…
                for kw in keywords:
                    if kw and isinstance(kw, str) and kw in heading_lower:
                        section_score += ScoringConfig.KEYWORD_IN_HEADING
                        reasons.append(f"heading_kw:{kw}")

                # æ–‡æœ¬å…³é”®è¯åŒ¹é…
                for kw in keywords:
                    if not kw or not isinstance(kw, str):
                        continue
                    occurrences = text_lower.count(kw)
                    if occurrences:
                        kw_score = min(
                            occurrences * ScoringConfig.KEYWORD_IN_TEXT_PER_OCCURRENCE,
                            ScoringConfig.MAX_KEYWORD_TEXT_SCORE
                        )
                        section_score += kw_score
                        reasons.append(f"text_kw:{kw}x{occurrences}")

                # æ¨¡å—æ„å›¾ç‰¹æ®Šå¤„ç†
                if intent == "modules":
                    if "â€¢" in text or "\n-" in text or text.count("\n") > 5:
                        section_score += ScoringConfig.LIST_STRUCTURE_BONUS
                        reasons.append("list_structure")
                    if re.search(r"\b[A-Z]{4}\d{4}\b", text):
                        section_score += ScoringConfig.COURSE_CODE_BONUS
                        reasons.append("course_code")

                # é•¿åº¦å½’ä¸€åŒ–
                text_len = max(len(text), 1)
                normalization_factor = 1 + math.log10(text_len / ScoringConfig.SECTION_LENGTH_NORMALIZATION_BASE + 1)
                section_score /= normalization_factor

                if section_score > 0:
                    snippet = text.strip().replace("\u00a0", " ")[:1000]
                    relevant_sections.append({
                        "heading": heading,
                        "text": snippet,
                        "score": section_score,
                        "reasons": reasons,
                    })
                    score += section_score

            except Exception as e:
                logger.debug(f"âš ï¸  Section scoring error: {e}")
                continue

        relevant_sections.sort(key=lambda item: item["score"], reverse=True)
        return score, relevant_sections[:5]

    def _extract_smart_keywords(self, query: str) -> List[str]:
        """æ™ºèƒ½æå–æŸ¥è¯¢å…³é”®è¯"""
        if not query or not isinstance(query, str):
            return []
        
        stopwords = {
            "what", "how", "where", "when", "which", "who", "the", "a", "an",
            "is", "are", "was", "were", "do", "does", "did", "about", "for",
            "çš„", "æ˜¯", "æœ‰", "åœ¨", "å—", "å‘¢", "å•Š", "äº†",
        }

        try:
            query_lower = query.lower()

            # æå–è‹±æ–‡å•è¯
            english_words = re.findall(r"\b[a-z]+\b", query_lower)
            english_keywords = [word for word in english_words if word not in stopwords and len(word) > 2]

            # æå–ä¸­æ–‡å…³é”®è¯
            chinese_keywords: List[str] = []
            chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
            for chunk in chinese_matches:
                if HAVE_JIEBA:
                    chinese_keywords.extend([
                        token for token in jieba.cut(chunk) 
                        if len(token) > 1 and token not in stopwords
                    ])
                else:
                    chinese_keywords.extend([
                        chunk[i : i + j] 
                        for i in range(len(chunk)) 
                        for j in range(2, 5) 
                        if i + j <= len(chunk)
                    ])

            keywords: List[str] = english_keywords + chinese_keywords

            # æ‰©å±•å¸¸è§çŸ­è¯­
            for phrase, expansions in CommonPhrases.PHRASES.items():
                if phrase in query_lower:
                    keywords.extend(expansions)

            # å»é‡
            expanded: List[str] = []
            seen = set()
            for kw in keywords:
                if not kw or not isinstance(kw, str) or kw in seen:
                    continue
                expanded.append(kw)
                seen.add(kw)

            # è¿‡æ»¤é¢†åŸŸè¯æ±‡
            domain_filtered = [kw for kw in expanded if len(kw) > 3 or kw in self.domain_vocab]

            return domain_filtered or expanded

        except Exception as e:
            logger.error(f"âŒ Keyword extraction error: {e}")
            return []

    def _load_semantic_model(self) -> None:
        """åŠ è½½è¯­ä¹‰æ¨¡å‹"""
        if not HAVE_SEMANTIC:
            return

        try:
            self.semantic_model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
            logger.info("âœ… è¯­ä¹‰æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as exc:
            logger.warning(f"âš ï¸  è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {exc}")
            self.enable_semantic = False

    def _precompute_embeddings(self, documents: List[dict]) -> None:
        """é¢„è®¡ç®—æ–‡æ¡£ embeddings"""
        if not self.enable_semantic or self.semantic_model is None:
            return

        for doc_idx, doc in enumerate(documents):
            if not doc or not isinstance(doc, dict):
                continue
            
            if doc_idx in self._section_embeddings_cache:
                continue

            self._section_embeddings_cache[doc_idx] = {}
            sections = doc.get("sections", [])
            if not sections or not isinstance(sections, list):
                continue
            
            for sec_idx, section in enumerate(sections):
                if not section or not isinstance(section, dict):
                    continue
                text = section.get("text") or ""
                if text and isinstance(text, str) and len(text) > 50:
                    try:
                        emb = self.semantic_model.encode(text, convert_to_tensor=True)
                        self._section_embeddings_cache[doc_idx][sec_idx] = emb
                    except Exception as exc:
                        logger.debug(f"âš ï¸  è®¡ç®— embedding å¤±è´¥: {exc}")

    def _encode_query(self, query: str) -> Optional[Any]:
        """ç¼–ç æŸ¥è¯¢"""
        if not self.enable_semantic or self.semantic_model is None:
            return None

        try:
            return self.semantic_model.encode(query, convert_to_tensor=True)
        except Exception as exc:
            logger.warning(f"âš ï¸  æŸ¥è¯¢ç¼–ç å¤±è´¥: {exc}")
            return None

    def _semantic_boost(self, doc_idx: int, sections: List[dict], query_emb: Any) -> float:
        """è¯­ä¹‰ç›¸ä¼¼åº¦åŠ æˆ"""
        if not sections or query_emb is None:
            return 0.0

        best_sim = 0.0

        if doc_idx in self._section_embeddings_cache:
            for sec_idx, section_emb in self._section_embeddings_cache[doc_idx].items():
                try:
                    similarity = util.pytorch_cos_sim(query_emb, section_emb).item()
                    best_sim = max(best_sim, similarity)
                except Exception:
                    continue

        return best_sim * ScoringConfig.SEMANTIC_MULTIPLIER

    def _build_domain_vocab(self) -> set:
        """æ„å»ºé¢†åŸŸè¯æ±‡è¡¨"""
        repo_root = Path(__file__).resolve().parents[1]
        data_paths = [
            repo_root / "public" / "data" / "ucl_programs.json",
            repo_root / "public" / "data" / "ucl_services.json",
        ]

        words: List[str] = []
        for path in data_paths:
            if not path.exists():
                continue
            try:
                with path.open("r", encoding="utf-8") as fh:
                    data = json.load(fh)
                    if not data or not isinstance(data, list):
                        continue
                    for item in data:
                        if not item or not isinstance(item, dict):
                            continue
                        title = item.get("title", "")
                        if not title or not isinstance(title, str):
                            continue
                        tokens = re.findall(r"\b\w+\b|[\u4e00-\u9fff]+", title.lower())
                        words.extend(tokens)
            except Exception as exc:
                logger.warning(f"âš ï¸  è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {exc}")
                continue

        counter = Counter(words)
        vocab = {token for token, freq in counter.items() if freq > 1 and len(token) > 2}

        logger.info(f"ğŸ“š Built domain vocab with {len(vocab)} words")
        return vocab


def create_retriever(enable_semantic: bool = True, cache_embeddings: bool = True) -> EnhancedRetriever:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ£€ç´¢å™¨"""
    return EnhancedRetriever(enable_semantic=enable_semantic, cache_embeddings=cache_embeddings)

def search_duckduckgo(query, max_results=5):
    """DuckDuckGo æœç´¢å‡½æ•°"""
    try:
        # DuckDuckGo æœç´¢ API
        url = f"https://duckduckgo.com/html/?q={query}&kl=uk-en"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        results = []
        for result in soup.select('.result__body')[:max_results]:
            title = result.select_one('.result__title').get_text(strip=True)
            url = result.select_one('.result__url')['href'] if result.select_one('.result__url') else ''
            snippet = result.select_one('.result__snippet').get_text(strip=True) if result.select_one('.result__snippet') else ''
            
            results.append({
                'title': title,
                'url': url,
                'snippet': snippet,
                'source': 'web_search'
            })
        
        return results
    
    except Exception as e:
        print(f"âŒ ç½‘ç»œæœç´¢å¤±è´¥: {e}")
        return []

def search_local_documents(query, data, top_k=5):
    """
    å¢å¼ºç‰ˆæ–‡æ¡£æ£€ç´¢ - è‡ªåŠ¨å›é€€åˆ°ç½‘ç»œæœç´¢
    """
    print(f"ğŸ” æœ¬åœ°æ£€ç´¢: {query}")
    
    # 1. é¦–å…ˆå°è¯•æœ¬åœ°æ£€ç´¢
    documents = _search_local(query, data, top_k)
    
    # 2. å¦‚æœæœ¬åœ°æ²¡æœ‰æ‰¾åˆ°ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç½‘ç»œæœç´¢
    if not documents or len(documents) == 0:
        print("âš ï¸ æœ¬åœ°æ£€ç´¢æ— ç»“æœï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç½‘ç»œæœç´¢...")
        
        # ç½‘ç»œæœç´¢
        web_results = search_duckduckgo(query, max_results=top_k)
        
        if web_results:
            print(f"âœ… ç½‘ç»œæœç´¢æ‰¾åˆ° {len(web_results)} ä¸ªç»“æœ")
            
            # è½¬æ¢ä¸ºä¸æœ¬åœ°æ–‡æ¡£ç›¸åŒçš„æ ¼å¼
            documents = []
            for result in web_results:
                documents.append({
                    'title': result['title'],
                    'url': result['url'],
                    'sections': [{
                        'heading': 'Web Search Result',
                        'text': result['snippet']
                    }],
                    'source': 'web_search',
                    'score': 0.8  # ç»™ç½‘ç»œç»“æœä¸€ä¸ªä¸­ç­‰åˆ†æ•°
                })
        else:
            print("âŒ ç½‘ç»œæœç´¢ä¹Ÿæ— ç»“æœ")
    
    return documents

def _search_local(query, data, top_k):
    """åŸå§‹æœ¬åœ°æœç´¢é€»è¾‘"""
    # ä½ çš„ç°æœ‰æœ¬åœ°æœç´¢ä»£ç 
    # ... ä¿æŒä¸å˜
    pass