"""å¢å¼ºç‰ˆæ£€ç´¢å™¨ - æ™ºèƒ½è¯­ä¹‰æ£€ç´¢ä¸æ„å›¾è¯†åˆ«"""
import re
import logging
from typing import List, Dict, Tuple
from collections import Counter
import json
import os
from pathlib import Path

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ sentence-transformers ç”¨äºè¯­ä¹‰æœç´¢
try:
    from sentence_transformers import SentenceTransformer, util
    HAVE_SEMANTIC = True
    logger.info("âœ… sentence-transformers loaded for semantic search")
except ImportError:
    HAVE_SEMANTIC = False
    logger.warning("âš ï¸ sentence-transformers not available, semantic search disabled")

# å°è¯•å¯¼å…¥ jieba ç”¨äºä¸­æ–‡åˆ†è¯
try:
    import jieba
    HAVE_JIEBA = True
    logger.info("âœ… jieba loaded for Chinese tokenization")
except ImportError:
    HAVE_JIEBA = False
    logger.warning("âš ï¸ jieba not available, fallback to regex for Chinese")

class EnhancedRetriever:
    """å¢å¼ºç‰ˆæ£€ç´¢å™¨"""
    
    def __init__(self):
        self.intent_keywords = {
            'modules': ['module', 'course', 'subject', 'curriculum', 'syllabus', 'teaching', 'learn', 'è¯¾ç¨‹', 'æ¨¡å—'],
            'requirements': ['requirement', 'prerequisite', 'entry', 'admission', 'qualification', 'ielts', 'toefl', 'è¦æ±‚', 'ç”³è¯·'],
            'career': ['career', 'job', 'employment', 'graduate', 'prospect', 'å°±ä¸š', 'èŒä¸š'],
            'fees': ['fee', 'tuition', 'cost', 'funding', 'scholarship', 'å­¦è´¹', 'è´¹ç”¨'],
        }
        
        # æ‰©å±• intent_headings
        self.intent_headings = {
            'modules': ['module', 'curriculum', 'syllabus', 'teaching', 'what you will learn', 'course structure', 'compulsory', 'optional', 'supervisor', 'placement', 'research topic'],
            'requirements': ['entry', 'requirement', 'admission', 'english', 'qualification', 'ielts', 'toefl'],
            'career': ['career', 'employment', 'graduate', 'prospects', 'outcomes'],
            'fees': ['fee', 'tuition', 'cost', 'funding'],
        }
        
        # è¯­ä¹‰æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if HAVE_SEMANTIC:
            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
            self.embedding_cache = {}  # ç¼“å­˜: (doc_id, section_id) -> embedding
            self.embedding_cache_file = Path(__file__).parent / 'embedding_cache.json'
            self._load_embedding_cache()
        else:
            self.semantic_model = None
        
        # é¢†åŸŸè¯è¡¨å’ŒåŒä¹‰è¯
        self.domain_vocab = set()  # é«˜é¢‘è¯
        self.custom_synonyms = {
            'data science': ['data science', 'æ•°æ®ç§‘å­¦', 'analytics', 'å¤§æ•°æ®'],
            'management': ['management', 'ç®¡ç†', 'business', 'å•†ä¸š'],
            # å¯ä»¥æ‰‹åŠ¨æ·»åŠ æ›´å¤šé¢†åŸŸç‰¹å®š
        }
        self._build_domain_vocab()  # å°è¯•ä» JSON æ„å»º
        
    def _build_domain_vocab(self):
        """ä» JSON æ–‡æ¡£æ ‡é¢˜ç»Ÿè®¡é«˜é¢‘è¯"""
        # ğŸ”¥ ä¿®å¤è·¯å¾„
        root = Path(__file__).resolve().parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
        paths = [
            root / "public" / "data" / "ucl_programs.json",
            root / "public" / "data" / "ucl_services.json"
        ]
        
        titles = []
        for path in paths:
            if path.exists():
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        titles.extend([d.get('title', '') for d in data])
                        logger.info(f"âœ… Loaded {len(data)} titles from {path.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to load {path} for vocab: {e}")
            else:
                logger.warning(f"âš ï¸ File not found: {path}")
        
        if not titles:
            logger.warning("âš ï¸ No titles loaded, using empty vocab")
            return
        
        # ç»Ÿè®¡é«˜é¢‘è¯
        all_words = []
        for title in titles:
            words = re.findall(r'\b\w+\b|[\u4e00-\u9fff]+', title.lower())
            all_words.extend(words)
        
        counter = Counter(all_words)
        # ğŸ”¥ é™ä½é˜ˆå€¼ï¼Œå¢åŠ è¯è¡¨
        self.domain_vocab = {w for w, c in counter.items() if c >= 1 and len(w) > 2}
        
        # æ·»åŠ åˆ° custom_synonyms
        for word in list(self.domain_vocab)[:100]:  # å¢åŠ åˆ° 100
            if word not in self.custom_synonyms:
                self.custom_synonyms[word] = [word]
        
        logger.info(f"ğŸ“š Built domain vocab with {len(self.domain_vocab)} words")
    
    def _load_embedding_cache(self):
        """åŠ è½½ embedding ç¼“å­˜"""
        if self.embedding_cache_file.exists():
            try:
                with open(self.embedding_cache_file, 'r') as f:
                    self.embedding_cache = json.load(f)
                logger.info(f"âœ… Loaded {len(self.embedding_cache)} embeddings from cache")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load embedding cache: {e}")
    
    def _save_embedding_cache(self):
        """ä¿å­˜ embedding ç¼“å­˜"""
        try:
            with open(self.embedding_cache_file, 'w') as f:
                json.dump(self.embedding_cache, f)
            logger.info(f"ğŸ’¾ Saved {len(self.embedding_cache)} embeddings to cache")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save embedding cache: {e}")
    
    def detect_intent(self, query: str) -> str:
        """æ£€æµ‹æŸ¥è¯¢æ„å›¾"""
        query_lower = query.lower()
        for intent, keywords in self.intent_keywords.items():
            if any(kw in query_lower for kw in keywords):
                return intent
        return 'general'
    
    def search_with_context(self, query: str, documents: List[dict], top_k: int = 8) -> List[Dict]:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ™ºèƒ½æ£€ç´¢"""
        intent = self.detect_intent(query)
        query_lower = query.lower()
        
        # æå–å…³é”®è¯
        keywords = self._extract_smart_keywords(query)
        
        logger.info(f"ğŸ” [æ£€ç´¢] æŸ¥è¯¢: {query}")
        logger.info(f"ğŸ¯ [æ£€ç´¢] æ„å›¾: {intent}")
        logger.info(f"ğŸ”‘ [æ£€ç´¢] å…³é”®è¯: {keywords}")
        
        # å¦‚æœæœ‰è¯­ä¹‰æ¨¡å‹ï¼Œè®¡ç®— query embedding
        query_emb = None
        if HAVE_SEMANTIC:
            query_emb = self.semantic_model.encode(query, convert_to_tensor=True)
        
        results = []
        
        for doc_idx, doc in enumerate(documents):
            score = 0
            matched_sections = []
            
            title = doc.get('title', '').lower()
            level = doc.get('level', '').lower() if doc.get('level') else ''
            
            # 1. æ ‡é¢˜åŒ¹é…ï¼ˆæƒé‡æœ€é«˜ï¼‰
            title_score = 0
            for kw in keywords:
                if kw in title:
                    title_score += 15
            
            # ç²¾ç¡®åŒ¹é…åŠ å€
            if query_lower in title:
                title_score += 30
            
            score += title_score
            
            # 2. Level åŒ¹é…ï¼ˆç¡•å£«è¯¾ç¨‹ä¼˜å…ˆï¼‰
            if level and ('postgraduate' in level or 'msc' in level.lower()):
                if 'master' in query_lower or 'msc' in query_lower or 'ç¡•å£«' in query_lower:
                    score += 10
            
            # 3. Sections æ·±åº¦åŒ¹é…ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
            sections_score, relevant_sections = self._score_sections(
                doc.get('sections', []), 
                keywords, 
                intent
            )
            score += sections_score
            matched_sections = relevant_sections
            
            # 4. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            semantic_score = 0
            if HAVE_SEMANTIC and query_emb is not None:
                doc_semantic_score = 0
                for sec_idx, section in enumerate(doc.get('sections', [])):
                    text = section.get('text', '')
                    if not text:
                        continue
                    cache_key = f"{doc_idx}_{sec_idx}"
                    if cache_key not in self.embedding_cache:
                        sec_emb = self.semantic_model.encode(text, convert_to_tensor=True)
                        self.embedding_cache[cache_key] = sec_emb.tolist()  # ä¿å­˜ä¸º list
                    else:
                        sec_emb = util.pytorch_cos_sim(query_emb, torch.tensor(self.embedding_cache[cache_key]))
                    
                    sim = util.pytorch_cos_sim(query_emb, sec_emb).item()
                    doc_semantic_score = max(doc_semantic_score, sim)
                
                # å½’ä¸€åŒ–åˆ° 0-100
                semantic_score = int(doc_semantic_score * 100)
                score += semantic_score  # çº¿æ€§ç»„åˆ
            
            if score > 0 or semantic_score > 30:  # å³ä½¿ heuristic score=0ï¼Œå¦‚æœè¯­ä¹‰>é˜ˆå€¼ä¹Ÿçº³å…¥
                results.append({
                    'doc': doc,
                    'score': score,
                    'title_score': title_score,
                    'sections_score': sections_score,
                    'semantic_score': semantic_score,
                    'matched_sections': matched_sections,
                    'intent': intent
                })
        
        # ä¿å­˜ç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨äº†è¯­ä¹‰ï¼‰
        if HAVE_SEMANTIC:
            self._save_embedding_cache()
        
        # æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        
        logger.info(f"ğŸ“Š [æ£€ç´¢] æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè¿”å› top {top_k}")
        
        return results[:top_k]
    
    def _score_sections(self, sections: List[dict], keywords: List[str], intent: str) -> Tuple[int, List[dict]]:
        """å¯¹sectionsè¿›è¡Œæ™ºèƒ½è¯„åˆ†"""
        score = 0
        relevant_sections = []
        
        target_headings = self.intent_headings.get(intent, [])
        
        for section in sections:
            heading = section.get('heading', '').lower()
            text = section.get('text', '').lower()
            section_score = 0
            reasons = []
            
            # 1. Heading åŒ¹é…æ„å›¾ï¼ˆé«˜æƒé‡ï¼‰
            for th in target_headings:
                if th in heading:
                    section_score += 20
                    reasons.append(f"Heading match: {th}")
            
            # 2. Heading åŒ¹é…å…³é”®è¯
            for kw in keywords:
                if kw in heading:
                    section_score += 10
                    reasons.append(f"Keyword in heading: {kw}")
            
            # 3. Text å†…å®¹åŒ¹é…
            for kw in keywords:
                count = text.count(kw)
                if count > 0:
                    section_score += min(count * 3, 15)  # å•ä¸ªå…³é”®è¯æœ€å¤šè´¡çŒ®15åˆ†
                    reasons.append(f"Keyword in text: {kw} (count: {count})")
            
            # 4. ç‰¹æ®ŠåŠ æƒï¼ˆmodules ç›¸å…³ï¼‰
            if intent == 'modules':
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ—è¡¨ç»“æ„
                if 'â€¢' in text or '\n-' in text or text.count('\n') > 5:
                    section_score += 15
                    reasons.append("List structure detected")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¾ç¨‹ä»£ç ï¼ˆå¦‚ COMP0001ï¼‰
                if re.search(r'\b[A-Z]{4}\d{4}\b', text):
                    section_score += 10
                    reasons.append("Course code detected")
            
            # å½’ä¸€åŒ–ï¼šé™¤ä»¥é•¿åº¦ï¼ˆé¿å…é•¿æ–‡æœ¬åé«˜ï¼‰
            text_len = len(text)
            if text_len > 0:
                section_score /= max(1, text_len / 100)  # æ¯100å­—ç¬¦å½’ä¸€
            
            if section_score > 0:
                score += section_score
                relevant_sections.append({
                    'heading': section.get('heading', ''),
                    'text': section.get('text', '')[:800],  # ä¿ç•™å‰800å­—ç¬¦
                    'score': section_score,
                    'reasons': reasons  # æ–°å¢ reasons
                })
        
        # æŒ‰åˆ†æ•°æ’åºç›¸å…³sections
        relevant_sections.sort(key=lambda x: x['score'], reverse=True)
        
        return score, relevant_sections[:5]  # æœ€å¤šè¿”å›5ä¸ªæœ€ç›¸å…³çš„section
    
    def _extract_smart_keywords(self, query: str) -> List[str]:
        """æ™ºèƒ½æå–å…³é”®è¯"""
        # åœç”¨è¯
        stopwords = {
            'what', 'how', 'where', 'when', 'which', 'who', 'the', 'a', 'an', 
            'is', 'are', 'was', 'were', 'do', 'does', 'did', 'about', 'for',
            'çš„', 'æ˜¯', 'æœ‰', 'åœ¨', 'å—', 'å‘¢', 'å•Š', 'äº†'
        }
        
        keywords = []
        
        # ä¸­æ–‡å¤„ç†
        chinese_parts = re.findall(r'[\u4e00-\u9fff]+', query)
        for part in chinese_parts:
            if HAVE_JIEBA:
                segs = jieba.cut(part)
                keywords.extend([s for s in segs if s not in stopwords and len(s) > 1])
            else:
                # Fallback: 2-4å­—çª—å£
                for i in range(len(part)):
                    for j in range(2, 5):
                        if i + j <= len(part):
                            word = part[i:i+j]
                            if word not in stopwords:
                                keywords.append(word)
        
        # è‹±æ–‡å•è¯
        english_words = re.findall(r'\b[a-z]+\b', query.lower())
        keywords.extend([w for w in english_words if w not in stopwords and len(w) > 2])
        
        # è¿‡æ»¤åˆ°é¢†åŸŸè¯è¡¨
        keywords = [kw for kw in keywords if kw in self.domain_vocab or len(kw) > 3]
        
        # åŒä¹‰è¯æ‰©å±•ï¼ˆä¿ç•™åŸå§‹é¡ºåºï¼‰
        expanded = []
        seen = set()
        for kw in keywords:
            if kw in seen:
                continue
            expanded.append(kw)
            seen.add(kw)
            if kw in self.custom_synonyms:
                for syn in self.custom_synonyms[kw]:
                    if syn != kw and syn not in seen:
                        expanded.append(syn)
                        seen.add(syn)
        
        return expanded
