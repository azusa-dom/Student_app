#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
qa_enhanced_wrapper.py - ç»ˆæä¼˜åŒ–ç‰ˆ (ä¸­æ–‡æœç´¢ä¿®å¤)
ä¿®å¤ï¼š
1. [å…³é”®] ä¿®å¤ _extract_program_nameï¼Œå¢åŠ ä¸­æ–‡åˆ«åæ˜ å°„ (å¦‚ "è®¡ç®—æœº" -> "computer science")
2. [å…³é”®] ä¿®å¤ _extract_keywordsï¼Œå¼•å…¥ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯ (æ›¿æ¢æ‰æ— ç”¨çš„ n-gram)
3. [å…³é”®] ä¼˜åŒ– Web æœç´¢é€»è¾‘ï¼Œå½“æ£€æµ‹åˆ°ä¸­æ–‡æŸ¥è¯¢+ç¨‹åºåˆ«åæ—¶ï¼Œä¼˜å…ˆæœç´¢è‹±æ–‡ä»¥æé«˜æˆåŠŸç‡ã€‚
4. [å…³é”®] é‡å†™ LLM æç¤ºè¯ï¼Œè§£å†³â€œå›ç­”æ„šè ¢â€ (Data Science) å’Œâ€œå›ç­”æ··ä¹±â€ (Computer Science) çš„é—®é¢˜ã€‚
"""

import os
import re
import json
import time
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional
from collections import defaultdict

# åŸºç¡€æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("qa_wrapper")

# å¯¼å…¥ Web æœç´¢
try:
    from scripts.web_search import search_web
    HAVE_WEB_SEARCH = True
    logger.info("âœ… web_search æ¨¡å—åŠ è½½æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ web_search æ¨¡å—åŠ è½½å¤±è´¥: {e}")
    HAVE_WEB_SEARCH = False

# âœ… [æœ¬æ¬¡ä¿®å¤] å¯¼å…¥ Jieba
try:
    import jieba
    HAVE_JIEBA = True
    logger.info("âœ… jieba åŠ è½½æˆåŠŸ")
except ImportError:
    HAVE_JIEBA = False
    logger.warning("âš ï¸ jieba æœªå®‰è£…, ä¸­æ–‡åˆ†è¯å°†å›é€€åˆ° n-gram")


# è·¯å¾„é…ç½®
ROOT = Path(__file__).resolve().parents[1] if "__file__" in globals() else Path.cwd()
PROGRAMS_PATH = ROOT / "public" / "data" / "ucl_programs.json"
SERVICES_PATH = ROOT / "public" / "data" / "ucl_services.json"

# å…¨å±€ç¼“å­˜
_GLOBAL_DOCS: List[Dict] = []
_PROGRAM_INDEX: Dict[str, List[Dict]] = {} 

# âœ… [æœ¬æ¬¡ä¿®å¤] ä¸­æ–‡åˆ«åæ˜ å°„
PROGRAM_ALIASES = {
    "computer science": ["computer science", "è®¡ç®—æœºç§‘å­¦", "è®¡ç®—æœº"],
    "data science": ["data science", "æ•°æ®ç§‘å­¦"],
    "intercultural communication": ["intercultural communication", "è·¨æ–‡åŒ–äº¤æµ"],
    "global health": ["global health", "å…¨çƒå¥åº·", "å…¨çƒå¥åº·ç®¡ç†"],
    "language requirements": ["language requirements", "è¯­è¨€è¦æ±‚", "é›…æ€", "ielts", "toefl", "æ‰˜ç¦"],
    "entry requirements": ["entry requirements", "å…¥å­¦è¦æ±‚"],
    "modules": ["modules", "è¯¾ç¨‹", "æ¨¡å—"],
    "fees": ["fees", "tuition", "å­¦è´¹"],
}


def _build_program_index(docs: List[Dict]) -> Dict[str, List[Dict]]:
    """æ„å»ºä¸“ä¸šåç§°ç´¢å¼•ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾"""
    index = defaultdict(list)
    
    for doc in docs:
        title = (doc.get("title") or "").lower()
        
        # å®Œæ•´æ ‡é¢˜
        index[title].append(doc)
        
        # åˆ†è¯ç´¢å¼•ï¼ˆå¦‚ "data science msc" -> "data", "science", "msc"ï¼‰
        words = title.split()
        for word in words:
            if len(word) > 2:
                index[word].append(doc)
        
        # ç¼©å†™ç´¢å¼•
        if "msc" in title: index["msc"].append(doc); index["master"].append(doc)
        if "ma" in title: index["ma"].append(doc); index["master"].append(doc)
        if "bsc" in title: index["bsc"].append(doc); index["bachelor"].append(doc)
        if "ba" in title: index["ba"].append(doc); index["bachelor"].append(doc)
        
        # ç‰¹æ®Šä¸“ä¸šå…³é”®è¯
        for internal_name, aliases in PROGRAM_ALIASES.items():
            if any(alias in title for alias in aliases if len(alias) > 3): # é¿å… "ba" åŒ¹é… "data"
                for kw in internal_name.split(): # ä½¿ç”¨å†…éƒ¨è‹±æ–‡åå»ºç«‹ç´¢å¼•
                    index[kw].append(doc)
    
    return dict(index)

def _load_documents() -> List[Dict]:
    """åŠ è½½æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•"""
    global _GLOBAL_DOCS, _PROGRAM_INDEX
    
    if _GLOBAL_DOCS:
        return _GLOBAL_DOCS
    
    docs = []
    for path in (PROGRAMS_PATH, SERVICES_PATH):
        if path.exists():
            try:
                data = json.loads(path.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    docs.extend(data)
                    logger.info(f"âœ… åŠ è½½ {path.name}: {len(data)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ {path.name} å¤±è´¥: {e}")
    
    logger.info(f"ğŸ“š æ€»å…±åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    
    _GLOBAL_DOCS = docs
    _PROGRAM_INDEX = _build_program_index(docs)
    logger.info(f"ğŸ“‘ å»ºç«‹ç´¢å¼•: {len(_PROGRAM_INDEX)} ä¸ªå…³é”®è¯")
    
    return docs

def _extract_program_name(query: str) -> Optional[str]:
    """âœ… [æœ¬æ¬¡ä¿®å¤] ä»æŸ¥è¯¢ä¸­æå–ä¸“ä¸šåç§° (æ”¯æŒä¸­æ–‡)"""
    query_lower = query.lower()
    
    # 1. æ£€æŸ¥ä¸­æ–‡åˆ«å
    for internal_name, aliases in PROGRAM_ALIASES.items():
        if any(alias in query_lower for alias in aliases):
            logger.info(f"ğŸ’¡ æ£€æµ‹åˆ°åˆ«åï¼ŒåŒ¹é…åˆ°: {internal_name}")
            return internal_name # è¿”å›å†…éƒ¨è‹±æ–‡å

    # 2. æ£€æŸ¥è‹±æ–‡æ¨¡å¼
    patterns = [
        r"([\w\s]+)\s+(msc|ma|bsc|ba|phd)",  # "Data Science MSc"
        r"(msc|ma|bsc|ba|phd)\s+in\s+([\w\s]+)",  # "MSc in Data Science"
        r"([\w\s]+)\s+(master|bachelor|doctorate)",  # "Data Science Master"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, query_lower)
        if match:
            # è¿”å›åŒ¹é…åˆ°çš„çŸ­è¯­ï¼Œä¾‹å¦‚ "data science msc"
            return match.group(0).strip()
    
    return None


def _smart_search(query: str, docs: List[Dict], top_k: int = 10) -> List[Dict]:
    """æ™ºèƒ½æœç´¢ï¼šç»“åˆç´¢å¼•æŸ¥æ‰¾å’Œç›¸å…³æ€§è¯„åˆ†"""
    query_lower = query.lower()
    results = []
    seen_titles = set()
    
    # 1. é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ä¸“ä¸šåç§°
    program_name = _extract_program_name(query)
    program_words = set((program_name or "").split())
    
    if program_name:
        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ä¸“ä¸šåç§°: {program_name}")
        
        candidate_docs = [] 
        for word in program_words:
            if word in _PROGRAM_INDEX and _PROGRAM_INDEX[word]:
                candidate_docs.extend(_PROGRAM_INDEX[word]) 
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(candidate_docs)} ä¸ªä¸“ä¸šç›¸å…³çš„å€™é€‰")

        for doc in candidate_docs:
            title = doc.get("title", "")
            if not title or title in seen_titles:
                continue
            
            seen_titles.add(title)
            title_lower = title.lower()
            
            score = 0
            # [æ–°] æ£€æŸ¥å†…éƒ¨åç§°æ˜¯å¦åœ¨æ ‡é¢˜ä¸­
            if all(word in title_lower for word in program_words):
                score = 100
            elif program_name in title_lower:
                score = 90
            
            if score > 0:
                results.append({
                    "doc": doc,
                    "score": score + _calculate_relevance(doc, [], query), # åŠ ä¸Šå†…å®¹åˆ†
                    "matched_sections": _extract_relevant_sections(doc, query, program_name) # ä¼ å…¥ program_name
                })

    
    # 2. å…³é”®è¯æœç´¢
    keywords = _extract_keywords(query)
    logger.info(f"ğŸ“ å…³é”®è¯: {keywords[:10]}")
    
    for doc in docs:
        title = doc.get("title", "")
        if not title or title in seen_titles:
            continue
        
        score = _calculate_relevance(doc, keywords, query)
        if score > 0:
            results.append({
                "doc": doc,
                "score": score,
                "matched_sections": _extract_relevant_sections(doc, query)
            })
            seen_titles.add(title)
    
    # 3. æ’åºå¹¶è¿”å›
    results.sort(key=lambda x: x["score"], reverse=True)
    final_results = results[:top_k]
    
    top_score = results[0]['score'] if results else 0
    logger.info(f"âœ… æ‰¾åˆ° {len(final_results)} ä¸ªç›¸å…³ç»“æœ (Top score: {top_score})")
    return final_results

def _extract_keywords(query: str) -> List[str]:
    """âœ… [æœ¬æ¬¡ä¿®å¤] æå–æŸ¥è¯¢å…³é”®è¯ (ä½¿ç”¨ Jieba)"""
    stopwords = {
        "what", "how", "where", "when", "which", "who", "the", "a", "an",
        "is", "are", "was", "were", "do", "does", "did", "about", "for",
        "of", "in", "on", "at", "to",
        "çš„", "æ˜¯", "æœ‰", "åœ¨", "å—", "å‘¢", "å•Š", "äº†", "å’Œ", "ä¸", "ä»€ä¹ˆ", "è¦æ±‚", "ä¸“ä¸š"
    }
    
    keywords = []
    query_lower = query.lower()
    
    # æå–è‹±æ–‡å•è¯
    english_words = re.findall(r'\b[a-z]{3,}\b', query_lower) # è‡³å°‘3ä¸ªå­—æ¯
    keywords.extend([w for w in english_words if w not in stopwords])
    
    # âœ… [ä¿®å¤] ä½¿ç”¨ Jieba æå–ä¸­æ–‡å…³é”®è¯
    chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
    if HAVE_JIEBA:
        for chunk in chinese_matches:
            keywords.extend([
                token for token in jieba.cut(chunk) 
                if len(token) > 1 and token not in stopwords
            ])
    else:
        # é™çº§ï¼šå¦‚æœ jieba å¤±è´¥
        for chunk in chinese_matches:
            for i in range(len(chunk) - 1):
                keywords.append(chunk[i:i+2])

    # æ·»åŠ ä¸“ä¸šç›¸å…³æ‰©å±•è¯ (åŸºäºè‹±æ–‡æ„å›¾)
    intent = _detect_intent(query_lower)
    if intent == "modules":
        keywords.extend(["compulsory", "core", "optional", "elective", "curriculum"])
    if intent in ("requirements", "language_requirements"):
        keywords.extend(["ielts", "toefl", "gpa", "degree", "qualification", "a-level"])
    if intent == "fees":
        keywords.extend(["scholarship", "funding", "payment", "international", "uk", "overseas"])
    
    return list(set(keywords))[:30]  # é™åˆ¶å…³é”®è¯æ•°é‡

def _calculate_relevance(doc: Dict, keywords: List[str], query: str) -> float:
    """è®¡ç®—æ–‡æ¡£ç›¸å…³æ€§å¾—åˆ†"""
    score = 0.0
    query_lower = query.lower()
    
    title = (doc.get("title") or "").lower()
    
    # å…³é”®è¯åŒ¹é…
    for kw in keywords:
        if kw in title:
            score += 5
            if f" {kw} " in f" {title} ":
                score += 3
    
    # çº§åˆ«åŒ¹é…
    level = str(doc.get("level", "")).lower()
    if "msc" in query_lower or "master" in query_lower or "ç¡•å£«" in query:
        if "msc" in level or "master" in level:
            score += 10
    elif "bsc" in query_lower or "bachelor" in query_lower or "æœ¬ç§‘" in query:
        if "bsc" in level or "bachelor" in level:
            score += 10
    
    # å†…å®¹åŒ¹é…
    sections = doc.get("sections", [])
    for section in sections[:20]:
        if not isinstance(section, dict): continue
        heading = (section.get("heading") or "").lower()
        text = (section.get("text") or "").lower()
        
        section_score = 0
        for kw in keywords[:15]:
            if kw in heading: section_score += 3
            if kw in text: section_score += 0.5
        
        score += min(section_score, 10)
    
    return score

def _extract_relevant_sections(doc: Dict, query: str, program_name: Optional[str] = None) -> List[Dict]:
    """æå–ç›¸å…³ç« èŠ‚"""
    query_lower = query.lower()
    keywords = _extract_keywords(query)
    relevant_sections = []
    
    sections = doc.get("sections", [])
    
    # [æ–°] ä¼˜å…ˆç« èŠ‚ (åŒ¹é…æŸ¥è¯¢æ„å›¾)
    priority_headings = []
    priority_terms = {
        "module": ["module", "course", "curriculum", "syllabus", "è¯¾ç¨‹", "æ¨¡å—", "compulsory", "optional"],
        "requirement": ["requirement", "entry", "admission", "qualification", "è¦æ±‚", "å…¥å­¦", "a-level", "ib diploma"],
        "fee": ["fee", "tuition", "cost", "scholarship", "å­¦è´¹", "è´¹ç”¨", "funding"],
        "language_requirement": ["ielts", "toefl", "language", "english", "è¯­è¨€", "é›…æ€", "æ‰˜ç¦"],
        "career": ["career", "employment", "job", "prospect", "å°±ä¸š", "èŒä¸š"],
    }
    
    intent = _detect_intent(query_lower)
    if intent in priority_terms:
        priority_headings = priority_terms[intent]

    # âœ… [æ–°] å¦‚æœæ˜¯ç²¾ç¡®çš„ä¸“ä¸šæœç´¢ï¼Œ"About this degree" ä¹Ÿæ˜¯é«˜ä¼˜å…ˆçº§
    if program_name:
        priority_headings.append("about this degree")

    for section in sections:
        if not isinstance(section, dict):
            continue
        
        heading = section.get("heading", "")
        text = section.get("text", "")
        
        if not text:
            continue
        
        relevance = 0
        heading_lower = heading.lower()
        text_lower = text.lower()
        
        # 1. æ„å›¾åŒ¹é… (æœ€é«˜æƒé‡)
        if any(term in heading_lower for term in priority_headings):
            relevance += 20
        
        # 2. å…³é”®è¯åŒ¹é…
        for kw in keywords[:10]:
            if kw in heading_lower:
                relevance += 3
            if kw in text_lower:
                relevance += 1
        
        if relevance > 0:
            relevant_sections.append({
                "heading": heading,
                "text": text[:800],
                "score": relevance
            })
    
    relevant_sections.sort(key=lambda x: x["score"], reverse=True)
    return relevant_sections[:5] 

def _format_context_for_llm(results: List[Dict]) -> str:
    """ä¸ºLLMå‡†å¤‡æœ¬åœ°æ–‡æ¡£çš„ä¸Šä¸‹æ–‡ (ç§»é™¤ 'æœ¬åœ°' å­—æ ·)"""
    if not results:
        return ""
    
    context_parts = []
    
    for i, result in enumerate(results[:3], 1): # åªå– Top 3 æ–‡æ¡£
        doc = result.get("doc", {})
        title = doc.get("title", "")
        sections = result.get("matched_sections", [])
        
        header = f"\nã€{title}ã€‘"
        context_parts.append(header)
        
        if sections:
            context_parts.append("ç›¸å…³å†…å®¹:")
            for section in sections[:3]:
                heading = section.get("heading", "")
                text = section.get("text", "").strip()
                
                if heading:
                    context_parts.append(f"\nâ–¸ {heading}")
                
                if text:
                    text = text.replace("\u00a0", " ")
                    text = re.sub(r'\s+', ' ', text)
                    text = text[:600]
                    context_parts.append(f"  {text}")
        
        context_parts.append("")
    
    return "\n".join(context_parts)

def _pick_best_snippets_from_web(results: List[Dict]) -> str:
    """ä¸ºLLMå‡†å¤‡ç½‘ç»œæœç´¢çš„ä¸Šä¸‹æ–‡"""
    if not results:
        return ""
    
    parts = []
    for i, r in enumerate(results[:3], 1):
        title = r.get("title", "")
        snippet = r.get("snippet", "")
        url = r.get("url", "")
        if title and snippet:
            parts.append(f"\nã€{title}ã€‘\né“¾æ¥: {url}\næ‘˜è¦: {snippet}")
    
    return "\n".join(parts)[:1500]

def _generate_comprehensive_answer(context: str, query: str, language: str, has_high_score_local: bool) -> str:
    """
    âœ… [æœ¬æ¬¡ä¿®å¤] åŠ¨æ€æç¤ºè¯
    """
    
    if not context:
        if language == "zh":
            return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ä¸æ‚¨æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯æˆ–æŸ¥è¯¢å…¶ä»–ä¸“ä¸šã€‚"
        else:
            return "Sorry, no relevant information found. Please try with more specific keywords or search for other programs."
    
    if os.getenv("GROQ_API_KEY"):
        try:
            from scripts.llm_client import chat_with_groq
            
            # âœ… [æ–°] åŠ¨æ€é€‰æ‹©æç¤ºè¯
            if has_high_score_local:
                # åœºæ™¯ 1: æœ¬åœ°æ‰¾åˆ°äº†é«˜åˆ†åŒ¹é… (å¦‚ "Data Science MSc")
                # ä½¿ç”¨ä¸¥æ ¼çš„ã€èšç„¦çš„æç¤ºè¯
                logger.info("ğŸ¤– ä½¿ç”¨ [é«˜åˆ†èšç„¦] æç¤ºè¯...")
                if language == "zh":
                    system_prompt = """ä½ æ˜¯UCLï¼ˆä¼¦æ•¦å¤§å­¦å­¦é™¢ï¼‰çš„AIåŠ©æ‰‹ã€‚
ä½ çš„**é¦–è¦ä»»åŠ¡**æ˜¯ç²¾å‡†å›ç­”ç”¨æˆ·å…³äº**ç‰¹å®šä¸“ä¸š**çš„æŸ¥è¯¢ã€‚
ä¸Šä¸‹æ–‡å¯èƒ½åŒ…å«å¤šä¸ªæ–‡æ¡£ï¼Œä½ **å¿…é¡»**ä¼˜å…ˆä½¿ç”¨æ ‡é¢˜ä¸ç”¨æˆ·æŸ¥è¯¢æœ€åŒ¹é…çš„æ–‡æ¡£ã€‚

**è§„åˆ™ï¼š**
1.  **èšç„¦ç­”æ¡ˆ**ï¼šå¦‚æœç”¨æˆ·è¯¢é—®ç‰¹å®šä¸“ä¸šï¼ˆå¦‚ "è®¡ç®—æœºç§‘å­¦"ï¼‰ï¼Œä½ å¿…é¡»åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ° `ã€Computer Science BScã€‘` æˆ– `ã€Data Science MScã€‘` è¿™æ ·çš„æ ‡é¢˜ï¼Œå¹¶**ä»…åŸºäºè¯¥æ–‡æ¡£çš„ä¿¡æ¯**è¿›è¡Œå›ç­”ã€‚
2.  **å¿½ç•¥æ— å…³é¡¹**ï¼š**ç»å¯¹ç¦æ­¢**ç½—åˆ—ä¸Šä¸‹æ–‡ä¸­å…¶ä»–ä¸ç›¸å…³ä¸“ä¸šçš„ä¿¡æ¯ï¼ˆæ¯”å¦‚å½“ç”¨æˆ·é—® "è®¡ç®—æœºç§‘å­¦" æ—¶ï¼Œç¦æ­¢æåŠ "Urban Spatial Science" æˆ– "Systems Engineering"ï¼‰ã€‚
3.  **å¤„ç†åæ•°æ® (å…³é”®)**ï¼šå¦‚æœä¸€ä¸ªæ–‡æ¡£ä¸­åŒæ—¶åŒ…å« `Compulsory modules` (æ¨¡å—åˆ—è¡¨) å’Œ `Teaching and learning` (æ•™å­¦æè¿°)ï¼Œä½  **å¿…é¡»** ä¼˜å…ˆä½¿ç”¨ `Compulsory modules` é‡Œçš„åˆ—è¡¨ã€‚**å¿…é¡»å¿½ç•¥** `Teaching and learning` ä¸­å…³äº "one compulsory module" ä¹‹ç±»çš„æ¨¡ç³Šæ€»ç»“ã€‚
4.  **æ ¼å¼**ï¼šä½¿ç”¨è¦ç‚¹ï¼Œå¹¶åœ¨è¦ç‚¹ä¹‹é—´æ·»åŠ ç©ºè¡Œ (`\n\n`)ã€‚
5.  **æ€åº¦**ï¼šç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦è¯´ "æ ¹æ®æ‰¾åˆ°çš„ä¿¡æ¯" æˆ– "æœ¬åœ°æ–‡æ¡£"ã€‚"""
                else:
                    system_prompt = """You are a UCL AI assistant.
Your **primary goal** is to answer the user's query about a **specific program**.
The context contains multiple documents. You **MUST** prioritize the document whose title *best matches* the user's query.

**Rules:**
1.  **Focus**: If the query is about "Computer Science", find the context section starting with `ã€Computer Science BScã€‘` or `ã€Data Science MScã€‘` and base your answer **ONLY** on that document.
2.  **Ignore Noise**: **Strictly forbid** summarizing other irrelevant programs (e.g., do not mention "Urban Spatial Science" when asked about "Computer Science").
3.  **Handle Bad Data (Critical)**: If a document contains both a `Compulsory modules` section (a list) and a `Teaching and learning` section (a vague summary), you **MUST** use the `Compulsory modules` list. **You MUST IGNORE** vague summaries like "students will take one compulsory module" found in other sections.
4.  **Format**: Use bullet points, with blank lines (`\n\n`) between them for readability.
5.  **Tone**: Be direct. Do not say "Based on the information found" or "local documents"."""
            else:
                # åœºæ™¯ 2: æœ¬åœ°ç»“æœåˆ†æ•°ä½ï¼Œæˆ–ä½¿ç”¨äº†ç½‘ç»œæœç´¢ (å¦‚ "å…¨çƒå¥åº·ç®¡ç†")
                # ä½¿ç”¨å®½æ¾çš„ã€æ€»ç»“æ€§çš„æç¤ºè¯
                logger.info("ğŸ¤– ä½¿ç”¨ [é€šç”¨æ€»ç»“] æç¤ºè¯...")
                if language == "zh":
                    system_prompt = """ä½ æ˜¯UCLï¼ˆä¼¦æ•¦å¤§å­¦å­¦é™¢ï¼‰çš„AIåŠ©æ‰‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯**å‹å¥½åœ°æ€»ç»“**ä¸Šä¸‹æ–‡ä¸­ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„**æ‰€æœ‰**ä¿¡æ¯ã€‚

**è§„åˆ™ï¼š**
1.  **å…¨é¢æ€»ç»“**ï¼šç»¼åˆä¸Šä¸‹æ–‡ä¸­æä¾›çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆå¯èƒ½æ¥è‡ªä¸åŒé¡µé¢ï¼‰è¿›è¡Œå›ç­”ã€‚
2.  **å¿…é¡»å›ç­”**ï¼š**å¿…é¡»**æ ¹æ®ä¸Šä¸‹æ–‡æä¾›å›ç­”ã€‚ä¸è¦è¯´ "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”"ï¼Œè€Œæ˜¯æ€»ç»“ä½ æ‰¾åˆ°çš„å†…å®¹ã€‚
3.  **æ ¼å¼**ï¼šä½¿ç”¨è¦ç‚¹ï¼Œå¹¶åœ¨è¦ç‚¹ä¹‹é—´æ·»åŠ ç©ºè¡Œ (`\n\n`)ã€‚
4.  **æ€åº¦**ï¼šç›´æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯æ¥è‡ªçŸ¥ä¹æˆ–ç™¾åº¦ï¼Œå¯ä»¥éæ­£å¼åœ°å¼•ç”¨æ ‡é¢˜ã€‚"""
                else:
                    system_prompt = """You are a UCL AI assistant.
Your task is to **helpfully summarize ALL** information from the context that is relevant to the user's query.

**Rules:**
1.  **Summarize All**: Combine information from all context snippets provided.
2.  **Must Answer**: You **must** provide an answer based on the context. Do not say "Sorry, I cannot answer". Instead, summarize what you found.
3.  **Format**: Use bullet points, with blank lines (`\n\n`) between them for readability.
4.  **Tone**: Be direct and helpful."""

            
            user_prompt = f"""Question: {query}

Available Information:
{context}

Please provide a focused and accurate answer based on these rules."""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            answer = chat_with_groq(
                messages=messages,
                temperature=0.0,
                max_retries=2,
                model="llama-3.3-70b-versatile"
            )
            
            if answer and len(answer.strip()) > 30:
                return answer.strip() 
            else:
                logger.warning("LLM è¿”å›è¿‡çŸ­ï¼Œé™çº§åˆ°ç®€å•æ ¼å¼åŒ–")

        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
    
    return context 

def _detect_language(text: str) -> str:
    """æ£€æµ‹è¯­è¨€"""
    if not text: return "en"
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(text.strip())
    if total_chars > 0 and chinese_chars / total_chars > 0.3:
        return "zh"
    return "en"

def _detect_intent(query: str) -> str:
    """æ£€æµ‹æŸ¥è¯¢æ„å›¾"""
    q_lower = query.lower()
    
    # [æ–°] ä½¿ç”¨åˆ«åæ¥æ£€æµ‹æ„å›¾
    for intent, aliases in PROGRAM_ALIASES.items():
        if any(alias in q_lower for alias in aliases):
            # è¿™æ˜¯ä¸€ä¸ª "known_entity" æ„å›¾ï¼Œä½†æˆ‘ä»¬æŒ‰è¦æ±‚è¿”å›ç‰¹å®šæ„å›¾
            if intent in ("language_requirements", "requirements", "modules", "fees"):
                return intent
    
    # åŸå§‹æ„å›¾æ£€æµ‹
    intent_patterns = {
        "career": ["career", "employment", "job", "prospect", "å°±ä¸š", "èŒä¸š"],
        "services": ["service", "support", "counseling", "å’¨è¯¢", "æœåŠ¡"]
    }
    for intent, patterns in intent_patterns.items():
        if any(p in q_lower for p in patterns):
            return intent
    
    return "general"

def answer_enhanced(
    query: str,
    top_k: int = 10,
    language: str = "auto",
    **kwargs
) -> Dict[str, Any]:
    """ä¸»å…¥å£å‡½æ•° - ç»ˆæä¼˜åŒ–ç‰ˆ (Web æœç´¢é›†æˆ)"""
    start_time = time.time()
    
    if language == "auto":
        language = _detect_language(query)
    
    intent = _detect_intent(query)
    
    logger.info(f"ğŸ” æŸ¥è¯¢: '{query[:100]}...' | è¯­è¨€: {language} | æ„å›¾: {intent}")
    
    docs = _load_documents()
    
    local_results = _smart_search(query, docs, top_k)
    
    web_search_used = False
    web_context = ""
    web_citations: List[Dict] = []

    top_score = 0
    if local_results:
        top_score = local_results[0].get("score", 0)

    # è§¦å‘æ¡ä»¶ï¼š1. æœ¬åœ°æ²¡ç»“æœ, æˆ– 2. æœ¬åœ°æœ€é«˜åˆ† < 30
    if (not local_results or top_score < 30) and HAVE_WEB_SEARCH:
        logger.warning(f"âš ï¸ æœ¬åœ°ç»“æœä¸è¶³ (Top score: {top_score}). å¯åŠ¨ç½‘ç»œæœç´¢ for '{query}'...")
        try:
            # âœ… [æœ¬æ¬¡ä¿®å¤] ä¼˜åŒ–ä¸­æ–‡çš„ç½‘ç»œæœç´¢
            search_query = query
            program_name = _extract_program_name(query)
            
            if language == 'zh' and program_name:
                # ä¼˜å…ˆæœç´¢è‹±æ–‡æœ¯è¯­ï¼ŒæˆåŠŸç‡æ›´é«˜
                search_query_parts = [program_name]
                if intent == "language_requirements": search_query_parts.append("language requirements")
                elif intent == "requirements": search_query_parts.append("entry requirements")
                elif intent == "modules": search_query_parts.append("modules")
                elif intent == "fees": search_query_parts.append("tuition fees")
                
                search_query = " ".join(search_query_parts)
                logger.info(f"ğŸŒ ä¸­æ–‡æŸ¥è¯¢æ˜ å°„åˆ°è‹±æ–‡ç½‘ç»œæœç´¢: {search_query}")
            
            web_results = search_web(search_query, language=language, max_results=3)
            logger.info(f"ğŸŒ ç½‘ç»œæœç´¢å®Œæˆ: {len(web_results)} ä¸ªç»“æœ")
            
            if web_results:
                web_search_used = True
                web_context = _pick_best_snippets_from_web(web_results)
                
                for r in web_results:
                    web_citations.append({
                        "title": r.get("title", "Network Source"),
                        "url": r.get("url", ""),
                        "relevance_score": 0.0,
                        "source": "web"
                    })
        except Exception as e:
            logger.error(f"âŒ ç½‘ç»œæœç´¢å¤±è´¥: {e}")

    local_context = _format_context_for_llm(local_results)
    
    final_context = (local_context + "\n\n" + web_context).strip()
    
    # âœ… [æ–°] å‘ŠçŸ¥ LLM ä½¿ç”¨å“ªä¸ªæç¤ºè¯
    use_strict_prompt = (top_score > 90) and (not web_search_used)
    
    answer = _generate_comprehensive_answer(final_context, query, language, use_strict_prompt)
    
    citations = []
    for result in local_results[:5]:
        doc = result.get("doc", {})
        citations.append({
            "title": doc.get("title", ""),
            "url": doc.get("url", ""),
            "relevance_score": float(result.get("score", 0)),
            "source": "local"
        })
    
    citations.extend(web_citations)
    
    response_time = f"{time.time() - start_time:.2f}s"
    
    logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ: {response_time}, {len(local_results)} (æœ¬åœ°), {len(web_citations)} (ç½‘ç»œ)")
    
    return {
        "intent": intent,
        "answer": answer,
        "citations": citations,
        "reranked": local_results,
        "rewritten_queries": [],
        "response_time": response_time,
        "num_docs": len(local_results),
        "language": language,
        "semantic_used": False,
        "web_search_used": web_search_used
    }

if __name__ == "__main__":
    # æµ‹è¯•
    test_queries = [
        ("What are the modules for Intercultural Communication MA?", "auto"),
        ("Data Science MSc modules", "auto"),
        ("é›…æ€è¦æ±‚", "auto"),
        ("è®¡ç®—æœºç§‘å­¦å…¥å­¦è¦æ±‚", "auto"),
        ("å…¨çƒå¥åº·ç®¡ç†", "auto"),
    ]
    
    for query, lang in test_queries:
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"è¯­è¨€: {lang}")
        print('='*60)
        result = answer_enhanced(query, top_k=5, language=lang)
        print(f"æ‰¾åˆ°ç»“æœ (æœ¬åœ°): {result['num_docs']}")
        print(f"ä½¿ç”¨ç½‘ç»œ: {result['web_search_used']}")
        print(f"ç­”æ¡ˆé¢„è§ˆ: \n{result['answer'][:500]}...")