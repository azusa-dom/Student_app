#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
qa_enhanced_wrapper_FIXED_V2.py - ä¿®å¤ logging é”™è¯¯

å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç† llm_client çš„æ‰€æœ‰å¼‚å¸¸
"""

import os, re, json, time, logging
from pathlib import Path
from typing import Any, Dict, List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("qa_wrapper")

ROOT = Path(__file__).resolve().parents[1]
PROGRAMS_PATH = ROOT / "public" / "data" / "ucl_programs.json"
SERVICES_PATH = ROOT / "public" / "data" / "ucl_services.json"

def _load_documents() -> List[Dict]:
    docs: List[Dict] = []
    for p in (PROGRAMS_PATH, SERVICES_PATH):
        if p.exists():
            try:
                data = json.loads(p.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    docs.extend(data)
                    logger.info(f"âœ… åŠ è½½ {p.name}: {len(data)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ {p.name} å¤±è´¥: {e}")
    logger.info(f"ğŸ“š æ€»å…±åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    return docs

def _detect_language(text: str) -> str:
    if not text:
        return "en"
    text = text.strip()
    if not text:
        return "en"
    chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
    total_chars = len(text)
    if total_chars == 0:
        return "en"
    chinese_ratio = chinese_chars / total_chars
    logger.debug(f"è¯­è¨€æ£€æµ‹: '{text[:50]}...' | ä¸­æ–‡: {chinese_chars}/{total_chars} ({chinese_ratio:.1%})")
    return "zh" if chinese_ratio > 0.2 else "en"

def _detect_intent(q: str) -> str:
    ql = (q or "").lower()
    intent_patterns = {
        "language_requirements": ["ielts", "toefl", "language requirement", "english", "è¯­è¨€è¦æ±‚", "é›…æ€", "æ‰˜ç¦"],
        "requirements": ["entry requirement", "admission", "prerequisite", "å…¥å­¦", "ç”³è¯·", "è¦æ±‚", "èµ„æ ¼"],
        "modules": ["module", "course", "curriculum", "syllabus", "è¯¾ç¨‹", "æ¨¡å—", "compulsory", "core", "å¿…ä¿®"],
        "fees": ["fee", "tuition", "cost", "scholarship", "å­¦è´¹", "è´¹ç”¨", "å¥–å­¦é‡‘", "price"],
        "career": ["career", "employment", "job", "å°±ä¸š", "èŒä¸š", "å·¥ä½œ"],
        "services": ["service", "support", "counseling", "å’¨è¯¢", "æœåŠ¡", "æ”¯æŒ"],
    }
    for intent, patterns in intent_patterns.items():
        if any(k in ql for k in patterns):
            return intent
    return "general"

def _pick_best_snippets(results: List[Dict]) -> str:
    if not results:
        return ""
    parts = []
    for r in results[:3]:
        doc = r.get("doc", {})
        title = doc.get("title", "")
        matched_sections = r.get("matched_sections", [])
        for sec in matched_sections[:2]:
            heading = sec.get("heading", "")
            text = sec.get("text", "")
            if not text:
                continue
            text = text.replace("\u00a0", " ")
            text = re.sub(r'\s+', ' ', text).strip()
            text = text[:800]
            if title and heading:
                snippet = f"ã€{title} - {heading}ã€‘\n{text}"
            elif title:
                snippet = f"ã€{title}ã€‘\n{text}"
            else:
                snippet = text
            parts.append(snippet)
    context = "\n\n".join(parts)
    return context[:2500]

def _format_answer_with_llm(context: str, query: str, lang: str) -> str:
    """ä½¿ç”¨ LLM æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰"""
    if not context:
        if lang == "zh":
            return "æŠ±æ­‰ï¼Œæœªæ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®è®¿é—® UCL å®˜ç½‘è·å–æœ€æ–°ä¿¡æ¯ã€‚"
        else:
            return "Sorry, no relevant information found. Please check the UCL official website."
    
    # ğŸ”¥ ä¿®å¤ï¼šä¸‰å±‚å¼‚å¸¸å¤„ç†
    try:
        # ç¬¬1å±‚ï¼šå°è¯•å¯¼å…¥
        try:
            from scripts.llm_client import chat_with_groq, is_configured
        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥ llm_client: {e}")
            return _format_answer_simple(context, lang)
        
        # ç¬¬2å±‚ï¼šæ£€æŸ¥é…ç½®
        try:
            if not is_configured():
                logger.warning("âš ï¸ LLM æœªé…ç½® (GROQ_API_KEY æœªè®¾ç½®)")
                return _format_answer_simple(context, lang)
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥ LLM é…ç½®å¤±è´¥: {e}")
            return _format_answer_simple(context, lang)
        
        # æ„å»ºæç¤ºè¯
        if lang == "zh":
            system_prompt = "ä½ æ˜¯ UCLï¼ˆä¼¦æ•¦å¤§å­¦å­¦é™¢ï¼‰çš„AIåŠ©æ‰‹ã€‚ç”¨ç®€æ´ã€ä¸“ä¸šã€å‹å¥½çš„ä¸­æ–‡å›ç­”ã€‚"
            user_prompt = f"""æ ¹æ®å‚è€ƒä¿¡æ¯ç”¨ä¸­æ–‡å›ç­”ã€‚

é—®é¢˜ï¼š{query}
å‚è€ƒï¼š{context}

è¦æ±‚ï¼š
1. å¿…é¡»ç”¨ä¸­æ–‡
2. æ¸…æ™°ç»„ç»‡ç­”æ¡ˆ
3. ä¸“ä¸šä½†å‹å¥½
4. åªåŸºäºå‚è€ƒä¿¡æ¯

å›ç­”ï¼š"""
        else:
            system_prompt = "You are UCL AI assistant. Answer in concise, professional English."
            user_prompt = f"""Answer based on reference.

Question: {query}
Reference: {context}

Requirements:
1. English only
2. Clear organization
3. Professional tone
4. Reference-based only

Answer:"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        logger.info(f"ğŸ¤– ä½¿ç”¨ LLM ç”Ÿæˆ {lang} ç­”æ¡ˆ...")
        
        # ç¬¬3å±‚ï¼šè°ƒç”¨ API
        try:
            answer = chat_with_groq(messages=messages, temperature=0.3, max_retries=2)
        except Exception as e:
            logger.error(f"âŒ Groq API è°ƒç”¨å¤±è´¥: {e}")
            return _format_answer_simple(context, lang)
        
        if not answer or len(answer.strip()) < 20:
            logger.warning("âš ï¸ LLM è¿”å›ç­”æ¡ˆè¿‡çŸ­")
            return _format_answer_simple(context, lang)
        
        # éªŒè¯è¯­è¨€
        detected_lang = _detect_language(answer)
        if detected_lang != lang and lang == "zh" and detected_lang == "en":
            logger.warning(f"âš ï¸ è¯­è¨€ä¸åŒ¹é…ï¼Œé™çº§")
            return _format_answer_simple(context, lang)
        
        logger.info(f"âœ… LLM ç­”æ¡ˆ: {len(answer)} å­—ç¬¦")
        return answer.strip()
        
    except Exception as e:
        logger.error(f"âŒ LLM å¼‚å¸¸: {e}")
        logger.info("ğŸ”„ é™çº§åˆ°ç®€å•æ ¼å¼åŒ–")
        return _format_answer_simple(context, lang)

def _format_answer_simple(context: str, lang: str) -> str:
    if not context:
        return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚" if lang == "zh" else "Sorry, no information found."
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', context)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
    if not sentences:
        return context[:500]
    bullets = [f"â€¢ {s}" for s in sentences[:5]]
    answer = "\n".join(bullets)
    if lang == "zh":
        answer += "\n\nğŸ’¡ å»ºè®®è®¿é—® UCL å®˜ç½‘è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
    else:
        answer += "\n\nğŸ’¡ Visit UCL website for details."
    return answer

def answer_enhanced(query: str, top_k: int = 8, language: str = "auto", **kwargs) -> Dict[str, Any]:
    t0 = time.time()
    if language == "auto":
        language = _detect_language(query)
    intent = _detect_intent(query)
    logger.info(f"ğŸ” æŸ¥è¯¢: '{query}' | è¯­è¨€: {language} | æ„å›¾: {intent}")
    docs = _load_documents()
    reranked: List[Dict[str, Any]] = []
    semantic_used = False
    try:
        from scripts.enhanced_retriever import EnhancedRetriever
        logger.info("ğŸ§  è¯­ä¹‰æ£€ç´¢...")
        retriever = EnhancedRetriever(enable_semantic=True)
        raw = retriever.search_with_context(query=query, documents=docs, top_k=max(5, top_k), intent=intent)
        if raw:
            logger.info(f"âœ… è¯­ä¹‰æ£€ç´¢: {len(raw)} ä¸ªç»“æœ")
            for r in raw:
                reranked.append({"doc": r.get("doc", {}), "score": float(r.get("score", 0.0)), "matched_sections": r.get("matched_sections", [])})
            semantic_used = True
    except Exception as e:
        logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
    if not reranked:
        logger.info("ğŸ”„ å…³é”®è¯æ£€ç´¢...")
        reranked = _keyword_fallback(query, docs, intent, top_k)
    logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ: {len(reranked)} ä¸ªç»“æœ")
    context = _pick_best_snippets(reranked)
    answer = _format_answer_with_llm(context, query, language)
    citations = []
    for item in reranked[:5]:
        d = item.get("doc", {})
        citations.append({"title": d.get("title", ""), "url": d.get("url", ""), "relevance_score": float(item.get("score", 0.0)), "source": "local"})
    rt = f"{time.time() - t0:.2f}s"
    result = {"intent": intent, "answer": answer, "citations": citations, "reranked": reranked, "rewritten_queries": [], "response_time": rt, "num_docs": len(reranked), "language": language, "web_search_used": False, "semantic_used": semantic_used}
    logger.info(f"âœ… é—®ç­”å®Œæˆ: {rt}, {len(answer)} å­—ç¬¦")
    return result

def _keyword_fallback(query: str, docs: List[Dict], intent: str, top_k: int) -> List[Dict]:
    keywords = _extract_keywords(query)
    if not keywords:
        return []
    scored: List[Dict[str, Any]] = []
    for d in docs:
        if not d or not isinstance(d, dict):
            continue
        result = _score_document_keyword(d, keywords, intent)
        if result["score"] > 0:
            scored.append({"doc": d, "score": result["score"], "matched_sections": result.get("matched_sections", [])})
    scored.sort(key=lambda x: x["score"], reverse=True)
    return scored[:max(1, min(top_k, 10))]

def _extract_keywords(query: str) -> List[str]:
    stopwords = {"what", "how", "where", "when", "which", "who", "the", "a", "an", "is", "are", "was", "were", "do", "does", "did", "about", "for", "çš„", "æ˜¯", "æœ‰", "åœ¨", "å—", "å‘¢", "å•Š", "äº†"}
    keywords = []
    english_words = re.findall(r"\b[a-z]+\b", query.lower())
    keywords.extend([w for w in english_words if w not in stopwords and len(w) > 2])
    chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
    for chunk in chinese_matches:
        for i in range(len(chunk) - 1):
            keywords.append(chunk[i:i+2])
            if i + 3 <= len(chunk):
                keywords.append(chunk[i:i+3])
    return list(set(keywords))

def _score_document_keyword(doc: Dict, keywords: List[str], intent: str) -> Dict[str, Any]:
    score = 0.0
    title = (doc.get("title") or "").lower()
    title_hits = []
    for k in keywords:
        if k and k in title:
            score += 8
            title_hits.append(k)
    level = str(doc.get("level", "")).lower()
    if any(x in level for x in ["msc", "master", "postgraduate"]):
        score += 3
    matched_sections: List[Dict] = []
    sections = doc.get("sections") or []
    intent_headings = {
        "modules": ["module", "curriculum", "syllabus", "compulsory", "optional", "è¯¾ç¨‹"],
        "requirements": ["entry", "requirement", "admission", "qualification", "å…¥å­¦", "è¦æ±‚"],
        "fees": ["fee", "tuition", "cost", "scholarship", "å­¦è´¹", "è´¹ç”¨"],
        "career": ["career", "employment", "å°±ä¸š", "èŒä¸š"],
        "services": ["service", "support", "å’¨è¯¢", "æœåŠ¡"],
    }.get(intent, [])
    for s in sections[:30]:
        if not isinstance(s, dict):
            continue
        heading = (s.get("heading") or "")
        text = (s.get("text") or "")
        if not heading and not text:
            continue
        hlow, tlow = heading.lower(), text.lower()
        sec_score = 0.0
        if any(h in hlow for h in intent_headings):
            sec_score += 8
        hitk = sum(1 for k in keywords[:50] if k and (k in hlow or k in tlow))
        sec_score += min(hitk, 6) * 2
        if sec_score > 0:
            matched_sections.append({"heading": heading, "text": text[:600].replace("\u00a0", " "), "score": sec_score})
            score += sec_score
    matched_sections.sort(key=lambda x: x["score"], reverse=True)
    return {"score": score, "title_hits": list(set(title_hits)), "matched_sections": matched_sections[:5]}
