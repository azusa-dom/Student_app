import os
import json
import sys
import traceback
from pathlib import Path
from typing import Dict, List, Tuple

# åŠ¨æ€åŠ å…¥é¡¹ç›®æ ¹è·¯å¾„
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "scripts"))

try:
    from scripts.llm_client import chat_completion, LLMUnavailable, OLLAMA_MODEL
except Exception:
    class LLMUnavailable(Exception): pass
    def chat_completion(*_args, **_kwargs): raise LLMUnavailable("LLM æœªåˆå§‹åŒ–")
    OLLAMA_MODEL = "unknown"

# ------------------------- åŠ è½½çœŸå®æ–‡æ¡£ -------------------------

REAL_DOCS: List[Dict[str, str]] = []

def load_documents_from_file(path: Path) -> List[Dict[str, str]]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)  # æ˜¯ JSON æ•°ç»„ï¼Œä¸æ˜¯ JSONL
        docs = []
        for item in data:
            title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
            url = item.get("url", "#")
            sections = item.get("sections", [])
            text = "\n".join(s.get("text", "") for s in sections if isinstance(s, dict))
            if text.strip():
                docs.append({"title": title.strip(), "url": url, "text": text.strip()})
        return docs
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {path}: {e}")
        return []

def load_all_documents():
    global REAL_DOCS
    program_path = ROOT / "public/data/ucl_programs.json"
    service_path = ROOT / "public/data/ucl_services.json"
    REAL_DOCS = load_documents_from_file(program_path) + load_documents_from_file(service_path)
    print(f"âœ… æˆåŠŸåŠ è½½çœŸå®æ–‡æ¡£æ•°: {len(REAL_DOCS)}")

load_all_documents()

# ------------------------- Query æ”¹å†™ -------------------------

def _detect_language_and_rewrite(query: str) -> Tuple[str, List[str]]:
    q = (query or "").strip()
    if not q:
        return "zh", [""]
    has_zh = any("\u4e00" <= ch <= "\u9fff" for ch in q)
    lang = "zh" if has_zh else "en"
    rewrites = [q]
    if lang == "zh":
        rewrites.append(f"{q} site:ucl.ac.uk")
    else:
        rewrites.append(f"{q} site:ucl.ac.uk")
    return lang, rewrites

# ------------------------- æ–‡æ¡£æ£€ç´¢ï¼ˆå…³é”®è¯ï¼‰ -------------------------

def _retrieve_documents(query: str, top_k: int) -> List[Dict[str, str]]:
    """åŸºäºå…³é”®è¯åŒ¹é…æ£€ç´¢æ–‡æ¡£ï¼Œæ”¯æŒä¸­è‹±æ–‡åˆ†è¯"""
    q = (query or "").lower()
    
    # ä¸­æ–‡åˆ†è¯
    import re
    # ç®€å•ä¸­æ–‡åˆ†è¯ï¼šæŒ‰å­—ç¬¦å’Œå¸¸è§è¯åˆ†
    words = []
    # æå–è‹±æ–‡è¯
    english_words = re.findall(r'[a-zA-Z]+', q)
    words.extend([w.lower() for w in english_words if len(w) > 2])
    
    # æå–ä¸­æ–‡è¯ï¼ˆ2-4å­—ï¼‰
    chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', q)
    words.extend(chinese_words)
    
    if not words:
        words = q.split()
    
    scores = []
    for doc in REAL_DOCS:
        title = (doc.get("title") or "").lower()
        text = (doc.get("text") or "").lower()
        combined = title + " " + text
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°ï¼ˆæ ‡é¢˜æƒé‡æ›´é«˜ï¼‰
        title_score = sum(3 for word in words if word in title)
        text_score = sum(1 for word in words if word in combined)
        total_score = title_score + text_score
        
        scores.append((total_score, doc))
    
    scores.sort(key=lambda item: item[0], reverse=True)
    
    # åªè¿”å›æœ‰åŒ¹é…çš„æ–‡æ¡£ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›å‰å‡ ä¸ª
    relevant_docs = [doc for score, doc in scores if score > 0]
    if relevant_docs:
        return relevant_docs[:top_k]
    else:
        return [doc for _, doc in scores[:top_k]]

# ------------------------- æœ¬åœ°æ‘˜è¦ç”Ÿæˆ -------------------------

def _generate_local_summary(query: str, docs: List[Dict[str, str]]) -> str:
    """å½“ LLM ä¸å¯ç”¨æ—¶ï¼ŒåŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆç®€å•æ‘˜è¦"""
    if not docs:
        return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªé—®æ³•æˆ–è”ç³» UCL å®˜æ–¹ã€‚"
    
    # æå–å‰3ä¸ªæ–‡æ¡£çš„å…³é”®ä¿¡æ¯
    summary_parts = [f"æ ¹æ® UCL å®˜æ–¹èµ„æ–™ï¼Œå…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ï¼š\n"]
    
    for idx, doc in enumerate(docs[:3], start=1):
        title = doc.get('title', 'æœªçŸ¥æ¥æº')
        text = doc.get('text', '')
        # æˆªå–å‰200ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
        snippet = text[:200].strip()
        if len(text) > 200:
            snippet += "..."
        summary_parts.append(f"\nã€{idx}ã€‘{title}\n{snippet}")
    
    summary_parts.append("\n\nğŸ’¡ æç¤ºï¼šä»¥ä¸Šä¸ºè‡ªåŠ¨æ‘˜è¦ï¼Œè¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ä¸‹æ–¹å‚è€ƒæ¥æºã€‚")
    return "".join(summary_parts)

# ------------------------- Prompt æ„å»º -------------------------

SYSTEM_PROMPT = (
    "ä½ æ˜¯ UCL çš„é—®ç­”åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºæä¾›çš„èµ„æ–™å›ç­”é—®é¢˜ï¼Œ"
    "è‹¥èµ„æ–™ä¸è¶³è¯·æ˜ç¡®è¯´æ˜'æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”'ã€‚"
    "è¯·ç”¨ä¸­æ–‡ä½œç­”ï¼Œå›ç­”è¦ç®€æ´å‡†ç¡®ï¼Œä¸è¶…è¿‡ 200 å­—ã€‚"
)

def _build_prompt(query: str, docs: List[Dict[str, str]]) -> List[Dict[str, str]]:
    context_lines = []
    # åªå–å‰ 3 ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå‡å°‘ token æ•°é‡
    for idx, doc in enumerate(docs[:3], start=1):
        text = doc.get('text', '')[:500]  # é™åˆ¶æ¯ä¸ªæ–‡æ¡£ 500 å­—ç¬¦
        context_lines.append(f"[{idx}] {doc.get('title', 'æœªçŸ¥')}: {text}")
    context_block = "\n".join(context_lines)

    user_prompt = (
        f"èµ„æ–™ï¼š\n{context_block}\n\n"
        f"é—®é¢˜ï¼š{query}\nå›ç­”ï¼ˆä¸è¶…è¿‡200å­—ï¼‰ï¼š"
    )

    return [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt},
    ]

# ------------------------- ä¸»å…¥å£ -------------------------

def answer_enhanced(query: str, top_k: int = 5) -> Dict[str, object]:
    lang, rewrites = _detect_language_and_rewrite(query)
    docs = _retrieve_documents(query, top_k)
    
    try:
        messages = _build_prompt(query, docs)
        print(f"[RAG] è°ƒç”¨ Ollama æ¨¡å‹: {OLLAMA_MODEL}ï¼Œæ£€ç´¢æ–‡æ¡£ {len(docs)} æ¡")
        answer = chat_completion(messages, temperature=0.2, timeout=120)  # å¢åŠ è¶…æ—¶åˆ° 120 ç§’
        print(f"âœ… RAG æˆåŠŸç”Ÿæˆç­”æ¡ˆ")
    except LLMUnavailable as exc:
        print(f"âš ï¸  LLM ä¸å¯ç”¨: {exc}ï¼Œä½¿ç”¨æœ¬åœ°æ‘˜è¦æ¨¡å¼")
        answer = _generate_local_summary(query, docs)
    except Exception as exc:
        print(f"âŒ RAG æ‰§è¡Œå¤±è´¥: {exc}")
        traceback.print_exc()
        # é™çº§åˆ°æœ¬åœ°æ‘˜è¦è€Œä¸æ˜¯è¿”å›é”™è¯¯
        print(f"âš ï¸  é™çº§åˆ°æœ¬åœ°æ‘˜è¦æ¨¡å¼")
        answer = _generate_local_summary(query, docs)

    citations = [
        {"title": doc.get("title", "æœªçŸ¥æ¥æº"), "url": doc.get("url", "#")}
        for doc in docs
    ]

    return {
        "intent": lang,
        "answer": answer or "æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”ã€‚",
        "citations": citations,
        "rewritten_queries": rewrites,
        "reranked": docs,
    }
