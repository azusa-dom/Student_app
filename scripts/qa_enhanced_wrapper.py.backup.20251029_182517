#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
qa_enhanced_wrapper_FINAL.py - å®Œæ•´ä¿®å¤ç‰ˆ
ä¿®å¤ï¼šè¯­ä¹‰æ£€ç´¢å‚æ•° + è¯­è¨€æ£€æµ‹ + LLMç­”æ¡ˆæ ¼å¼åŒ–
"""

import os, re, json, time, logging
from pathlib import Path
from typing import Any, Dict, List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("qa_wrapper")

ROOT = Path(__file__).resolve().parents[1]
PROGRAMS_PATH = ROOT / "public" / "data" / "ucl_programs.json"
SERVICES_PATH = ROOT / "public" / "data" / "ucl_services.json"

def _load_documents() -> List[Dict]:
    """åŠ è½½æ–‡æ¡£"""
    docs: List[Dict] = []
    for p in (PROGRAMS_PATH, SERVICES_PATH):
        if p.exists():
            try:
                data = json.loads(p.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    docs.extend(data)
                    logger.info(f"âœ… åŠ è½½ {p.name}: {len(data)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ {p.name} å¤±è´¥: {e}")
    logger.info(f"ğŸ“š æ€»å…±åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    return docs

def _detect_language(text: str) -> str:
    """
    æ”¹è¿›çš„è¯­è¨€æ£€æµ‹
    
    ç­–ç•¥ï¼šè®¡ç®—ä¸­æ–‡å­—ç¬¦å æ¯”
    - ä¸­æ–‡å­—ç¬¦ > 20% â†’ zh
    - å¦åˆ™ â†’ en
    """
    if not text:
        return "en"
    
    text = text.strip()
    if not text:
        return "en"
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
    chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
    total_chars = len(text)
    
    if total_chars == 0:
        return "en"
    
    chinese_ratio = chinese_chars / total_chars
    
    # è°ƒè¯•æ—¥å¿—
    logger.debug(f"è¯­è¨€æ£€æµ‹: '{text[:50]}...' | ä¸­æ–‡å­—ç¬¦: {chinese_chars}/{total_chars} ({chinese_ratio:.1%})")
    
    # 20% é˜ˆå€¼
    return "zh" if chinese_ratio > 0.2 else "en"

def _detect_intent(q: str) -> str:
    """æ£€æµ‹æŸ¥è¯¢æ„å›¾"""
    ql = (q or "").lower()
    
    intent_patterns = {
        "language_requirements": ["ielts", "toefl", "language requirement", "english", "è¯­è¨€è¦æ±‚", "é›…æ€", "æ‰˜ç¦"],
        "requirements": ["entry requirement", "admission", "prerequisite", "å…¥å­¦", "ç”³è¯·", "è¦æ±‚", "èµ„æ ¼"],
        "modules": ["module", "course", "curriculum", "syllabus", "è¯¾ç¨‹", "æ¨¡å—", "compulsory", "core", "å¿…ä¿®"],
        "fees": ["fee", "tuition", "cost", "scholarship", "å­¦è´¹", "è´¹ç”¨", "å¥–å­¦é‡‘", "price"],
        "career": ["career", "employment", "job", "å°±ä¸š", "èŒä¸š", "å·¥ä½œ"],
        "services": ["service", "support", "counseling", "å’¨è¯¢", "æœåŠ¡", "æ”¯æŒ"],
    }
    
    for intent, patterns in intent_patterns.items():
        if any(k in ql for k in patterns):
            return intent
    
    return "general"

def _pick_best_snippets(results: List[Dict]) -> str:
    """
    æå–æœ€ä½³ç‰‡æ®µ
    
    Returns:
        æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    if not results:
        return ""
    
    parts = []
    
    for r in results[:3]:  # å–å‰3ä¸ªç»“æœ
        doc = r.get("doc", {})
        title = doc.get("title", "")
        
        matched_sections = r.get("matched_sections", [])
        
        for sec in matched_sections[:2]:  # æ¯ä¸ªæ–‡æ¡£å–2ä¸ªsection
            heading = sec.get("heading", "")
            text = sec.get("text", "")
            
            if not text:
                continue
            
            # æ¸…ç†æ–‡æœ¬
            text = text.replace("\u00a0", " ")  # æ›¿æ¢ä¸é—´æ–­ç©ºæ ¼
            text = re.sub(r'\s+', ' ', text).strip()  # å‹ç¼©ç©ºç™½
            text = text[:800]  # æˆªæ–­
            
            # æ„å»ºç‰‡æ®µ
            if title and heading:
                snippet = f"ã€{title} - {heading}ã€‘\n{text}"
            elif title:
                snippet = f"ã€{title}ã€‘\n{text}"
            else:
                snippet = text
            
            parts.append(snippet)
    
    context = "\n\n".join(parts)
    return context[:2500]  # æ€»é•¿åº¦é™åˆ¶

def _format_answer_with_llm(context: str, query: str, lang: str) -> str:
    """
    ä½¿ç”¨ LLM æ ¼å¼åŒ–ç­”æ¡ˆ
    
    Args:
        context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        query: ç”¨æˆ·æŸ¥è¯¢
        lang: ç›®æ ‡è¯­è¨€ (zh/en)
    
    Returns:
        æ ¼å¼åŒ–åçš„ç­”æ¡ˆ
    """
    if not context:
        if lang == "zh":
            return "æŠ±æ­‰ï¼Œæœªæ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®è®¿é—® UCL å®˜ç½‘è·å–æœ€æ–°ä¿¡æ¯ï¼Œæˆ–å°è¯•æ¢ä¸ªè¯´æ³•é‡æ–°æé—®ã€‚"
        else:
            return "Sorry, no relevant information found. Please check the UCL official website for the latest details, or try rephrasing your question."
    
    # å°è¯•ä½¿ç”¨ LLM
    try:
        from scripts.llm_client import chat_with_groq, is_configured
        
        if not is_configured():
            logger.warning("âš ï¸ LLM æœªé…ç½® (GROQ_API_KEY)ï¼Œä½¿ç”¨ç®€å•æ ¼å¼åŒ–")
            return _format_answer_simple(context, lang)
        
        # ğŸ”¥ æ„å»ºæç¤ºè¯ï¼ˆæ ¹æ®è¯­è¨€ï¼‰
        if lang == "zh":
            system_prompt = "ä½ æ˜¯ UCLï¼ˆä¼¦æ•¦å¤§å­¦å­¦é™¢ï¼‰çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ç®€æ´ã€ä¸“ä¸šã€å‹å¥½çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒä¿¡æ¯ï¼Œç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š
{query}

å‚è€ƒä¿¡æ¯ï¼š
{context}

è¦æ±‚ï¼š
1. å¿…é¡»ç”¨ä¸­æ–‡å›ç­”
2. ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦é‡å¤é—®é¢˜
3. ç”¨æ¸…æ™°çš„æ®µè½æˆ–è¦ç‚¹ç»„ç»‡ç­”æ¡ˆ
4. ä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”
5. å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œå»ºè®®ç”¨æˆ·è®¿é—® UCL å®˜ç½‘
6. ä¸è¦ç¼–é€ ä¿¡æ¯ï¼ŒåªåŸºäºå‚è€ƒä¿¡æ¯å›ç­”

å›ç­”ï¼š"""
        else:
            system_prompt = "You are an AI assistant for UCL (University College London). Answer user questions in concise, professional, and friendly English."
            user_prompt = f"""Please answer the user's question based on the following reference information.

User Question:
{query}

Reference Information:
{context}

Requirements:
1. Answer in English
2. Be direct and concise
3. Organize the answer in clear paragraphs or bullet points
4. Maintain a professional but friendly tone
5. If information is incomplete, suggest visiting the UCL website
6. Do not make up information - only answer based on the reference

Answer:"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        logger.info(f"ğŸ¤– ä½¿ç”¨ LLM ç”Ÿæˆ {lang} ç­”æ¡ˆ...")
        
        # è°ƒç”¨ Groq API
        answer = chat_with_groq(
            messages=messages,
            temperature=0.3,  # é™ä½æ¸©åº¦ï¼Œæ›´åŠ å‡†ç¡®
            max_retries=2
        )
        
        if not answer or len(answer.strip()) < 20:
            logger.warning("âš ï¸ LLM è¿”å›ç­”æ¡ˆè¿‡çŸ­ï¼Œä½¿ç”¨ç®€å•æ ¼å¼åŒ–")
            return _format_answer_simple(context, lang)
        
        # éªŒè¯ç­”æ¡ˆè¯­è¨€
        detected_lang = _detect_language(answer)
        if detected_lang != lang:
            logger.warning(f"âš ï¸ LLM è¿”å›è¯­è¨€ä¸åŒ¹é…: æœŸæœ› {lang}, å®é™… {detected_lang}")
            logger.warning(f"ç­”æ¡ˆé¢„è§ˆ: {answer[:100]}...")
            
            # å¦‚æœå·®å¼‚å¾ˆå¤§ï¼Œä½¿ç”¨ç®€å•æ ¼å¼åŒ–
            if lang == "zh" and detected_lang == "en":
                return _format_answer_simple(context, lang)
        
        logger.info(f"âœ… LLM ç­”æ¡ˆç”ŸæˆæˆåŠŸ: {len(answer)} å­—ç¬¦")
        return answer.strip()
        
    except Exception as e:
        logger.error(f"âŒ LLM æ ¼å¼åŒ–å¤±è´¥: {e}")
        logger.info("ğŸ”„ é™çº§åˆ°ç®€å•æ ¼å¼åŒ–")
        return _format_answer_simple(context, lang)

def _format_answer_simple(context: str, lang: str) -> str:
    """
    ç®€å•æ ¼å¼åŒ–ï¼ˆLLM ä¸å¯ç”¨æ—¶çš„é™çº§æ–¹æ¡ˆï¼‰
    
    Args:
        context: ä¸Šä¸‹æ–‡
        lang: è¯­è¨€
    
    Returns:
        æ ¼å¼åŒ–çš„ç­”æ¡ˆ
    """
    if not context:
        if lang == "zh":
            return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        else:
            return "Sorry, no relevant information found."
    
    # åˆ†å¥
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', context)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
    
    if not sentences:
        return context[:500]
    
    # æ ¼å¼åŒ–ä¸ºè¦ç‚¹
    bullets = [f"â€¢ {s}" for s in sentences[:5]]
    answer = "\n".join(bullets)
    
    # æ·»åŠ æç¤º
    if lang == "zh":
        answer += "\n\nğŸ’¡ å»ºè®®è®¿é—® UCL å®˜ç½‘è·å–æ›´è¯¦ç»†ä¿¡æ¯ã€‚"
    else:
        answer += "\n\nğŸ’¡ For more details, please visit the UCL official website."
    
    return answer

def answer_enhanced(query: str, top_k: int = 8, language: str = "auto", **kwargs) -> Dict[str, Any]:
    """
    å¢å¼ºç‰ˆé—®ç­”ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        top_k: è¿”å›ç»“æœæ•°
        language: è¯­è¨€ (auto/zh/en)
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        é—®ç­”ç»“æœå­—å…¸
    """
    t0 = time.time()
    
    # è¯­è¨€æ£€æµ‹
    if language == "auto":
        language = _detect_language(query)
    
    intent = _detect_intent(query)
    
    logger.info(f"ğŸ” æŸ¥è¯¢: '{query}' | è¯­è¨€: {language} | æ„å›¾: {intent}")
    
    # åŠ è½½æ–‡æ¡£
    docs = _load_documents()
    
    reranked: List[Dict[str, Any]] = []
    semantic_used = False
    
    # ğŸ”¥ å°è¯•è¯­ä¹‰æ£€ç´¢ï¼ˆä¿®å¤å‚æ•°ï¼‰
    try:
        from scripts.enhanced_retriever import EnhancedRetriever
        
        logger.info("ğŸ§  å°è¯•è¯­ä¹‰æ£€ç´¢...")
        
        # âš¡ å…³é”®ä¿®å¤ï¼šä¸ä¼  cache_embeddings å‚æ•°
        retriever = EnhancedRetriever(enable_semantic=True)
        
        raw = retriever.search_with_context(
            query=query,
            documents=docs,
            top_k=max(5, top_k),
            intent=intent
        )
        
        if raw:
            logger.info(f"âœ… è¯­ä¹‰æ£€ç´¢æˆåŠŸ: {len(raw)} ä¸ªç»“æœ")
            
            for r in raw:
                reranked.append({
                    "doc": r.get("doc", {}),
                    "score": float(r.get("score", 0.0)),
                    "matched_sections": r.get("matched_sections", [])
                })
            
            semantic_used = True
        else:
            logger.warning("âš ï¸ è¯­ä¹‰æ£€ç´¢è¿”å›ç©ºç»“æœ")
    
    except Exception as e:
        logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
    
    # é™çº§åˆ°å…³é”®è¯
    if not reranked:
        logger.info("ğŸ”„ é™çº§åˆ°å…³é”®è¯æ£€ç´¢...")
        reranked = _keyword_fallback(query, docs, intent, top_k)
    
    logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ: semantic={semantic_used}, {len(reranked)} ä¸ªç»“æœ")
    
    # ğŸ”¥ æå–ä¸Šä¸‹æ–‡å¹¶ç”¨ LLM æ ¼å¼åŒ–
    context = _pick_best_snippets(reranked)
    answer = _format_answer_with_llm(context, query, language)
    
    # æ„å»ºå¼•ç”¨
    citations = []
    for item in reranked[:5]:
        d = item.get("doc", {})
        citations.append({
            "title": d.get("title", ""),
            "url": d.get("url", ""),
            "relevance_score": float(item.get("score", 0.0)),
            "source": "local"
        })
    
    rt = f"{time.time() - t0:.2f}s"
    
    result = {
        "intent": intent,
        "answer": answer,
        "citations": citations,
        "reranked": reranked,
        "rewritten_queries": [],
        "response_time": rt,
        "num_docs": len(reranked),
        "language": language,
        "web_search_used": False,
        "semantic_used": semantic_used
    }
    
    logger.info(f"âœ… é—®ç­”å®Œæˆ: {rt}, {len(answer)} å­—ç¬¦")
    
    return result

def _keyword_fallback(query: str, docs: List[Dict], intent: str, top_k: int) -> List[Dict]:
    """å…³é”®è¯é™çº§æ£€ç´¢"""
    keywords = _extract_keywords(query)
    logger.info(f"ğŸ“ æå–å…³é”®è¯: {keywords[:10]}")
    
    if not keywords:
        logger.warning("âš ï¸ æœªæå–åˆ°å…³é”®è¯")
        return []
    
    scored: List[Dict[str, Any]] = []
    for d in docs:
        if not d or not isinstance(d, dict):
            continue
        
        result = _score_document_keyword(d, keywords, intent)
        if result["score"] > 0:
            scored.append({
                "doc": d,
                "score": result["score"],
                "matched_sections": result.get("matched_sections", [])
            })
    
    scored.sort(key=lambda x: x["score"], reverse=True)
    
    final_results = scored[:max(1, min(top_k, 10))]
    logger.info(f"âœ… å…³é”®è¯æ£€ç´¢å®Œæˆ: {len(final_results)} ä¸ªç»“æœ")
    
    return final_results

def _extract_keywords(query: str) -> List[str]:
    """æå–å…³é”®è¯"""
    stopwords = {
        "what", "how", "where", "when", "which", "who", "the", "a", "an",
        "is", "are", "was", "were", "do", "does", "did", "about", "for",
        "çš„", "æ˜¯", "æœ‰", "åœ¨", "å—", "å‘¢", "å•Š", "äº†", "å’Œ"
    }
    
    keywords = []
    
    # è‹±æ–‡å…³é”®è¯
    english_words = re.findall(r"\b[a-z]+\b", query.lower())
    keywords.extend([w for w in english_words if w not in stopwords and len(w) > 2])
    
    # ä¸­æ–‡å…³é”®è¯ï¼ˆç®€å•n-gramï¼‰
    chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
    for chunk in chinese_matches:
        for i in range(len(chunk) - 1):
            keywords.append(chunk[i:i+2])
            if i + 3 <= len(chunk):
                keywords.append(chunk[i:i+3])
    
    return list(set(keywords))

def _score_document_keyword(doc: Dict, keywords: List[str], intent: str) -> Dict[str, Any]:
    """å…³é”®è¯æ‰“åˆ†"""
    score = 0.0
    
    # æ ‡é¢˜åŒ¹é…
    title = (doc.get("title") or "").lower()
    title_hits = []
    for k in keywords:
        if k and k in title:
            score += 8
            title_hits.append(k)
    
    # Level åŒ¹é…
    level = str(doc.get("level", "")).lower()
    if any(x in level for x in ["msc", "master", "postgraduate", "ç ”ç©¶ç”Ÿ"]):
        score += 3
    
    # Section åŒ¹é…
    matched_sections: List[Dict] = []
    sections = doc.get("sections") or []
    
    intent_headings = {
        "modules": ["module", "curriculum", "syllabus", "compulsory", "optional", "è¯¾ç¨‹", "æ¨¡å—"],
        "requirements": ["entry", "requirement", "admission", "qualification", "english", "è¯­è¨€", "å…¥å­¦", "è¦æ±‚"],
        "fees": ["fee", "tuition", "cost", "scholarship", "å­¦è´¹", "è´¹ç”¨", "å¥–å­¦é‡‘"],
        "career": ["career", "employment", "prospect", "å°±ä¸š", "èŒä¸š"],
        "services": ["service", "support", "counseling", "å’¨è¯¢", "æœåŠ¡"],
    }.get(intent, [])
    
    for s in sections[:30]:
        if not isinstance(s, dict):
            continue
        
        heading = (s.get("heading") or "")
        text = (s.get("text") or "")
        
        if not heading and not text:
            continue
        
        hlow, tlow = heading.lower(), text.lower()
        sec_score = 0.0
        
        # Intent åŒ¹é…
        if any(h in hlow for h in intent_headings):
            sec_score += 8
        
        # å…³é”®è¯åŒ¹é…
        hitk = sum(1 for k in keywords[:50] if k and (k in hlow or k in tlow))
        sec_score += min(hitk, 6) * 2
        
        if sec_score > 0:
            matched_sections.append({
                "heading": heading,
                "text": text[:600].replace("\u00a0", " "),
                "score": sec_score
            })
            score += sec_score
    
    matched_sections.sort(key=lambda x: x["score"], reverse=True)
    
    return {
        "score": score,
        "title_hits": list(set(title_hits)),
        "matched_sections": matched_sections[:5]
    }


# ============ æµ‹è¯• ============
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•é—®ç­”ç³»ç»Ÿ")
    print("=" * 60)
    
    test_queries = [
        ("What are the tuition fees for Data Science MSc?", "auto"),
        ("æ•°æ®ç§‘å­¦ç¡•å£«çš„å­¦è´¹æ˜¯å¤šå°‘ï¼Ÿ", "auto"),
        ("IELTS requirements", "auto"),
    ]
    
    for query, lang in test_queries:
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"è¯­è¨€: {lang}")
        print('='*60)
        
        result = answer_enhanced(query, top_k=5, language=lang)
        
        print(f"\næ£€æµ‹è¯­è¨€: {result['language']}")
        print(f"æ„å›¾: {result['intent']}")
        print(f"è¯­ä¹‰æ£€ç´¢: {result['semantic_used']}")
        print(f"å“åº”æ—¶é—´: {result['response_time']}")
        print(f"\nç­”æ¡ˆ:\n{result['answer']}")
        print()
    
    print("=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 60)
