#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
qa_enhanced_wrapper.py â€” å®Œå…¨ä¿®å¤ç‰ˆï¼ˆè¯­ä¹‰æ£€ç´¢ä¼˜å…ˆï¼‰

ğŸ”¥ å…³é”®ä¿®å¤ï¼š
1. æ­£ç¡®å¯¼å…¥å’Œè°ƒç”¨ EnhancedRetriever
2. æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
3. ç¡®ä¿è¯­ä¹‰æ£€ç´¢å¤±è´¥åæ‰é™çº§
4. ä¿®å¤å¼‚å¸¸å¤„ç†é€»è¾‘
"""

import os
import re
import json
import time
import logging
from pathlib import Path
from typing import Any, Dict, List


logger = logging.getLogger(__name__)

def load_all_documents() -> List[Dict]:
    """å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    repo_root = Path(__file__).resolve().parents[1]
    data_paths = [
        repo_root / "public" / "data" / "ucl_programs.json",
        repo_root / "public" / "data" / "ucl_services.json",
    ]
    
    docs = []
    for p in data_paths:
        if not p.exists():
            logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {p}")
            continue
        try:
            with open(p, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    docs.extend(data)
            logger.info(f"âœ… åŠ è½½æ–‡æ¡£: {p.name} ({len(data)} æ¡)")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¤±è´¥: {p.name} - {e}")
    
    logger.info(f"ğŸ“š æ€»å…±åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    return docs

# ğŸ”¥ å¯åŠ¨æ—¶é¢„åŠ è½½
logger.info("ğŸ“¥ å¼€å§‹é¢„åŠ è½½æ–‡æ¡£...")
PRELOADED_DOCS = load_all_documents()

# ç„¶ååœ¨åˆ›å»º retriever æ—¶ä¼ å…¥
from scripts.enhanced_retriever import create_retriever

logger.info("ğŸš€ åˆ›å»ºæ£€ç´¢å™¨å¹¶é¢„è®¡ç®— embeddings...")
retriever = create_retriever(
    enable_semantic=True,
    preload_documents=PRELOADED_DOCS  # ä¼ å…¥æ–‡æ¡£åˆ—è¡¨
)

# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("qa_wrapper")

# ============================================================================
# è·¯å¾„é…ç½®
# ============================================================================
ROOT = Path(__file__).resolve().parents[1]
PROGRAMS_PATH = ROOT / "public" / "data" / "ucl_programs.json"
SERVICES_PATH = ROOT / "public" / "data" / "ucl_services.json"


def _load_documents() -> List[Dict]:
    """åŠ è½½æ–‡æ¡£æ•°æ®"""
    docs: List[Dict] = []
    for p in (PROGRAMS_PATH, SERVICES_PATH):
        if p.exists():
            try:
                data = json.loads(p.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    docs.extend(data)
                    logger.info(f"âœ… åŠ è½½ {p.name}: {len(data)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ {p.name} å¤±è´¥: {e}")
    
    logger.info(f"ğŸ“š æ€»å…±åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    return docs


def _detect_language(text: str) -> str:
    """æ£€æµ‹è¯­è¨€"""
    return "zh" if re.search(r"[\u4e00-\u9fff]", text or "") else "en"


def _detect_intent(q: str) -> str:
    """æ£€æµ‹æŸ¥è¯¢æ„å›¾"""
    ql = (q or "").lower()
    
    intent_patterns = {
        "language_requirements": ["ielts", "toefl", "language requirement", "english", "è¯­è¨€è¦æ±‚"],
        "requirements": ["entry requirement", "admission", "prerequisite", "å…¥å­¦", "ç”³è¯·", "è¦æ±‚"],
        "modules": ["module", "course", "curriculum", "syllabus", "è¯¾ç¨‹", "æ¨¡å—", "compulsory", "core"],
        "fees": ["fee", "tuition", "cost", "scholarship", "å­¦è´¹", "è´¹ç”¨", "å¥–å­¦é‡‘"],
        "career": ["career", "employment", "job", "å°±ä¸š", "èŒä¸š"],
        "services": ["service", "support", "counseling", "å’¨è¯¢", "æœåŠ¡", "æ”¯æŒ"]
    }
    
    for intent, patterns in intent_patterns.items():
        if any(k in ql for k in patterns):
            return intent
    
    return "general"


def _pick_best_snippets(results: List[Dict]) -> str:
    """ä»ç»“æœä¸­æå–æœ€ä½³ç‰‡æ®µ"""
    if not results:
        return ""
    
    parts = []
    for r in results[:3]:  # å–å‰3ä¸ªç»“æœ
        doc = r["doc"]
        title = doc.get("title", "")
        
        # è·å–åŒ¹é…çš„sections
        matched_sections = r.get("matched_sections", [])
        for sec in matched_sections[:2]:  # æ¯ä¸ªæ–‡æ¡£å–å‰2ä¸ªsection
            snippet = sec.get("text", "")
            if title and snippet:
                parts.append(f"[{title}] {snippet}")
    
    context = "\n".join(parts)[:1500]  # é™åˆ¶æ€»é•¿åº¦
    logger.debug(f"ğŸ“ æå–ä¸Šä¸‹æ–‡: {len(context)} å­—ç¬¦")
    return context


def _format_answer(context: str, lang: str) -> str:
    """æ ¼å¼åŒ–ç­”æ¡ˆ"""
    if not context:
        return (
            "æŠ±æ­‰ï¼Œæœªæ£€ç´¢åˆ°ä¸é—®é¢˜é«˜åº¦ç›¸å…³çš„å®˜æ–¹ä¿¡æ¯ã€‚å»ºè®®è®¿é—® UCL å®˜ç½‘æˆ–å°è¯•æ¢ä¸ªè¯´æ³•ã€‚" 
            if lang == "zh" 
            else "Sorry, no highly relevant official info found. Please check UCL website or rephrase your question."
        )
    
    # åˆ†å¥
    sents = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+", context)
    
    # æå–è¦ç‚¹
    bullets = []
    for s in sents:
        s = s.strip()
        if len(s) >= 20:
            bullets.append("â€¢ " + s[:150])
        if len(bullets) >= 5:
            break
    
    if not bullets:
        bullets = ["â€¢ " + (context[:150] + ("..." if len(context) > 150 else ""))]
    
    return "\n".join(bullets)


def answer_enhanced(
    query: str, 
    top_k: int = 8, 
    language: str = "auto",
    force_keyword: bool = False,  # æ–°å¢ï¼šå¼ºåˆ¶ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    **kwargs
) -> Dict[str, Any]:
    """
    å¢å¼ºç‰ˆé—®ç­”æ¥å£ï¼ˆè¯­ä¹‰æ£€ç´¢ä¼˜å…ˆï¼‰
    
    Args:
        query: æŸ¥è¯¢é—®é¢˜
        top_k: è¿”å›ç»“æœæ•°é‡
        language: è¯­è¨€ (auto/zh/en)
        force_keyword: å¼ºåˆ¶ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆè°ƒè¯•ç”¨ï¼‰
    
    Returns:
        åŒ…å«ç­”æ¡ˆã€å¼•ç”¨ã€å…ƒæ•°æ®çš„å­—å…¸
    """
    t0 = time.time()
    
    # è¯­è¨€å’Œæ„å›¾æ£€æµ‹
    language = language if language in ("zh", "en") else _detect_language(query)
    intent = _detect_intent(query)
    
    logger.info(f"ğŸ” æŸ¥è¯¢: '{query}' | è¯­è¨€: {language} | æ„å›¾: {intent}")
    
    # åŠ è½½æ–‡æ¡£
    docs = _load_documents()
    if not docs:
        logger.error("âŒ æ–‡æ¡£æ•°æ®ä¸ºç©º")
        return {
            "intent": intent,
            "answer": "ç³»ç»Ÿé”™è¯¯ï¼šæ— æ³•åŠ è½½æ•°æ®" if language == "zh" else "System error: Cannot load data",
            "citations": [],
            "reranked": [],
            "rewritten_queries": [],
            "response_time": f"{time.time() - t0:.2f}s",
            "num_docs": 0,
            "language": language,
            "web_search_used": False,
            "semantic_used": False
        }
    
    # æ£€ç´¢ç»“æœ
    reranked: List[Dict[str, Any]] = []
    semantic_used = False
    
    # ğŸ”¥ å°è¯•è¯­ä¹‰æ£€ç´¢ï¼ˆé™¤éå¼ºåˆ¶ä½¿ç”¨å…³é”®è¯ï¼‰
    if not force_keyword:
        try:
            logger.info("ğŸ§  å°è¯•è¯­ä¹‰æ£€ç´¢...")
            
            # å¯¼å…¥ä¿®å¤ç‰ˆçš„ EnhancedRetriever
            from scripts.enhanced_retriever import EnhancedRetriever
            
            retriever = EnhancedRetriever(enable_semantic=True, preload_documents=documents)
            
            # è°ƒç”¨ search_with_context
            raw = retriever.search_with_context(
                query=query, 
                documents=docs, 
                top_k=max(5, top_k),
                intent=intent
            )
            
            if raw:
                logger.info(f"âœ… è¯­ä¹‰æ£€ç´¢æˆåŠŸ: æ‰¾åˆ° {len(raw)} ä¸ªç»“æœ")
                for r in raw:
                    reranked.append({
                        "doc": r.get("doc", {}),
                        "score": float(r.get("score", 0.0)),
                        "matched_sections": r.get("matched_sections", [])
                    })
                semantic_used = True
            else:
                logger.warning("âš ï¸ è¯­ä¹‰æ£€ç´¢è¿”å›ç©ºç»“æœ")
        
        except ImportError as e:
            logger.error(f"âŒ æ— æ³•å¯¼å…¥ EnhancedRetriever: {e}")
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
    
    # ğŸ”¥ è¯­ä¹‰å¤±è´¥æˆ–å¼ºåˆ¶å…³é”®è¯ â†’ é™çº§åˆ°å…³é”®è¯æ£€ç´¢
    if not reranked:
        logger.info("ğŸ”„ é™çº§åˆ°å…³é”®è¯æ£€ç´¢...")
        reranked = _keyword_fallback(query, docs, intent, top_k)
        logger.info(f"âœ… å…³é”®è¯æ£€ç´¢å®Œæˆ: {len(reranked)} ä¸ªç»“æœ")
    
    # æ„å»ºç­”æ¡ˆ
    context = _pick_best_snippets(reranked)
    answer = _format_answer(context, language)
    
    # æ„å»ºå¼•ç”¨
    citations = []
    for item in reranked[:5]:
        d = item["doc"]
        citations.append({
            "title": d.get("title", ""),
            "url": d.get("url", ""),
            "relevance_score": float(item.get("score", 0.0)),
            "source": "local"
        })
    
    # è¿”å›ç»“æœ
    rt = f"{time.time() - t0:.2f}s"
    result = {
        "intent": intent,
        "answer": answer,
        "citations": citations,
        "reranked": reranked,
        "rewritten_queries": [],
        "response_time": rt,
        "num_docs": len(reranked),
        "language": language,
        "web_search_used": False,
        "semantic_used": semantic_used
    }
    
    logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ: semantic={semantic_used}, {rt}, {len(reranked)} ä¸ªç»“æœ")
    return result


def _keyword_fallback(query: str, docs: List[Dict], intent: str, top_k: int) -> List[Dict]:
    """å…³é”®è¯æ£€ç´¢é™çº§æ–¹æ¡ˆ"""
    
    # æå–å…³é”®è¯
    keywords = _extract_keywords(query)
    logger.info(f"ğŸ“ æå–å…³é”®è¯: {keywords[:10]}")
    
    if not keywords:
        logger.warning("âš ï¸ æœªæå–åˆ°æœ‰æ•ˆå…³é”®è¯")
        return []
    
    # å¯¹æ¯ä¸ªæ–‡æ¡£æ‰“åˆ†
    scored: List[Dict[str, Any]] = []
    for d in docs:
        if not d or not isinstance(d, dict):
            continue
        
        result = _score_document_keyword(d, keywords, intent)
        if result["score"] > 0:
            scored.append({
                "doc": d,
                "score": result["score"],
                "matched_sections": result.get("matched_sections", [])
            })
    
    # æ’åº
    scored.sort(key=lambda x: x["score"], reverse=True)
    
    return scored[:max(1, min(top_k, 10))]


def _extract_keywords(query: str) -> List[str]:
    """æå–æŸ¥è¯¢å…³é”®è¯"""
    stopwords = {
        "what", "how", "where", "when", "which", "who", "the", "a", "an",
        "is", "are", "was", "were", "do", "does", "did", "about", "for",
        "çš„", "æ˜¯", "æœ‰", "åœ¨", "å—", "å‘¢", "å•Š", "äº†",
    }
    
    keywords = []
    
    # è‹±æ–‡å…³é”®è¯
    english_words = re.findall(r"\b[a-z]+\b", query.lower())
    keywords.extend([w for w in english_words if w not in stopwords and len(w) > 2])
    
    # ä¸­æ–‡å…³é”®è¯ï¼ˆç®€å•n-gramï¼‰
    chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
    for chunk in chinese_matches:
        for i in range(len(chunk) - 1):
            keywords.append(chunk[i:i+2])
            if i + 3 <= len(chunk):
                keywords.append(chunk[i:i+3])
    
    # å»é‡
    seen = set()
    unique_keywords = []
    for kw in keywords:
        if kw and kw not in seen:
            unique_keywords.append(kw)
            seen.add(kw)
    
    return unique_keywords


def _score_document_keyword(doc: Dict, keywords: List[str], intent: str) -> Dict[str, Any]:
    """å…³é”®è¯æ‰“åˆ†"""
    score = 0.0
    title_hits = []
    
    # æ ‡é¢˜åŒ¹é…
    title = (doc.get("title") or "").lower()
    for k in keywords:
        if k and k in title:
            score += 8
            title_hits.append(k)
    
    # LevelåŒ¹é…
    level = str(doc.get("level", "")).lower()
    if any(x in level for x in ["msc", "master", "postgraduate", "ç ”ç©¶ç”Ÿ"]):
        score += 3
    
    # SectionåŒ¹é…
    matched_sections: List[Dict] = []
    sections = doc.get("sections") or []
    
    intent_headings = {
        "modules": ["module", "curriculum", "syllabus", "compulsory", "optional", "è¯¾ç¨‹", "æ¨¡å—"],
        "requirements": ["entry", "requirement", "admission", "qualification", "english", "è¯­è¨€", "å…¥å­¦", "è¦æ±‚"],
        "fees": ["fee", "tuition", "cost", "scholarship", "å­¦è´¹", "è´¹ç”¨", "å¥–å­¦é‡‘"],
        "career": ["career", "employment", "prospect", "å°±ä¸š", "èŒä¸š"],
        "services": ["service", "support", "counseling", "å’¨è¯¢", "æœåŠ¡"],
    }.get(intent, [])
    
    for s in sections[:30]:
        if not isinstance(s, dict):
            continue
        
        heading = (s.get("heading") or "")
        text = (s.get("text") or "")
        if not heading and not text:
            continue
        
        hlow, tlow = heading.lower(), text.lower()
        sec_score = 0.0
        
        # IntentåŒ¹é…
        if any(h in hlow for h in intent_headings):
            sec_score += 8
        
        # å…³é”®è¯åŒ¹é…
        hitk = sum(1 for k in keywords[:50] if k and (k in hlow or k in tlow))
        sec_score += min(hitk, 6) * 2
        
        if sec_score > 0:
            matched_sections.append({
                "heading": heading,
                "text": text[:600].replace("\u00a0", " "),
                "score": sec_score
            })
            score += sec_score
    
    # æ’åº
    matched_sections.sort(key=lambda x: x["score"], reverse=True)
    
    return {
        "score": score,
        "title_hits": list(set(title_hits)),
        "matched_sections": matched_sections[:5]
    }


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• answer_enhanced")
    print("=" * 60)
    
    test_queries = [
        "What are the core modules in Data Science MSc?",
        "è¯­è¨€è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
        "Machine learning requirements"
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢: {query}")
        print('='*60)
        
        result = answer_enhanced(query, top_k=5, language="auto")
        
        print(f"\nè¯­ä¹‰æ£€ç´¢: {'âœ…' if result['semantic_used'] else 'âŒ'}")
        print(f"æ„å›¾: {result['intent']}")
        print(f"ç»“æœæ•°: {result['num_docs']}")
        print(f"å“åº”æ—¶é—´: {result['response_time']}")
        print(f"\nç­”æ¡ˆ:\n{result['answer']}")
        print(f"\nå¼•ç”¨: {len(result['citations'])} ä¸ª")
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 60)