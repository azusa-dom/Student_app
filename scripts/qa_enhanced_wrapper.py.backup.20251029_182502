#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
qa_enhanced_wrapper.py — 完全修复版（语义检索优先）

🔥 关键修复：
1. 正确导入和调用 EnhancedRetriever
2. 添加详细的调试日志
3. 确保语义检索失败后才降级
4. 修复异常处理逻辑
"""

import os
import re
import json
import time
import logging
from pathlib import Path
from typing import Any, Dict, List


logger = logging.getLogger(__name__)

def load_all_documents() -> List[Dict]:
    """启动时加载所有文档"""
    repo_root = Path(__file__).resolve().parents[1]
    data_paths = [
        repo_root / "public" / "data" / "ucl_programs.json",
        repo_root / "public" / "data" / "ucl_services.json",
    ]
    
    docs = []
    for p in data_paths:
        if not p.exists():
            logger.warning(f"⚠️ 数据文件不存在: {p}")
            continue
        try:
            with open(p, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    docs.extend(data)
            logger.info(f"✅ 加载文档: {p.name} ({len(data)} 条)")
        except Exception as e:
            logger.error(f"❌ 加载失败: {p.name} - {e}")
    
    logger.info(f"📚 总共加载 {len(docs)} 个文档")
    return docs

# 🔥 启动时预加载
logger.info("📥 开始预加载文档...")
PRELOADED_DOCS = load_all_documents()

# 然后在创建 retriever 时传入
from scripts.enhanced_retriever import create_retriever

logger.info("🚀 创建检索器并预计算 embeddings...")
retriever = create_retriever(
    enable_semantic=True,
    preload_documents=PRELOADED_DOCS  # 传入文档列表
)

# ============================================================================
# 日志配置
# ============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("qa_wrapper")

# ============================================================================
# 路径配置
# ============================================================================
ROOT = Path(__file__).resolve().parents[1]
PROGRAMS_PATH = ROOT / "public" / "data" / "ucl_programs.json"
SERVICES_PATH = ROOT / "public" / "data" / "ucl_services.json"


def _load_documents() -> List[Dict]:
    """加载文档数据"""
    docs: List[Dict] = []
    for p in (PROGRAMS_PATH, SERVICES_PATH):
        if p.exists():
            try:
                data = json.loads(p.read_text(encoding="utf-8"))
                if isinstance(data, list):
                    docs.extend(data)
                    logger.info(f"✅ 加载 {p.name}: {len(data)} 个文档")
            except Exception as e:
                logger.warning(f"⚠️ 加载 {p.name} 失败: {e}")
    
    logger.info(f"📚 总共加载 {len(docs)} 个文档")
    return docs


def _detect_language(text: str) -> str:
    """检测语言"""
    return "zh" if re.search(r"[\u4e00-\u9fff]", text or "") else "en"


def _detect_intent(q: str) -> str:
    """检测查询意图"""
    ql = (q or "").lower()
    
    intent_patterns = {
        "language_requirements": ["ielts", "toefl", "language requirement", "english", "语言要求"],
        "requirements": ["entry requirement", "admission", "prerequisite", "入学", "申请", "要求"],
        "modules": ["module", "course", "curriculum", "syllabus", "课程", "模块", "compulsory", "core"],
        "fees": ["fee", "tuition", "cost", "scholarship", "学费", "费用", "奖学金"],
        "career": ["career", "employment", "job", "就业", "职业"],
        "services": ["service", "support", "counseling", "咨询", "服务", "支持"]
    }
    
    for intent, patterns in intent_patterns.items():
        if any(k in ql for k in patterns):
            return intent
    
    return "general"


def _pick_best_snippets(results: List[Dict]) -> str:
    """从结果中提取最佳片段"""
    if not results:
        return ""
    
    parts = []
    for r in results[:3]:  # 取前3个结果
        doc = r["doc"]
        title = doc.get("title", "")
        
        # 获取匹配的sections
        matched_sections = r.get("matched_sections", [])
        for sec in matched_sections[:2]:  # 每个文档取前2个section
            snippet = sec.get("text", "")
            if title and snippet:
                parts.append(f"[{title}] {snippet}")
    
    context = "\n".join(parts)[:1500]  # 限制总长度
    logger.debug(f"📝 提取上下文: {len(context)} 字符")
    return context


def _format_answer(context: str, lang: str) -> str:
    """格式化答案"""
    if not context:
        return (
            "抱歉，未检索到与问题高度相关的官方信息。建议访问 UCL 官网或尝试换个说法。" 
            if lang == "zh" 
            else "Sorry, no highly relevant official info found. Please check UCL website or rephrase your question."
        )
    
    # 分句
    sents = re.split(r"(?<=[。！？.!?])\s+", context)
    
    # 提取要点
    bullets = []
    for s in sents:
        s = s.strip()
        if len(s) >= 20:
            bullets.append("• " + s[:150])
        if len(bullets) >= 5:
            break
    
    if not bullets:
        bullets = ["• " + (context[:150] + ("..." if len(context) > 150 else ""))]
    
    return "\n".join(bullets)


def answer_enhanced(
    query: str, 
    top_k: int = 8, 
    language: str = "auto",
    force_keyword: bool = False,  # 新增：强制使用关键词检索（用于调试）
    **kwargs
) -> Dict[str, Any]:
    """
    增强版问答接口（语义检索优先）
    
    Args:
        query: 查询问题
        top_k: 返回结果数量
        language: 语言 (auto/zh/en)
        force_keyword: 强制使用关键词检索（调试用）
    
    Returns:
        包含答案、引用、元数据的字典
    """
    t0 = time.time()
    
    # 语言和意图检测
    language = language if language in ("zh", "en") else _detect_language(query)
    intent = _detect_intent(query)
    
    logger.info(f"🔍 查询: '{query}' | 语言: {language} | 意图: {intent}")
    
    # 加载文档
    docs = _load_documents()
    if not docs:
        logger.error("❌ 文档数据为空")
        return {
            "intent": intent,
            "answer": "系统错误：无法加载数据" if language == "zh" else "System error: Cannot load data",
            "citations": [],
            "reranked": [],
            "rewritten_queries": [],
            "response_time": f"{time.time() - t0:.2f}s",
            "num_docs": 0,
            "language": language,
            "web_search_used": False,
            "semantic_used": False
        }
    
    # 检索结果
    reranked: List[Dict[str, Any]] = []
    semantic_used = False
    
    # 🔥 尝试语义检索（除非强制使用关键词）
    if not force_keyword:
        try:
            logger.info("🧠 尝试语义检索...")
            
            # 导入修复版的 EnhancedRetriever
            from scripts.enhanced_retriever import EnhancedRetriever
            
            retriever = EnhancedRetriever(enable_semantic=True, preload_documents=documents)
            
            # 调用 search_with_context
            raw = retriever.search_with_context(
                query=query, 
                documents=docs, 
                top_k=max(5, top_k),
                intent=intent
            )
            
            if raw:
                logger.info(f"✅ 语义检索成功: 找到 {len(raw)} 个结果")
                for r in raw:
                    reranked.append({
                        "doc": r.get("doc", {}),
                        "score": float(r.get("score", 0.0)),
                        "matched_sections": r.get("matched_sections", [])
                    })
                semantic_used = True
            else:
                logger.warning("⚠️ 语义检索返回空结果")
        
        except ImportError as e:
            logger.error(f"❌ 无法导入 EnhancedRetriever: {e}")
        except Exception as e:
            logger.error(f"❌ 语义检索失败: {e}", exc_info=True)
    
    # 🔥 语义失败或强制关键词 → 降级到关键词检索
    if not reranked:
        logger.info("🔄 降级到关键词检索...")
        reranked = _keyword_fallback(query, docs, intent, top_k)
        logger.info(f"✅ 关键词检索完成: {len(reranked)} 个结果")
    
    # 构建答案
    context = _pick_best_snippets(reranked)
    answer = _format_answer(context, language)
    
    # 构建引用
    citations = []
    for item in reranked[:5]:
        d = item["doc"]
        citations.append({
            "title": d.get("title", ""),
            "url": d.get("url", ""),
            "relevance_score": float(item.get("score", 0.0)),
            "source": "local"
        })
    
    # 返回结果
    rt = f"{time.time() - t0:.2f}s"
    result = {
        "intent": intent,
        "answer": answer,
        "citations": citations,
        "reranked": reranked,
        "rewritten_queries": [],
        "response_time": rt,
        "num_docs": len(reranked),
        "language": language,
        "web_search_used": False,
        "semantic_used": semantic_used
    }
    
    logger.info(f"✅ 查询完成: semantic={semantic_used}, {rt}, {len(reranked)} 个结果")
    return result


def _keyword_fallback(query: str, docs: List[Dict], intent: str, top_k: int) -> List[Dict]:
    """关键词检索降级方案"""
    
    # 提取关键词
    keywords = _extract_keywords(query)
    logger.info(f"📝 提取关键词: {keywords[:10]}")
    
    if not keywords:
        logger.warning("⚠️ 未提取到有效关键词")
        return []
    
    # 对每个文档打分
    scored: List[Dict[str, Any]] = []
    for d in docs:
        if not d or not isinstance(d, dict):
            continue
        
        result = _score_document_keyword(d, keywords, intent)
        if result["score"] > 0:
            scored.append({
                "doc": d,
                "score": result["score"],
                "matched_sections": result.get("matched_sections", [])
            })
    
    # 排序
    scored.sort(key=lambda x: x["score"], reverse=True)
    
    return scored[:max(1, min(top_k, 10))]


def _extract_keywords(query: str) -> List[str]:
    """提取查询关键词"""
    stopwords = {
        "what", "how", "where", "when", "which", "who", "the", "a", "an",
        "is", "are", "was", "were", "do", "does", "did", "about", "for",
        "的", "是", "有", "在", "吗", "呢", "啊", "了",
    }
    
    keywords = []
    
    # 英文关键词
    english_words = re.findall(r"\b[a-z]+\b", query.lower())
    keywords.extend([w for w in english_words if w not in stopwords and len(w) > 2])
    
    # 中文关键词（简单n-gram）
    chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
    for chunk in chinese_matches:
        for i in range(len(chunk) - 1):
            keywords.append(chunk[i:i+2])
            if i + 3 <= len(chunk):
                keywords.append(chunk[i:i+3])
    
    # 去重
    seen = set()
    unique_keywords = []
    for kw in keywords:
        if kw and kw not in seen:
            unique_keywords.append(kw)
            seen.add(kw)
    
    return unique_keywords


def _score_document_keyword(doc: Dict, keywords: List[str], intent: str) -> Dict[str, Any]:
    """关键词打分"""
    score = 0.0
    title_hits = []
    
    # 标题匹配
    title = (doc.get("title") or "").lower()
    for k in keywords:
        if k and k in title:
            score += 8
            title_hits.append(k)
    
    # Level匹配
    level = str(doc.get("level", "")).lower()
    if any(x in level for x in ["msc", "master", "postgraduate", "研究生"]):
        score += 3
    
    # Section匹配
    matched_sections: List[Dict] = []
    sections = doc.get("sections") or []
    
    intent_headings = {
        "modules": ["module", "curriculum", "syllabus", "compulsory", "optional", "课程", "模块"],
        "requirements": ["entry", "requirement", "admission", "qualification", "english", "语言", "入学", "要求"],
        "fees": ["fee", "tuition", "cost", "scholarship", "学费", "费用", "奖学金"],
        "career": ["career", "employment", "prospect", "就业", "职业"],
        "services": ["service", "support", "counseling", "咨询", "服务"],
    }.get(intent, [])
    
    for s in sections[:30]:
        if not isinstance(s, dict):
            continue
        
        heading = (s.get("heading") or "")
        text = (s.get("text") or "")
        if not heading and not text:
            continue
        
        hlow, tlow = heading.lower(), text.lower()
        sec_score = 0.0
        
        # Intent匹配
        if any(h in hlow for h in intent_headings):
            sec_score += 8
        
        # 关键词匹配
        hitk = sum(1 for k in keywords[:50] if k and (k in hlow or k in tlow))
        sec_score += min(hitk, 6) * 2
        
        if sec_score > 0:
            matched_sections.append({
                "heading": heading,
                "text": text[:600].replace("\u00a0", " "),
                "score": sec_score
            })
            score += sec_score
    
    # 排序
    matched_sections.sort(key=lambda x: x["score"], reverse=True)
    
    return {
        "score": score,
        "title_hits": list(set(title_hits)),
        "matched_sections": matched_sections[:5]
    }


# ============================================================================
# 测试代码
# ============================================================================
if __name__ == "__main__":
    print("=" * 60)
    print("🧪 测试 answer_enhanced")
    print("=" * 60)
    
    test_queries = [
        "What are the core modules in Data Science MSc?",
        "语言要求是什么？",
        "Machine learning requirements"
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"查询: {query}")
        print('='*60)
        
        result = answer_enhanced(query, top_k=5, language="auto")
        
        print(f"\n语义检索: {'✅' if result['semantic_used'] else '❌'}")
        print(f"意图: {result['intent']}")
        print(f"结果数: {result['num_docs']}")
        print(f"响应时间: {result['response_time']}")
        print(f"\n答案:\n{result['answer']}")
        print(f"\n引用: {len(result['citations'])} 个")
    
    print("\n" + "=" * 60)
    print("✅ 测试完成")
    print("=" * 60)