#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""è”ç½‘æœç´¢æ¨¡å— - ä½¿ç”¨ DuckDuckGo çœŸå®å®ç°"""

import logging
import requests
from typing import List, Dict
from bs4 import BeautifulSoup
import re
import time

logger = logging.getLogger(__name__)

class WebSearcher:
    """è”ç½‘æœç´¢å™¨ - ä½¿ç”¨ DuckDuckGo HTML æœç´¢"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.session = requests.Session()
    
    def search_ucl_info(self, query: str, max_results: int = 3) -> List[Dict]:
        """
        æœç´¢ UCL ç›¸å…³ä¿¡æ¯
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤šè¿”å›ç»“æœæ•°
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        # ğŸ”¥ é™åˆ¶æœç´¢èŒƒå›´åœ¨ UCL å®˜ç½‘
        search_query = f"site:ucl.ac.uk {query}"
        
        try:
            # ä½¿ç”¨ DuckDuckGo HTML æœç´¢
            url = "https://html.duckduckgo.com/html/"
            params = {"q": search_query}
            
            logger.info(f"ğŸŒ Searching: {search_query}")
            
            response = self.session.post(
                url, 
                data=params, 
                headers=self.headers, 
                timeout=10
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # è§£ææœç´¢ç»“æœ
            result_divs = soup.find_all('div', class_='result')
            
            logger.info(f"ğŸ“Š Found {len(result_divs)} raw results")
            
            for result_div in result_divs[:max_results * 2]:  # å¤šå–ä¸€äº›ä»¥å¤‡è¿‡æ»¤
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = result_div.find('a', class_='result__a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    result_url = title_elem.get('href', '')
                    
                    # æ¸…ç† DuckDuckGo çš„è·³è½¬é“¾æ¥
                    if result_url.startswith('//duckduckgo.com/l/'):
                        # æå–çœŸå®URL
                        match = re.search(r'uddg=(https?://[^&]+)', result_url)
                        if match:
                            result_url = match.group(1)
                    
                    # ç¡®ä¿æ˜¯ UCL åŸŸå
                    if 'ucl.ac.uk' not in result_url:
                        continue
                    
                    # æå–æ‘˜è¦
                    snippet_elem = result_div.find('a', class_='result__snippet')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œå°è¯•å…¶ä»–å…ƒç´ 
                    if not snippet:
                        snippet_elem = result_div.find('div', class_='result__snippet')
                        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    if not snippet:
                        snippet = "No description available"
                    
                    results.append({
                        "title": title,
                        "url": result_url,
                        "snippet": snippet[:500],  # é™åˆ¶é•¿åº¦
                        "source": "web"
                    })
                    
                    if len(results) >= max_results:
                        break
                    
                except Exception as e:
                    logger.debug(f"âš ï¸  Failed to parse result: {e}")
                    continue
            
            logger.info(f"âœ… Web search found {len(results)} valid UCL results")
            return results
            
        except requests.Timeout:
            logger.error("âŒ Web search timeout")
            return []
        except requests.RequestException as e:
            logger.error(f"âŒ Web search request failed: {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ Web search failed: {e}")
            return []


def search_web(query: str, language: str = "en") -> List[Dict]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè”ç½‘æœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        language: è¯­è¨€ï¼ˆç”¨äºæç¤ºè¯ï¼‰
        
    Returns:
        List[Dict]: æœç´¢ç»“æœ
    """
    try:
        searcher = WebSearcher()
        return searcher.search_ucl_info(query)
    except Exception as e:
        logger.error(f"âŒ Search wrapper failed: {e}")
        return []


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    test_query = "Computer Science MSc entry requirements"
    results = search_web(test_query)
    
    print(f"\n{'='*80}")
    print(f"Search: {test_query}")
    print(f"{'='*80}\n")
    
    for i, result in enumerate(results, 1):
        print(f"[{i}] {result['title']}")
        print(f"    URL: {result['url']}")
        print(f"    Snippet: {result['snippet'][:150]}...")
        print()