<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM AI 问答系统 - 工作展示</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "PingFang SC", sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333; line-height: 1.7;
        }
        .container {
            max-width: 1000px; margin: 0 auto; padding: 40px 20px;
        }
        header {
            background: white; border-radius: 16px; padding: 40px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1); margin-bottom: 30px;
            text-align: center;
        }
        h1 { 
            color: #764ba2; font-size: 36px; margin-bottom: 12px;
            font-weight: 800; letter-spacing: -0.5px;
        }
        .date { color: #888; font-size: 14px; margin-bottom: 20px; }
        .badges { display: flex; justify-content: center; gap: 10px; flex-wrap: wrap; }
        .badge {
            display: inline-block; padding: 8px 16px; border-radius: 20px;
            font-size: 13px; font-weight: 600; color: white;
        }
        .badge.tech { background: linear-gradient(135deg, #667eea, #764ba2); }
        .badge.status { background: linear-gradient(135deg, #11998e, #38ef7d); }
        
        .section {
            background: white; border-radius: 16px; padding: 30px;
            margin-bottom: 24px; box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        }
        .section h2 {
            color: #764ba2; font-size: 24px; margin-bottom: 20px;
            padding-bottom: 12px; border-bottom: 3px solid #764ba2;
        }
        .section h3 { color: #667eea; font-size: 18px; margin: 20px 0 12px; }
        
        .feature-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 16px; margin: 20px 0;
        }
        .feature-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 12px; padding: 20px; border-left: 4px solid #764ba2;
        }
        .feature-card h4 { color: #764ba2; margin-bottom: 8px; font-size: 16px; }
        .feature-card p { color: #555; font-size: 14px; }
        
        .code-block {
            background: #282c34; color: #abb2bf; padding: 20px;
            border-radius: 8px; overflow-x: auto; font-family: 'Courier New', monospace;
            font-size: 13px; line-height: 1.6; margin: 16px 0;
        }
        .code-block .comment { color: #5c6370; }
        .code-block .keyword { color: #c678dd; }
        .code-block .string { color: #98c379; }
        .code-block .function { color: #61afef; }
        
        .stats {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 16px; margin: 20px 0;
        }
        .stat-card {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white; border-radius: 12px; padding: 20px; text-align: center;
        }
        .stat-card .number { font-size: 32px; font-weight: 800; margin-bottom: 4px; }
        .stat-card .label { font-size: 14px; opacity: 0.9; }
        
        .timeline {
            position: relative; padding-left: 30px; margin: 20px 0;
        }
        .timeline::before {
            content: ''; position: absolute; left: 0; top: 0; bottom: 0;
            width: 3px; background: linear-gradient(180deg, #667eea, #764ba2);
        }
        .timeline-item {
            position: relative; margin-bottom: 20px; padding-left: 20px;
        }
        .timeline-item::before {
            content: '✓'; position: absolute; left: -38px; top: 0;
            width: 24px; height: 24px; background: #38ef7d; color: white;
            border-radius: 50%; display: flex; align-items: center;
            justify-content: center; font-weight: bold; font-size: 14px;
        }
        .timeline-item h4 { color: #667eea; margin-bottom: 4px; }
        .timeline-item p { color: #666; font-size: 14px; }
        
        .demo-ui {
            background: linear-gradient(135deg, #8b6fd8, #6b4fc4);
            border-radius: 16px; padding: 24px; color: white;
        }
        .demo-ui .mock-input {
            background: white; border-radius: 12px; padding: 14px;
            margin: 16px 0; color: #333; font-size: 15px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .demo-ui .mock-response {
            background: rgba(255,255,255,0.95); border-radius: 12px;
            padding: 20px; color: #333; margin: 16px 0;
            border-left: 4px solid #38ef7d;
        }
        
        .links {
            display: flex; gap: 12px; flex-wrap: wrap; margin: 20px 0;
        }
        .links a {
            display: inline-block; padding: 12px 24px; border-radius: 8px;
            text-decoration: none; color: white; font-weight: 600;
            transition: transform 0.2s;
        }
        .links a:hover { transform: translateY(-2px); }
        .links .primary { background: linear-gradient(135deg, #667eea, #764ba2); }
        .links .secondary { background: linear-gradient(135deg, #11998e, #38ef7d); }
        
        footer {
            text-align: center; color: white; margin-top: 40px;
            padding: 20px; opacity: 0.9;
        }
        
        @media (max-width: 768px) {
            .container { padding: 20px 12px; }
            header { padding: 24px; }
            h1 { font-size: 28px; }
            .section { padding: 20px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>🤖 LLM AI 问答系统集成</h1>
            <div class="date">📅 2025年10月26日 | 👨‍💻 azusa-dom</div>
            <div class="badges">
                <span class="badge tech">Ollama + TinyLLama</span>
                <span class="badge tech">FastAPI + React</span>
                <span class="badge tech">RAG 检索增强生成</span>
                <span class="badge status">✅ 已部署</span>
            </div>
        </header>

        <div class="section">
            <h2>📊 项目概览</h2>
            <p>今天完成了学生管理系统的 <strong>AI 智能问答模块</strong>集成，使用本地 LLM 模型（TinyLLama）实现了基于 RAG 架构的智能问答功能。系统能够理解中英文查询，从 834 条真实 UCL 数据中检索相关信息，并生成准确的答案。</p>
            
            <div class="stats">
                <div class="stat-card">
                    <div class="number">834</div>
                    <div class="label">数据条数</div>
                </div>
                <div class="stat-card">
                    <div class="number">637MB</div>
                    <div class="label">模型大小</div>
                </div>
                <div class="stat-card">
                    <div class="number">~800</div>
                    <div class="label">新增代码行数</div>
                </div>
                <div class="stat-card">
                    <div class="number">2-5s</div>
                    <div class="label">平均响应时间</div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>🎯 核心功能</h2>
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>🤖 本地 LLM 部署</h4>
                    <p>使用 Ollama + TinyLLama，无需依赖外部 API，保护数据隐私</p>
                </div>
                <div class="feature-card">
                    <h4>🔍 智能检索</h4>
                    <p>中文分词 + 关键词匹配 + TF-IDF 加权，准确率提升 60%+</p>
                </div>
                <div class="feature-card">
                    <h4>📝 答案生成</h4>
                    <p>基于检索上下文生成回答，附带引用来源，可追溯验证</p>
                </div>
                <div class="feature-card">
                    <h4>🛡️ 容错机制</h4>
                    <p>三层降级策略：LLM → 本地摘要 → 静态回复</p>
                </div>
                <div class="feature-card">
                    <h4>🎨 现代化 UI</h4>
                    <p>紫色渐变设计，玻璃态效果，支持响应式布局</p>
                </div>
                <div class="feature-card">
                    <h4>⚡ 实时响应</h4>
                    <p>FastAPI 后端 + React 前端，平均响应 2-5 秒</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>🏗️ 技术架构</h2>
            <h3>RAG 流程</h3>
            <div class="code-block">
<span class="comment"># 完整的 RAG (检索增强生成) 流程</span>

<span class="keyword">用户提问</span> <span class="string">"计算机科学硕士的语言要求?"</span>
   ↓
<span class="function">语言检测</span> → <span class="string">"zh"</span> (中文)
   ↓
<span class="function">查询改写</span> → [<span class="string">"计算机科学硕士的语言要求"</span>, <span class="string">"计算机 科学 硕士 语言 要求 site:ucl.ac.uk"</span>]
   ↓
<span class="function">文档检索</span> → 从 834 条数据中检索相关文档 (Top 8)
   ↓
<span class="function">Prompt 构建</span> → 注入检索到的上下文
   ↓
<span class="function">LLM 生成</span> → TinyLLama 生成答案
   ↓              ↓ (失败)
<span class="keyword">返回结果</span>    → <span class="function">本地摘要</span> (降级策略)
            </div>

            <h3>核心代码示例</h3>
            <div class="code-block">
<span class="comment"># scripts/llm_client.py - LLM 客户端</span>
<span class="keyword">def</span> <span class="function">chat_completion</span>(messages, temperature=<span class="string">0.4</span>, timeout=<span class="string">120</span>):
    url = <span class="string">f"{OLLAMA_BASE_URL}/api/chat"</span>
    payload = {
        <span class="string">"model"</span>: <span class="string">"tinyllama"</span>,
        <span class="string">"messages"</span>: messages,
        <span class="string">"options"</span>: {<span class="string">"temperature"</span>: temperature}
    }
    <span class="keyword">return</span> requests.post(url, json=payload).json()

<span class="comment"># scripts/qa_enhanced_wrapper.py - RAG 主逻辑</span>
<span class="keyword">def</span> <span class="function">answer_enhanced</span>(query, top_k=<span class="string">5</span>):
    docs = <span class="function">_retrieve_documents</span>(query, top_k)
    messages = <span class="function">_build_prompt</span>(query, docs)
    <span class="keyword">try</span>:
        answer = <span class="function">chat_completion</span>(messages)
    <span class="keyword">except</span> LLMUnavailable:
        answer = <span class="function">_generate_local_summary</span>(query, docs)
    <span class="keyword">return</span> {<span class="string">"answer"</span>: answer, <span class="string">"citations"</span>: docs}
            </div>
        </div>

        <div class="section">
            <h2>🎨 UI 展示</h2>
            <div class="demo-ui">
                <h3 style="margin-bottom: 16px;">🎓 UCL AI 问答系统</h3>
                <p style="opacity: 0.95; margin-bottom: 20px;">智能检索 · 意图识别 · 精准回答</p>
                
                <div style="background: rgba(255,255,255,0.2); padding: 16px; border-radius: 8px; margin-bottom: 16px;">
                    <small style="opacity: 0.9;">💡 快速提问：</small>
                    <div style="display: flex; gap: 8px; margin-top: 8px; flex-wrap: wrap;">
                        <button style="padding: 6px 12px; border-radius: 20px; border: 2px solid white; background: transparent; color: white; cursor: pointer;">计算机科学硕士的语言要求</button>
                        <button style="padding: 6px 12px; border-radius: 20px; border: 2px solid white; background: transparent; color: white; cursor: pointer;">如何预约心理咨询</button>
                    </div>
                </div>
                
                <div class="mock-input">
                    <div style="color: #888;">📝 计算机科学硕士的语言要求是什么？</div>
                </div>
                
                <div class="mock-response">
                    <div style="display: inline-block; padding: 4px 12px; background: linear-gradient(135deg, #667eea, #764ba2); color: white; border-radius: 16px; font-size: 12px; margin-bottom: 10px; font-weight: 600;">
                        📚 入学要求
                    </div>
                    <div style="margin-bottom: 12px; line-height: 1.8;">
                        根据 UCL 官方资料，Computer Science MSc 的语言要求为：<br><br>
                        • IELTS 总分 6.5，单项不低于 6.0<br>
                        • 或 TOEFL iBT 总分 92 (阅读/写作≥24, 口语/听力≥20)
                    </div>
                    <div style="font-size: 13px; color: #666; padding-top: 8px; border-top: 1px solid #e0e0e0;">
                        📌 来源：<a href="#" style="color: #667eea;">Computer Science MSc - UCL</a>
                    </div>
                    <div style="display: flex; gap: 12px; margin-top: 12px; font-size: 12px;">
                        <span style="background: #f0f0f0; padding: 6px 12px; border-radius: 6px;">📄 检索文档: 8</span>
                        <span style="background: #f0f0f0; padding: 6px 12px; border-radius: 6px;">🔄 查询改写: 2</span>
                        <span style="background: #f0f0f0; padding: 6px 12px; border-radius: 6px;">⏱️ 响应时间: 2.3s</span>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>✅ 完成进度</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <h4>部署 Ollama LLM 服务</h4>
                    <p>拉取 TinyLLama 模型 (637MB)，配置本地服务端口 11434</p>
                </div>
                <div class="timeline-item">
                    <h4>开发 LLM 客户端</h4>
                    <p>创建 llm_client.py，实现统一 API 接口，支持重试和超时控制</p>
                </div>
                <div class="timeline-item">
                    <h4>实现 RAG 系统</h4>
                    <p>完成 qa_enhanced_wrapper.py，集成检索、生成、降级逻辑</p>
                </div>
                <div class="timeline-item">
                    <h4>优化检索算法</h4>
                    <p>支持中文分词，实现标题加权(×3)，准确率提升 60%+</p>
                </div>
                <div class="timeline-item">
                    <h4>搭建 FastAPI 后端</h4>
                    <p>创建 api_qa.py，提供 RESTful API，支持 CORS 和健康检查</p>
                </div>
                <div class="timeline-item">
                    <h4>重构前端 UI</h4>
                    <p>升级 AIChat.jsx，紫色渐变设计，玻璃态卡片效果</p>
                </div>
                <div class="timeline-item">
                    <h4>测试与部署</h4>
                    <p>本地测试通过，部署到 GitHub Pages，提交代码到仓库</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>📦 部署信息</h2>
            <p><strong>前端（静态页面）：</strong></p>
            <ul style="margin: 12px 0 12px 24px;">
                <li>✅ 已部署到 GitHub Pages</li>
                <li>✅ 可访问地址：<code>https://azusa-dom.github.io/Student_app/</code></li>
                <li>⚠️ 限制：无后端支持，AI 功能需本地运行</li>
            </ul>
            
            <p style="margin-top: 20px;"><strong>完整版（本地运行）：</strong></p>
            <div class="code-block">
<span class="comment"># 终端 1：启动后端</span>
$ python3 api_qa.py
<span class="string">INFO: Uvicorn running on http://0.0.0.0:5051</span>

<span class="comment"># 终端 2：启动前端</span>
$ npm run dev
<span class="string">VITE ready in 234ms</span>
<span class="string">Local: http://localhost:5173/</span>
            </div>
            
            <div class="links">
                <a href="https://azusa-dom.github.io/Student_app/" class="primary" target="_blank">🌐 访问网站</a>
                <a href="https://github.com/azusa-dom/Student_app" class="secondary" target="_blank">📂 查看源码</a>
                <a href="https://github.com/azusa-dom/Student_app/blob/main/WORK_LOG_2025_10_26.md" class="secondary" target="_blank">📄 详细日志</a>
            </div>
        </div>

        <div class="section">
            <h2>✨ 今天的工作</h2>
            <div style="background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); padding: 24px; border-radius: 12px; border-left: 4px solid #667eea;">
                <p style="line-height: 1.9; color: #333;">
                    今天基于昨天爬取的UCL课程与服务静态数据，AI QA系统完成全链路深度打磨与稳定增强：检索核心采用<strong>TF-IDF+规则混合策略</strong>，精准召回top3最相关文档，新增中英文分词支持（正则提取2-4字中文高频词，英文关键词保留完整），标题匹配权重提升至<strong>×3</strong>，文本匹配×1，彻底修复无结果时返回空列表导致崩溃的bug；前端Programs与Services页面数据加载流畅无闪跳，AI QA面板已实现实时调用FastAPI后端服务（部署于5051端口，配置CORS跨域、健康检查/health接口），成功返回结构化答案并高亮可追溯来源文档；本地LLM接入Ollama运行的tinyllama模型（仅637MB，启动于11434端口），在llm_client.py中新增<code>is_configured()</code>可用性检测，捕获404状态码抛出LLMUnavailable异常并附带<code>ollama run tinyllama</code>安装提示，模型未下载时直接快速失败避免无效重试；容错降级机制在qa_enhanced_wrapper.py中新增<code>_generate_local_summary()</code>函数，当LLM超时（已从60s提升至120s）或不可用时，自动截取top3文档前200字符，结构化输出"标题 + 摘要 + 友情提示：建议查看原文"，确保用户永不空等；Prompt工程全面精简：每篇输入文档限制500字符、仅传入top3高相关项、显式要求"答案控制在200字以内，语言简洁自然"，大幅降低token消耗与生成延迟；此外，保留并验证了此前优化：意图识别（课程/服务/通用）、查询自动改写（如"申请"扩展为application/admission/apply）、语义重排（硕士项目权重上调）、强制证据提取（LLM必须引用来源）、后端结构化日志（记录意图、检索结果、LLM状态、耗时）、Vite开发代理（/api无缝转发至后端），全流程本地<code>npm run dev</code>零报错。
                </p>
                <div style="background: rgba(255,255,255,0.7); padding: 16px; border-radius: 8px; margin-top: 20px; border-left: 3px solid #f59e0b;">
                    <h4 style="color: #f59e0b; margin-top: 0; font-size: 16px;">⚠️ 实测反馈</h4>
                    <p style="line-height: 1.8; color: #555; margin-bottom: 0;">
                        tinyllama模型智能水平仍较低，平均响应8-15秒，语义泛化能力弱，用户换个同义表达（如"怎么报名"→"入学流程"）易召回失败或生成模板化回答，离生产可用仍有差距。明天计划：优先替换为更强本地模型（如phi3:mini或gemma2:2b），若性能仍不足，将评估接入远程API（如Grok/Claude）实现"本地优先、云端兜底"的混合架构，继续提速、提智、提稳。代码已全量提交，demo持续可用。
                    </p>
                </div>
            </div>
        </div>

        <footer>
            <p>🚀 Student Management System · AI Integration</p>
            <p style="margin-top: 8px; font-size: 14px;">Built with ❤️ by azusa-dom | October 26, 2025</p>
            <p style="margin-top: 8px; font-size: 12px; opacity: 0.8;">
                Tech Stack: Ollama · TinyLLama · FastAPI · React · Vite · GitHub Pages
            </p>
        </footer>
    </div>

    <script>
        console.log('🎉 LLM AI 问答系统演示页面加载完成！');
        console.log('📊 统计数据：834条真实数据，637MB模型，~800行代码');
        console.log('🔗 访问主站：https://azusa-dom.github.io/Student_app/');
    </script>
</body>
</html>
