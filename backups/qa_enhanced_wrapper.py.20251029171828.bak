#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""qa_enhanced_wrapper_v2.py - 智能答案生成器

核心改进:
1. 智能查询验证 - 识别无意义查询
2. 改进的意图识别 - 更精确的理解
3. 更好的 Prompt - 告诉 LLM 如何思考
4. 质量检查 - 验证答案相关性
"""

import os, sys, json, time, logging, re
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "scripts"))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("qa_wrapper_v2")

# 导入依赖
try:
    from scripts.llm_client import chat_with_groq, is_configured as groq_configured
    logger.info("✅ Using llm_client.py")
except Exception:
    try:
        from scripts.groq_client import chat_with_groq, is_configured as groq_configured
        logger.info("✅ Using groq_client.py")
    except Exception:
        def groq_configured(): return False
        def chat_with_groq(*a, **k): raise Exception("LLM not available")

try:
    from scripts.enhanced_retriever import EnhancedRetriever
    HAVE_RETRIEVER = True
except Exception:
    HAVE_RETRIEVER = False

try:
    from scripts.web_search import search_web
    HAVE_WEB_SEARCH = True
except Exception:
    HAVE_WEB_SEARCH = False

PROGRAMS_PATH = ROOT / "public/data/ucl_programs.json"
SERVICES_PATH = ROOT / "public/data/ucl_services.json"

# ============================================================================
# 智能查询验证
# ============================================================================

def is_valid_query(query: str) -> tuple[bool, str]:
    """验证查询是否有意义
    
    Returns:
        (is_valid, reason)
    """
    if not query or len(query.strip()) < 2:
        return False, "too_short"
    
    query_lower = query.lower().strip()
    
    # 1. 检测测试查询
    test_queries = [
        'test', 'hello', 'hi', 'hey', 'testing', 
        '测试', '你好', 'こんにちは'
    ]
    if query_lower in test_queries:
        return False, "test_query"
    
    # 2. 检测单个无意义词
    meaningless = [
        'a', 'the', 'is', 'are', 'what', 'how',
        'why', 'when', 'where', 'who'
    ]
    if query_lower in meaningless:
        return False, "meaningless"
    
    # 3. 检测是否包含实质性内容
    words = re.findall(r'\b[a-zA-Z]+\b', query_lower)
    stopwords = {'the', 'is', 'are', 'what', 'how', 'a', 'an', 'in', 'on', 'at', 'to', 'for'}
    meaningful_words = [w for w in words if w not in stopwords and len(w) > 2]
    
    if len(meaningful_words) == 0:
        return False, "no_meaningful_words"
    
    return True, "valid"

def get_friendly_response(query: str, reason: str, language: str = "en") -> str:
    """为无效查询生成友好回应"""
    if language == "zh":
        responses = {
            "test_query": "你好！我是 UCL AI 助手 👋\n\n我可以帮你解答关于 UCL 的问题，比如：\n• 专业课程信息\n• 入学要求\n• 学费和奖学金\n• 校园服务\n\n请问有什么我可以帮你的？",
            "too_short": "你的问题太短了，我无法理解。请详细描述一下你想了解什么？",
            "meaningless": "请问你想了解 UCL 的什么信息？\n\n比如：\n• 数据科学专业的课程？\n• 入学语言要求？\n• 奖学金信息？",
            "no_meaningful_words": "我没理解你的问题。能否换个方式表达？"
        }
        return responses.get(reason, "请换个方式提问")
    else:
        responses = {
            "test_query": "Hello! I'm UCL AI Assistant 👋\n\nI can help you with:\n• Program information\n• Entry requirements\n• Fees & funding\n• Campus services\n\nWhat would you like to know?",
            "too_short": "Your query is too short. Could you elaborate?",
            "meaningless": "What would you like to know about UCL?\n\nFor example:\n• Course modules?\n• Entry requirements?\n• Scholarships?",
            "no_meaningful_words": "I didn't understand your query. Could you rephrase?"
        }
        return responses.get(reason, "Please rephrase your question")

# ============================================================================
# 改进的意图识别
# ============================================================================

def detect_intent_smart(query: str) -> str:
    """智能意图识别"""
    ql = query.lower()
    
    # 多级检测 - 从具体到一般
    
    # 1. 语言/入学要求相关
    if any(k in ql for k in ['ielts', 'toefl', 'language requirement', 'english requirement', 
                              '语言要求', 'language proficiency', 'language test']):
        return 'language_requirements'
    
    # 2. 一般入学要求
    if any(k in ql for k in ['entry requirement', 'admission', 'qualification', 'prerequisite',
                              '入学', '申请', 'requirement']):
        return 'requirements'
    
    # 3. 课程模块
    if any(k in ql for k in ['module', 'course', 'curriculum', 'syllabus', 'core',
                              '课程', '模块', 'compulsory']):
        return 'modules'
    
    # 4. 学费费用
    if any(k in ql for k in ['fee', 'tuition', 'cost', 'price', 'funding', 'scholarship',
                              '学费', '费用', '奖学金']):
        return 'fees'
    
    # 5. 职业/就业
    if any(k in ql for k in ['career', 'job', 'employment', 'graduate outcome',
                              '就业', '职业', 'placement']):
        return 'career'
    
    # 6. 服务支持
    if any(k in ql for k in ['service', 'support', 'counseling', 'help', 'guidance',
                              '服务', '支持', '咨询']):
        return 'services'
    
    # 7. 专业比较
    if ' vs ' in ql or ' versus ' in ql or '比较' in ql or 'compare' in ql:
        return 'comparison'
    
    return 'general'

# ============================================================================
# 改进的上下文构建
# ============================================================================

def build_smart_context(results: List[Dict], query: str, intent: str) -> str:
    """智能构建上下文 - 只提取真正相关的内容"""
    if not results:
        return ""
    
    snippets = []
    query_lower = query.lower()
    
    for idx, result in enumerate(results[:3], 1):  # 只用前3个最相关的
        doc = result.get("doc", {})
        title = doc.get("title", "Unknown")
        sections = result.get("matched_sections", [])
        
        if not sections:
            continue
        
        # 筛选真正相关的章节
        relevant_sections = []
        for section in sections[:3]:  # 每个文档最多3个章节
            if not isinstance(section, dict):
                continue
            
            heading = section.get("heading", "").lower()
            text = section.get("text", "")
            
            if not text or len(text) < 50:
                continue
            
            # 检查相关性
            relevance_score = 0
            
            # 标题相关性
            if any(word in heading for word in query_lower.split() if len(word) > 3):
                relevance_score += 2
            
            # 意图匹配
            intent_keywords = {
                'language_requirements': ['ielts', 'toefl', 'english', 'language level'],
                'requirements': ['entry', 'requirement', 'qualification', 'admission'],
                'modules': ['module', 'course', 'curriculum', 'compulsory'],
                'fees': ['fee', 'tuition', 'cost', 'scholarship'],
            }
            
            if intent in intent_keywords:
                if any(kw in heading or kw in text.lower()[:200] 
                       for kw in intent_keywords[intent]):
                    relevance_score += 3
            
            if relevance_score >= 2:  # 只保留相关的
                relevant_sections.append({
                    'heading': section.get('heading', ''),
                    'text': text[:400],  # 限制长度
                    'score': relevance_score
                })
        
        if relevant_sections:
            # 按相关性排序
            relevant_sections.sort(key=lambda x: x['score'], reverse=True)
            
            doc_text = f"[{idx}] {title}\n"
            for sec in relevant_sections[:2]:  # 每个文档最多2个最相关章节
                if sec['heading']:
                    doc_text += f"• {sec['heading']}: {sec['text']}\n"
                else:
                    doc_text += f"• {sec['text']}\n"
            
            snippets.append(doc_text)
    
    context = "\n\n".join(snippets)
    logger.info(f"📝 Smart context: {len(context)} chars from {len(snippets)} docs")
    return context

# ============================================================================
# 改进的 LLM Prompt
# ============================================================================

def generate_smart_answer(
    query: str,
    context: str,
    intent: str,
    language: str = "en"
) -> str:
    """使用改进的 Prompt 生成智能答案"""
    
    if not context:
        if language == "zh":
            return "抱歉，我找不到相关信息。\n\n建议：\n• 访问 UCL 官网获取最新信息\n• 换个方式描述你的问题\n• 联系 UCL 招生办"
        return "Sorry, I couldn't find relevant information.\n\nSuggestions:\n• Visit UCL website\n• Rephrase your question\n• Contact UCL admissions"
    
    # 构建智能 Prompt
    if language == "zh":
        system = """你是 UCL 智能助手，专门回答关于 UCL 的问题。

核心原则:
1. 只回答与 UCL 相关的问题
2. 直接给出答案，不要说"根据文档"、"文档显示"等废话
3. 用简洁的要点形式（•）列出关键信息
4. 如果信息不完整，诚实告知并建议访问官网
5. 100-150字内完成回答

禁止:
❌ "文档中提到..."
❌ "根据提供的信息..."
❌ 重复问题
❌ 过度啰嗦"""

        user = f"""信息来源:
{context}

用户问题: {query}

要求:
1. 直接回答问题
2. 用 • 列出关键要点
3. 100-150字
4. 纯中文"""
    
    else:
        system = """You are UCL AI Assistant, specialized in answering UCL-related questions.

Core principles:
1. Answer only UCL-related questions
2. Give direct answers, no "according to documents" nonsense
3. Use bullet points (•) for key information
4. If incomplete info, be honest and suggest visiting website
5. Keep under 150 words

Forbidden:
❌ "The documents mention..."
❌ "Based on provided information..."
❌ Repeating the question
❌ Being overly verbose"""

        user = f"""Information:
{context}

Question: {query}

Requirements:
1. Answer directly
2. Use • for key points
3. Under 150 words
4. English only"""
    
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user}
    ]
    
    # 调用 LLM
    try:
        if groq_configured():
            logger.info(f"🤖 LLM call (lang={language}, intent={intent})...")
            # NEW: 设置一个温和的超参数，防止输出过长或失控
            answer = chat_with_groq(messages, temperature=0.1)  # 保持你原设定
            
            # 后处理 - 移除常见废话
            forbidden_phrases = [
                "according to the documents",
                "based on the information provided",
                "the documents mention",
                "根据文档",
                "文档中提到",
                "提供的信息显示"
            ]
            low = answer.lower()
            for phrase in forbidden_phrases:
                if phrase in low:
                    logger.warning(f"⚠️  Found forbidden phrase: {phrase}")
                    # NEW: 更稳健的清理，不会误删整段
                    answer = re.sub(rf"\b{re.escape(phrase)}\b[:,，]?\s*", "", answer, flags=re.IGNORECASE)
                    low = answer.lower()
            return answer.strip()
        
    except Exception as e:
        logger.error(f"❌ LLM failed: {e}")
    
    # Fallback
    if language == "zh":
        return "抱歉，暂时无法生成回答。请访问 UCL 官网或联系招生办。"
    return "Sorry, unable to generate answer. Please visit UCL website or contact admissions."

# ============================================================================
# 主函数
# ============================================================================

@lru_cache(maxsize=1)
def _load_documents() -> List[Dict]:
    """加载文档"""
    documents = []
    for path, name in [(PROGRAMS_PATH, "programs"), (SERVICES_PATH, "services")]:
        if not path.exists():
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                documents.extend(data)
            logger.info(f"✅ Loaded {len(data)} {name}")
        except Exception as e:
            logger.error(f"❌ Load {name} failed: {e}")
    return documents

def _normalize_lang(query: str, language: str) -> str:
    """NEW: 轻量的语言归一化，保持你原有自动检测逻辑"""
    if language and language in ("zh", "en"):
        return language
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', query))
    return "zh" if chinese_chars > 0 else "en"

def _web_fallback(query: str, top_k: int, intent: str, language: str = "en") -> List[Dict]:
    """NEW: 当本地检索为空时，优先使用你已有的 search_web 做回退"""
    if not HAVE_WEB_SEARCH:
        return []
    try:
        # 这里假设 search_web 返回: [{'title','url','snippet'或'text'}, ...]
        web_hits = search_web(query, language=language)  # 兼容你的实现
        results = []
        for w in (web_hits or [])[:top_k]:
            snippet = w.get("snippet") or w.get("text") or ""
            if not snippet:
                continue
            results.append({
                "doc": {
                    "title": w.get("title", "Web result"),
                    "url": w.get("url", ""),
                },
                "score": w.get("score", 0),
                "intent": intent,
                "matched_sections": [{"heading": "", "text": snippet[:600]}],
                "source": "web"
            })
        if results:
            logger.info(f"🌐 Web fallback used: {len(results)} results")
        return results
    except Exception as e:
        logger.error(f"❌ Web fallback failed: {e}")
        return []

