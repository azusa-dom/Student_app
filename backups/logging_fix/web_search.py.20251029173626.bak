#!/usr/bin/env python3
logger = logging.getLogger("web_search")
if not logging.getLogger().handlers:
    logging.basicConfig(level=logging.INFO)
# -*- coding: utf-8 -*-
"""web_search.py - ä¿®å¤ç‰ˆç½‘ç»œæœç´¢

ä¸»è¦ä¿®å¤ï¼š
1. å¢åŠ è¯¦ç»†è°ƒè¯•æ—¥å¿—
2. æ·»åŠ  fallback æœºåˆ¶
3. ä¿®å¤ UCL ç»“æœè¿‡æ»¤é€»è¾‘
"""

import requests
import logging
from typing import List, Dict
from bs4 import BeautifulSoup


def search_web(query: str, language: str = "en", max_results: int = 5) -> List[Dict]:
    """
    ç½‘ç»œæœç´¢å‡½æ•° - ä¿®å¤ç‰ˆ
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        language: è¯­è¨€ (zh/en)
        max_results: æœ€å¤§ç»“æœæ•°
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    
    # ğŸ”¥ æ–¹æ¡ˆ1: ä½¿ç”¨ DuckDuckGo HTML æœç´¢
    results = _search_duckduckgo(query, max_results)
    
    # ğŸ”¥ å¦‚æœ DuckDuckGo å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ¡ˆ
    if not results:
        logger.warning("âš ï¸ DuckDuckGo æœç´¢å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–æœç´¢å¼•æ“ä½œä¸ºå¤‡ç”¨
    
    # ğŸ”¥ è¿‡æ»¤ UCL ç›¸å…³ç»“æœï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
    ucl_results = []
    for r in results:
        url = r.get('url', '').lower()
        title = r.get('title', '').lower()
        snippet = r.get('snippet', '').lower()
        
        # ğŸ”¥ æ”¾å®½ UCL åŒ¹é…æ¡ä»¶
        is_ucl = (
            'ucl.ac.uk' in url or 
            'ucl' in title or 
            'university college london' in title or
            'ucl' in snippet
        )
        
        if is_ucl:
            ucl_results.append(r)
    
    logger.info(f"ğŸŒ æœç´¢ç»“æœ: æ€»å…± {len(results)} ä¸ªï¼ŒUCL ç›¸å…³ {len(ucl_results)} ä¸ª")
    
    # ğŸ”¥ å¦‚æœæ²¡æœ‰ UCL ç»“æœï¼Œè¿”å›æ‰€æœ‰ç»“æœï¼ˆè€Œä¸æ˜¯ç©ºï¼‰
    return ucl_results if ucl_results else results[:max_results]


def _search_duckduckgo(query: str, max_results: int = 5) -> List[Dict]:
    """DuckDuckGo æœç´¢å®ç°"""
    try:
        # ğŸ”¥ æ„å»ºæœç´¢ URL - æ˜ç¡®æŒ‡å®šç«™ç‚¹
        search_query = f"site:ucl.ac.uk {query}"
        url = f"https://html.duckduckgo.com/html/?q={requests.utils.quote(search_query)}"
        
        logger.info(f"ğŸ” æœç´¢ URL: {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        logger.info(f"ğŸ“¡ å“åº”çŠ¶æ€: {response.status_code}")
        
        if response.status_code != 200:
            logger.error(f"âŒ HTTP {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        results = []
        
        # ğŸ”¥ å°è¯•å¤šç§é€‰æ‹©å™¨
        result_divs = (
            soup.select('.result') or 
            soup.select('.web-result') or 
            soup.select('.links_main')
        )
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(result_divs)} ä¸ªåŸå§‹ç»“æœ")
        
        for idx, result_div in enumerate(result_divs[:max_results]):
            try:
                # æå–æ ‡é¢˜
                title_elem = (
                    result_div.select_one('.result__title') or
                    result_div.select_one('.result__a') or
                    result_div.select_one('a.result__url')
                )
                title = title_elem.get_text(strip=True) if title_elem else f"Result {idx+1}"
                
                # æå– URL
                url_elem = result_div.select_one('a.result__url')
                url = url_elem.get('href', '') if url_elem else ''
                
                # æå–æ‘˜è¦
                snippet_elem = (
                    result_div.select_one('.result__snippet') or
                    result_div.select_one('.result__description')
                )
                snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                
                if title and (url or snippet):
                    results.append({
                        'title': title,
                        'url': url,
                        'snippet': snippet,
                        'source': 'web'
                    })
                    logger.debug(f"âœ… ç»“æœ {idx+1}: {title[:50]}...")
            
            except Exception as e:
                logger.warning(f"âš ï¸ è§£æç»“æœ {idx+1} å¤±è´¥: {e}")
                continue
        
        return results
    
    except requests.Timeout:
        logger.error("âŒ æœç´¢è¶…æ—¶")
        return []
    except requests.RequestException as e:
        logger.error(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
        return []


# ğŸ”¥ æ·»åŠ ä¸€ä¸ªæµ‹è¯•å‡½æ•°
def test_search():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    test_queries = [
        "Computer Science MSc",
        "Data Science MSc modules",
        "UCL language requirements"
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•æŸ¥è¯¢: {query}")
        print('='*60)
        
        results = search_web(query, max_results=3)
        
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
        for i, r in enumerate(results, 1):
            print(f"{i}. {r['title']}")
            print(f"   URL: {r['url']}")
            print(f"   æ‘˜è¦: {r['snippet'][:100]}...")
            print()


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(level=logging.INFO)
    test_search()