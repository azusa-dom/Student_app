#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
qa_enhanced_wrapper.py â€” æ™ºèƒ½è¯­ä¹‰æ£€ç´¢ä¼˜å…ˆï¼Œå¤±è´¥è‡ªåŠ¨é™çº§ï¼ˆå…³é”®è¯æ£€ç´¢ï¼‰
- è‹¥ sentence-transformers/æ¨¡å‹åŠ è½½æˆåŠŸï¼šä½¿ç”¨ EnhancedRetriever(enable_semantic=True)
- å¦åˆ™è‡ªåŠ¨é™çº§ä¸ºè½»é‡å…³é”®è¯æ£€ç´¢
- è¾“å‡ºç»“æ„ä¸å‰ç«¯å®Œå…¨å…¼å®¹ï¼ˆintent/answer/citations/reranked/rewritten_queries/num_docs/response_timeï¼‰
"""
import os, re, json, time, logging
from pathlib import Path
from typing import Any, Dict, List

logger = logging.getLogger("qa_wrapper_live")
logging.basicConfig(level=logging.INFO)

ROOT = Path(__file__).resolve().parents[1]
PROGRAMS_PATH = ROOT / "public" / "data" / "ucl_programs.json"
SERVICES_PATH = ROOT / "public" / "data" / "ucl_services.json"

def _load_documents() -> List[Dict]:
    docs: List[Dict] = []
    for p in (PROGRAMS_PATH, SERVICES_PATH):
        if p.exists():
            try:
                data = json.loads(p.read_text(encoding="utf-8"))
                if isinstance(data, list): docs.extend(data)
            except Exception as e:
                logger.warning("âš ï¸ load %s failed: %s", p.name, e)
    logger.info("ğŸ“š loaded %d docs", len(docs))
    return docs

def _detect_language(text: str) -> str:
    return "zh" if re.search(r"[\u4e00-\u9fff]", text or "") else "en"

def _detect_intent(q: str) -> str:
    ql = (q or "").lower()
    if any(k in ql for k in ["ielts","toefl","language requirement","english","è¯­è¨€è¦æ±‚"]): return "language_requirements"
    if any(k in ql for k in ["entry requirement","admission","prerequisite","å…¥å­¦","ç”³è¯·","è¦æ±‚"]): return "requirements"
    if any(k in ql for k in ["module","course","curriculum","syllabus","è¯¾ç¨‹","æ¨¡å—","compulsory","core"]): return "modules"
    if any(k in ql for k in ["fee","tuition","cost","scholarship","å­¦è´¹","è´¹ç”¨","å¥–å­¦é‡‘"]): return "fees"
    if any(k in ql for k in ["career","employment","job","å°±ä¸š","èŒä¸š"]): return "career"
    if any(k in ql for k in ["service","support","counseling","å’¨è¯¢","æœåŠ¡","æ”¯æŒ"]): return "services"
    return "general"

def _keywords(q: str) -> List[str]:
    kw = set()
    for w in re.findall(r"[A-Za-z]{3,}", (q or "").lower()): kw.add(w)
    for chunk in re.findall(r"[\u4e00-\u9fff]+", q or ""):
        for i in range(len(chunk)-1):
            kw.add(chunk[i:i+2])
            if i+3 <= len(chunk): kw.add(chunk[i:i+3])
    return list(kw)

def _kw_score_doc(doc: Dict, kws: List[str], intent: str) -> Dict[str, Any]:
    score = 0.0
    title = (doc.get("title") or "").lower()
    title_hits = []
    for k in kws:
        if k and k in title:
            score += 8
            title_hits.append(k)
    lvl = str(doc.get("level","")).lower()
    if any(x in lvl for x in ["msc","master","postgraduate","ç ”ç©¶ç”Ÿ"]): score += 3
    if any(x in lvl for x in ["bsc","ba","undergraduate","æœ¬ç§‘"]): score += 1
    matched_sections: List[Dict] = []
    sections = doc.get("sections") or []
    intent_headings = {
        "modules": ["module","curriculum","syllabus","compulsory","optional","è¯¾ç¨‹","æ¨¡å—"],
        "requirements": ["entry","requirement","admission","qualification","english","è¯­è¨€","å…¥å­¦","è¦æ±‚"],
        "fees": ["fee","tuition","cost","scholarship","å­¦è´¹","è´¹ç”¨","å¥–å­¦é‡‘"],
        "career": ["career","employment","prospect","å°±ä¸š","èŒä¸š"],
        "services": ["service","support","counseling","å’¨è¯¢","æœåŠ¡"],
    }.get(intent, [])
    for s in sections[:30]:
        if not isinstance(s, dict): continue
        heading = (s.get("heading") or "")
        text = (s.get("text") or "")
        if not heading and not text: continue
        hlow, tlow = heading.lower(), text.lower()
        sec = 0.0
        if any(h in hlow for h in intent_headings): sec += 8
        hitk = 0
        for k in kws[:50]:
            if k and (k in hlow or k in tlow): hitk += 1
        sec += min(hitk, 6) * 2
        if sec > 0:
            matched_sections.append({
                "heading": heading,
                "text": text[:600].replace("\u00a0"," "),
                "score": sec
            })
            score += sec
    return {
        "score": score,
        "title_hits": list(set(title_hits)),
        "matched_sections": sorted(matched_sections, key=lambda x: x["score"], reverse=True)[:5]
    }

def _pick_best_snippets(results: List[Dict]) -> str:
    if not results: return ""
    parts = []
    for r in results[:2]:
        doc = r["doc"]; title = doc.get("title","")
        for sec in (r.get("matched_sections") or [])[:2]:
            snippet = sec.get("text","")
            if title and snippet: parts.append(f"[{title}] {snippet}")
    return "\n".join(parts)[:1200]

def _format_answer(context: str, lang: str) -> str:
    if not context:
        return "æŠ±æ­‰ï¼Œæœªæ£€ç´¢åˆ°ä¸é—®é¢˜é«˜åº¦ç›¸å…³çš„å®˜æ–¹ä¿¡æ¯ã€‚å»ºè®®è®¿é—® UCL å®˜ç½‘æˆ–å°è¯•æ¢ä¸ªè¯´æ³•ã€‚" if lang=="zh" \
            else "Sorry, no highly relevant official info found. Please check UCL website or rephrase your question."
    sents = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+", context)
    bullets = []
    for s in sents:
        s = s.strip()
        if len(s) >= 20:
            bullets.append("â€¢ " + s[:140])
        if len(bullets) >= 4: break
    if not bullets:
        bullets = ["â€¢ " + (context[:140] + ("..." if len(context)>140 else ""))]
    return "\n".join(bullets)

def answer_enhanced(query: str, top_k: int = 8, language: str = "auto", **kwargs) -> Dict[str, Any]:
    t0 = time.time()
    language = language if language in ("zh","en") else _detect_language(query)
    intent = _detect_intent(query)
    docs = _load_documents()

    reranked: List[Dict[str, Any]] = []
    semantic_used = False

    # å°è¯•è¯­ä¹‰æ£€ç´¢ï¼ˆå¯ç”¨åˆ™ä¼˜å…ˆï¼‰
    try:
        from scripts.enhanced_retriever import EnhancedRetriever  # ä½ é¡¹ç›®é‡Œçš„å®ç°
        retriever = EnhancedRetriever(enable_semantic=True, cache_embeddings=True)
        raw = retriever.search_with_context(query, docs, top_k=max(5, top_k))
        for r in raw:
            reranked.append({
                "doc": r.get("doc", {}),
                "score": float(r.get("score", 0.0)),
                "matched_sections": r.get("matched_sections", [])
            })
        semantic_used = len(reranked) > 0
        if semantic_used:
            logger.info("âœ… Semantic retrieval used: %d", len(reranked))
    except Exception as e:
        logger.warning("âš ï¸ Semantic retrieval unavailable, fallback to keyword: %s", e)

    # è¯­ä¹‰å¤±è´¥æˆ–æ— ç»“æœ â†’ å…³é”®è¯é™çº§
    if not reranked:
        kws = _keywords(query)
        scored: List[Dict[str, Any]] = []
        for d in docs:
            r = _kw_score_doc(d, kws, intent)
            if r["score"] > 0: scored.append({"doc": d, **r})
        scored.sort(key=lambda x: x["score"], reverse=True)
        reranked = scored[: max(1, min(top_k, 10))]
        logger.info("ğŸ” Keyword fallback results: %d", len(reranked))

    context = _pick_best_snippets(reranked)
    answer = _format_answer(context, language)

    citations = []
    for item in reranked[:5]:
        d = item["doc"]
        citations.append({
            "title": d.get("title",""),
            "url": d.get("url",""),
            "relevance_score": float(item.get("score",0.0)),
            "source": "local"
        })

    rt = f"{time.time()-t0:.2f}s"
    return {
        "intent": intent,
        "answer": answer,
        "citations": citations,
        "reranked": reranked,
        "rewritten_queries": [],
        "response_time": rt,
        "num_docs": len(reranked),
        "language": language,
        "web_search_used": False,   # éœ€è¦æ—¶ä½ å¯ä»¥æ¥å…¥çœŸå® web æœç´¢å†ç½® True
        "semantic_used": semantic_used
    }

if __name__ == "__main__":
    print(json.dumps(answer_enhanced("What are the core modules in Data Science MSc?", top_k=5, language="en"), ensure_ascii=False, indent=2))
