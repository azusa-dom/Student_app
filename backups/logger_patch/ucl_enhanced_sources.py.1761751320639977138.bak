#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UCL å…¨é¢çˆ¬è™«ç³»ç»Ÿ - åŸºäº Scrapy
æ‰©å±•ç°æœ‰çˆ¬è™«ï¼Œè¦†ç›–æ‰€æœ‰æ•°æ®ç±»å‹
"""

import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from datetime import datetime, timezone
import logging
from pathlib import Path

# ============================================================================
# 1. è¯¾ç¨‹çˆ¬è™«ï¼ˆå·²æœ‰ï¼Œè¿™é‡Œæ˜¯å¢å¼ºç‰ˆï¼‰
# ============================================================================

class UclProgramSpider(scrapy.Spider):
    """è¯¾ç¨‹ä¿¡æ¯çˆ¬è™« - å¢å¼ºç‰ˆ"""
    name = 'ucl_programs'
    allowed_domains = ['ucl.ac.uk']
    
    start_urls = [
        # æœ¬ç§‘è¯¾ç¨‹
        'https://www.ucl.ac.uk/prospective-students/undergraduate/degrees',
        'https://www.ucl.ac.uk/prospective-students/undergraduate/degrees/degrees-z',
        
        # ç ”ç©¶ç”Ÿæˆè¯¾å‹
        'https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees',
        'https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees/taught-degrees-z',
        
        # ç ”ç©¶å‹å­¦ä½
        'https://www.ucl.ac.uk/prospective-students/graduate/research-degrees',
    ]

    custom_settings = {
        'DOWNLOAD_DELAY': 2,
        'CONCURRENT_REQUESTS': 2,
        'ROBOTSTXT_OBEY': True,
        'USER_AGENT': 'Mozilla/5.0 (compatible; UCLBot/1.0; +https://your-site.com/bot)',
        'RETRY_TIMES': 3,
        'FEEDS': {
            'data/ucl_programs_raw.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
                'overwrite': False,
            }
        }
    }

    def parse(self, response):
        """è§£æè¯¾ç¨‹åˆ—è¡¨"""
        # æŸ¥æ‰¾è¯¾ç¨‹é“¾æ¥ - å¤šç§é€‰æ‹©å™¨
        course_selectors = [
            'a[href*="/prospective-students/"][href*="-msc"]',
            'a[href*="/prospective-students/"][href*="-bsc"]',
            'a[href*="/prospective-students/"][href*="-ba"]',
            'a[href*="/prospective-students/"][href*="-ma"]',
            'a[href*="/prospective-students/"][href*="-phd"]',
            '.course-link a',
            '.programme-link a',
            'a[class*="course"]',
        ]
        
        found_links = set()
        for selector in course_selectors:
            for link in response.css(selector):
                url = response.urljoin(link.attrib.get('href', ''))
                if url and url not in found_links:
                    found_links.add(url)
                    yield scrapy.Request(url, callback=self.parse_course)
        
        self.logger.info(f"æ‰¾åˆ° {len(found_links)} ä¸ªè¯¾ç¨‹é“¾æ¥")
        
        # åˆ†é¡µ
        next_page = response.css('a.next::attr(href), a.pagination-next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)

    def parse_course(self, response):
        """è§£æè¯¾ç¨‹è¯¦æƒ…"""
        title = (response.css('h1::text').get() or 
                response.css('title::text').get() or 
                'Unknown').strip()
        
        # æå–æ‰€æœ‰ç›¸å…³ç« èŠ‚
        sections = []
        keywords = [
            'overview', 'about', 'summary', 'description',
            'admission', 'entry', 'requirement', 'qualification',
            'module', 'structure', 'content', 'curriculum',
            'assessment', 'examination', 'coursework',
            'research', 'dissertation', 'project', 'thesis',
            'career', 'employment', 'graduate', 'outcome',
            'fee', 'funding', 'scholarship', 'cost',
            'staff', 'faculty', 'teaching', 'professor',
            'international', 'english', 'language',
            'contact', 'location', 'facility'
        ]
        
        for heading in response.css('h2, h3, .section-title'):
            heading_text = heading.css('::text').get(default='').strip().lower()
            
            if not heading_text or not any(k in heading_text for k in keywords):
                continue
            
            # æå–è¯¥æ ‡é¢˜ä¸‹çš„å†…å®¹
            content_parts = []
            for elem in heading.xpath('following-sibling::*[self::p or self::ul or self::div][position() <= 5]'):
                texts = elem.css('::text, li::text').getall()
                content_parts.extend([t.strip() for t in texts if t.strip()])
            
            content = ' '.join(content_parts).strip()
            if content and len(content) > 50:
                sections.append({
                    'heading': heading_text.title(),
                    'text': content[:2000]  # é™åˆ¶é•¿åº¦
                })
        
        if sections:
            yield {
                'source': 'web',
                'school': 'ucl',
                'type': 'program',
                'url': response.url,
                'title': title,
                'sections': sections,
                'last_crawled_at': datetime.now(timezone.utc).isoformat()
            }

# ============================================================================
# 2. æœåŠ¡çˆ¬è™«ï¼ˆå¢å¼ºç‰ˆï¼‰
# ============================================================================

class UclServicesSpider(scrapy.Spider):
    """å­¦ç”ŸæœåŠ¡çˆ¬è™« - å¢å¼ºç‰ˆ"""
    name = 'ucl_services'
    allowed_domains = ['ucl.ac.uk']
    
    start_urls = [
        # èŒä¸šæœåŠ¡
        'https://www.ucl.ac.uk/careers/',
        'https://www.ucl.ac.uk/careers/what-we-do',
        'https://www.ucl.ac.uk/careers/whats-on',
        'https://www.ucl.ac.uk/careers/your-resources',
        
        # å¥åº·ä¸ç¦åˆ©
        'https://www.ucl.ac.uk/students/support-and-wellbeing',
        'https://www.ucl.ac.uk/students/support-and-wellbeing/mental-health-support',
        'https://www.ucl.ac.uk/students/support-and-wellbeing/disability-support',
        'https://www.ucl.ac.uk/students/support-and-wellbeing/health-services',
        
        # å­¦æœ¯æ”¯æŒ
        'https://www.ucl.ac.uk/students/academic-support',
        'https://www.ucl.ac.uk/students/academic-writing-centre',
        'https://www.ucl.ac.uk/library/support',
        
        # å›½é™…å­¦ç”Ÿ
        'https://www.ucl.ac.uk/students/international-students',
        'https://www.ucl.ac.uk/students/international-students/visas-and-immigration',
        
        # ITæœåŠ¡
        'https://www.ucl.ac.uk/isd/',
        'https://www.ucl.ac.uk/isd/services',
    ]

    custom_settings = {
        'DOWNLOAD_DELAY': 1.5,
        'CONCURRENT_REQUESTS': 2,
        'DEPTH_LIMIT': 2,
        'FEEDS': {
            'data/ucl_services_raw.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
                'overwrite': False,
            }
        }
    }

    def parse(self, response):
        """è§£ææœåŠ¡é¡µé¢"""
        title = (response.css('h1::text').get() or 
                response.css('title::text').get() or 
                'Unknown Service').strip()
        
        # æå–æè¿°
        description_parts = []
        for p in response.css('p, .description, .summary, .intro'):
            text = ' '.join(p.css('::text').getall()).strip()
            if text and len(text) > 20:
                description_parts.append(text)
        
        description = ' '.join(description_parts[:5])[:1500]
        
        # æå–è”ç³»æ–¹å¼
        contact_info = {}
        
        # ç”µå­é‚®ä»¶
        emails = response.css('a[href^="mailto:"]::attr(href)').getall()
        if emails:
            contact_info['email'] = emails[0].replace('mailto:', '')
        
        # ç”µè¯
        phones = response.css('a[href^="tel:"]::text').getall()
        if phones:
            contact_info['phone'] = phones[0]
        
        # åœ°å€
        address_selectors = ['.address', '.location', '[class*="address"]', '[class*="location"]']
        for sel in address_selectors:
            addr = response.css(f'{sel}::text').get()
            if addr:
                contact_info['address'] = addr.strip()
                break
        
        # åˆ†ç±»
        url_lower = response.url.lower()
        if 'career' in url_lower:
            category = 'careers'
        elif any(k in url_lower for k in ['wellbeing', 'health', 'disability', 'mental']):
            category = 'wellbeing'
        elif 'library' in url_lower:
            category = 'library'
        elif 'isd' in url_lower or 'it' in url_lower:
            category = 'it_services'
        elif 'international' in url_lower:
            category = 'international'
        else:
            category = 'support'
        
        yield {
            'source': 'web',
            'school': 'ucl',
            'type': 'service',
            'url': response.url,
            'name': title,
            'category': category,
            'description': description,
            'contact': contact_info,
            'last_crawled_at': datetime.now(timezone.utc).isoformat()
        }
        
        # è·Ÿè¸ªç›¸å…³é“¾æ¥
        if response.meta.get('depth', 0) < 1:
            for href in response.css('a[href*="support"], a[href*="service"]::attr(href)').getall():
                yield response.follow(href, self.parse, meta={'depth': response.meta.get('depth', 0) + 1})

# ============================================================================
# 3. ä½å®¿çˆ¬è™«ï¼ˆæ–°å¢ï¼‰
# ============================================================================

class UclAccommodationSpider(scrapy.Spider):
    """ä½å®¿ä¿¡æ¯çˆ¬è™«"""
    name = 'ucl_accommodation'
    allowed_domains = ['ucl.ac.uk']
    
    start_urls = [
        'https://www.ucl.ac.uk/accommodation/residences',
        'https://www.ucl.ac.uk/accommodation/search',
    ]

    custom_settings = {
        'DOWNLOAD_DELAY': 2,
        'FEEDS': {
            'data/ucl_accommodation_raw.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
            }
        }
    }

    def parse(self, response):
        """è§£æä½å®¿åˆ—è¡¨"""
        # æŸ¥æ‰¾å®¿èˆé“¾æ¥
        for hall in response.css('.residence-card, .accommodation-item'):
            hall_link = hall.css('a::attr(href)').get()
            if hall_link:
                yield response.follow(hall_link, self.parse_hall)
        
        # ç›´æ¥åˆ—è¡¨é¡µ
        for link in response.css('a[href*="/residences/"]::attr(href)').getall():
            if 'residences/' in link:
                yield response.follow(link, self.parse_hall)

    def parse_hall(self, response):
        """è§£æå®¿èˆè¯¦æƒ…"""
        name = response.css('h1::text').get(default='Unknown Hall').strip()
        
        # æå–ä»·æ ¼
        price_text = response.css('.price, [class*="price"], [class*="cost"]::text').get(default='')
        
        # æå–ä½ç½®
        location = response.css('.location, [class*="location"]::text').get(default='')
        
        # æå–è®¾æ–½
        facilities = []
        for item in response.css('.facility, .amenity, li'):
            text = item.css('::text').get(default='').strip()
            if text and len(text) < 100:
                facilities.append(text)
        
        # æå–æè¿°
        description = ' '.join(response.css('p::text').getall()[:3])
        
        yield {
            'source': 'web',
            'school': 'ucl',
            'type': 'accommodation',
            'url': response.url,
            'name': name,
            'price': price_text,
            'location': location,
            'facilities': facilities[:20],
            'description': description[:1000],
            'last_crawled_at': datetime.now(timezone.utc).isoformat()
        }

# ============================================================================
# 4. å›¾ä¹¦é¦†çˆ¬è™«ï¼ˆæ–°å¢ï¼‰
# ============================================================================

class UclLibrariesSpider(scrapy.Spider):
    """å›¾ä¹¦é¦†ä¿¡æ¯çˆ¬è™«"""
    name = 'ucl_libraries'
    allowed_domains = ['ucl.ac.uk']
    
    start_urls = [
        'https://www.ucl.ac.uk/library/',
        'https://www.ucl.ac.uk/library/about-us/library-locations',
        'https://www.ucl.ac.uk/library/opening-hours',
    ]

    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'FEEDS': {
            'data/ucl_libraries_raw.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
            }
        }
    }

    def parse(self, response):
        """è§£æå›¾ä¹¦é¦†ä¿¡æ¯"""
        # æŸ¥æ‰¾å›¾ä¹¦é¦†ä½ç½®é“¾æ¥
        for link in response.css('a[href*="library"]::attr(href)').getall():
            if 'location' in link or 'library' in link:
                yield response.follow(link, self.parse_library)
        
        # å¦‚æœæ˜¯å›¾ä¹¦é¦†è¯¦æƒ…é¡µ
        if 'library' in response.url:
            yield from self.parse_library(response)

    def parse_library(self, response):
        """è§£æå›¾ä¹¦é¦†è¯¦æƒ…"""
        name = response.css('h1::text').get(default='').strip()
        
        if not name or 'UCL' not in name:
            return
        
        # æå–å¼€æ”¾æ—¶é—´
        hours_text = ' '.join(response.css('.hours, .opening-hours, [class*="opening"]::text').getall())
        
        # æå–åœ°å€
        address = response.css('.address, [class*="address"]::text').get(default='')
        
        # æå–æœåŠ¡
        services = []
        for item in response.css('.service li, .facility li'):
            text = item.css('::text').get(default='').strip()
            if text:
                services.append(text)
        
        yield {
            'source': 'web',
            'school': 'ucl',
            'type': 'library',
            'url': response.url,
            'name': name,
            'address': address,
            'opening_hours': hours_text[:500],
            'services': services,
            'last_crawled_at': datetime.now(timezone.utc).isoformat()
        }

# ============================================================================
# 5. æ–°é—»/æ´»åŠ¨çˆ¬è™«ï¼ˆæ–°å¢ï¼‰
# ============================================================================

class UclNewsSpider(scrapy.Spider):
    """æ–°é—»å’Œæ´»åŠ¨çˆ¬è™«"""
    name = 'ucl_news'
    allowed_domains = ['ucl.ac.uk']
    
    start_urls = [
        'https://www.ucl.ac.uk/news/',
        'https://www.ucl.ac.uk/events/',
    ]

    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'DEPTH_LIMIT': 1,
        'FEEDS': {
            'data/ucl_news_raw.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
            }
        }
    }

    def parse(self, response):
        """è§£ææ–°é—»/æ´»åŠ¨åˆ—è¡¨"""
        # æ–°é—»æ–‡ç« 
        for article in response.css('article, .news-item, .event-item'):
            title = article.css('h2::text, h3::text, .title::text').get(default='').strip()
            link = article.css('a::attr(href)').get()
            date = article.css('.date, time::text').get(default='')
            summary = article.css('p::text, .summary::text').get(default='')
            
            if title and link:
                yield {
                    'source': 'web',
                    'school': 'ucl',
                    'type': 'news' if '/news/' in response.url else 'event',
                    'url': response.urljoin(link),
                    'title': title,
                    'date': date,
                    'summary': summary[:500],
                    'last_crawled_at': datetime.now(timezone.utc).isoformat()
                }

# ============================================================================
# ä¸»è¿è¡Œå‡½æ•°
# ============================================================================

def run_spider(spider_class, output_file=None):
    """è¿è¡Œå•ä¸ªçˆ¬è™«"""
    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/5.0 (compatible; UCLBot/1.0)',
        'ROBOTSTXT_OBEY': True,
        'LOG_LEVEL': 'INFO',
        'CONCURRENT_REQUESTS': 2,
        'DOWNLOAD_DELAY': 1.5,
    })
    
    process.crawl(spider_class)
    process.start()

def run_all_spiders():
    """è¿è¡Œæ‰€æœ‰çˆ¬è™«"""
    spiders = [
        UclProgramSpider,
        UclServicesSpider,
        UclAccommodationSpider,
        UclLibrariesSpider,
        UclNewsSpider,
    ]
    
    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/5.0 (compatible; UCLBot/1.0)',
        'ROBOTSTXT_OBEY': True,
        'LOG_LEVEL': 'INFO',
        'CONCURRENT_REQUESTS': 2,
        'DOWNLOAD_DELAY': 2,
    })
    
    for spider in spiders:
        process.crawl(spider)
    
    process.start()

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        spider_name = sys.argv[1]
        spider_map = {
            'programs': UclProgramSpider,
            'services': UclServicesSpider,
            'accommodation': UclAccommodationSpider,
            'libraries': UclLibrariesSpider,
            'news': UclNewsSpider,
        }
        
        if spider_name in spider_map:
            run_spider(spider_map[spider_name])
        else:
            print(f"Unknown spider: {spider_name}")
            print(f"Available: {', '.join(spider_map.keys())}")
    else:
        print("ğŸš€ è¿è¡Œæ‰€æœ‰çˆ¬è™«...")
        run_all_spiders()