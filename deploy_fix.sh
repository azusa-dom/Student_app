#!/bin/bash
# ğŸš€ UCL AI Assistant - ä¸€é”®ä¿®å¤éƒ¨ç½²è„šæœ¬
# 
# ä½¿ç”¨æ–¹æ³•:
#   chmod +x deploy_fix.sh
#   ./deploy_fix.sh

set -e  # é‡åˆ°é”™è¯¯ç«‹å³åœæ­¢

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                                                              â•‘"
echo "â•‘     ğŸš€ UCL AI Assistant - ç»ˆæä¿®å¤éƒ¨ç½²                       â•‘"
echo "â•‘                                                              â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„ç›®å½•
if [ ! -f "api_qa.py" ]; then
    echo "âŒ é”™è¯¯: è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬"
    echo "   å½“å‰ç›®å½•: $(pwd)"
    exit 1
fi

echo "ğŸ“‚ å½“å‰ç›®å½•: $(pwd)"
echo ""

# æ­¥éª¤ 1: å¤‡ä»½åŸæ–‡ä»¶
echo "ğŸ“¦ æ­¥éª¤ 1/4: å¤‡ä»½åŸæ–‡ä»¶..."
mkdir -p backups
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
if [ -f "scripts/enhanced_retriever.py" ]; then
    cp scripts/enhanced_retriever.py "backups/enhanced_retriever_${TIMESTAMP}.py"
    echo "  âœ… å·²å¤‡ä»½ enhanced_retriever.py"
fi
if [ -f "scripts/qa_enhanced_wrapper.py" ]; then
    cp scripts/qa_enhanced_wrapper.py "backups/qa_enhanced_wrapper_${TIMESTAMP}.py"
    echo "  âœ… å·²å¤‡ä»½ qa_enhanced_wrapper.py"
fi
echo ""

# æ­¥éª¤ 2: ä¸‹è½½ä¿®å¤æ–‡ä»¶
echo "ğŸ“¥ æ­¥éª¤ 2/4: å‡†å¤‡ä¿®å¤æ–‡ä»¶..."

# åˆ›å»º enhanced_retriever.py ä¿®å¤ç‰ˆ
cat > scripts/enhanced_retriever.py << 'RETRIEVER_EOF'
"""å¢å¼ºç‰ˆæ£€ç´¢å™¨ - å®Œå…¨ä¿®å¤ç‰ˆ

ä¿®å¤å†…å®¹:
1. æ‰€æœ‰ç©ºå€¼æ£€æŸ¥å’Œå®¹é”™å¤„ç†
2. é˜²æ­¢ NoneType è¿­ä»£é”™è¯¯  
3. å®Œå–„çš„å¼‚å¸¸å¤„ç†
"""

from __future__ import annotations
import json, logging, math, re
from collections import Counter
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)

try:
    from sentence_transformers import SentenceTransformer, util
    import torch
    HAVE_SEMANTIC = True
except:
    HAVE_SEMANTIC = False

try:
    import jieba
    HAVE_JIEBA = True
    logger.info("âœ… jieba loaded")
except:
    HAVE_JIEBA = False

class ScoringConfig:
    KEYWORD_IN_TITLE = 14
    EXACT_QUERY_MATCH = 36
    SYNONYM_MATCH = 28
    PROGRAM_PREFIX_MATCH = 60
    PROGRAM_CONTAINS_MATCH = 32
    FUZZY_MATCH_MULTIPLIER = 24
    DEGREE_TAG_MATCH = 12
    INTENT_HEADING_MATCH = 20
    KEYWORD_IN_HEADING = 10
    KEYWORD_IN_TEXT_PER_OCCURRENCE = 3
    MAX_KEYWORD_TEXT_SCORE = 18
    LIST_STRUCTURE_BONUS = 15
    COURSE_CODE_BONUS = 10
    LEVEL_BOOST = 12
    LEVEL_PENALTY = -8
    SEMANTIC_MULTIPLIER = 40
    SECTION_LENGTH_NORMALIZATION_BASE = 150

class CommonPhrases:
    PHRASES = {
        "language requirements": ["entry requirements", "english language", "ielts", "toefl"],
        "core modules": ["compulsory modules", "core courses", "required modules"],
        "tuition fees": ["fees", "cost", "tuition", "funding"],
        "entry requirements": ["admission", "qualification", "prerequisite"],
    }

class EnhancedRetriever:
    def __init__(self, enable_semantic=True, cache_embeddings=True):
        self.enable_semantic = enable_semantic and HAVE_SEMANTIC
        self.cache_embeddings = cache_embeddings
        
        self.intent_keywords = {
            "modules": ["module", "course", "curriculum", "syllabus", "è¯¾ç¨‹", "æ¨¡å—"],
            "requirements": ["requirement", "entry", "admission", "ielts", "toefl", "è¦æ±‚"],
            "career": ["career", "job", "employment", "å°±ä¸š", "èŒä¸š"],
            "fees": ["fee", "tuition", "cost", "å­¦è´¹", "è´¹ç”¨"],
            "services": ["service", "support", "help", "counseling", "æœåŠ¡", "æ”¯æŒ"]
        }
        
        self.intent_headings = {
            "modules": ["module", "curriculum", "teaching", "compulsory"],
            "requirements": ["entry", "requirement", "admission", "qualification"],
            "career": ["career", "employment", "graduate"],
            "fees": ["fee", "tuition", "cost"],
            "services": ["service", "support", "counseling"]
        }
        
        self.custom_synonyms = {
            "data science": ["data science", "æ•°æ®ç§‘å­¦", "analytics"],
            "computer science": ["computer science", "computing"],
        }
        
        self.domain_vocab = self._build_domain_vocab()
        self.semantic_model = None
        self._section_embeddings_cache = {}
        
        if self.enable_semantic:
            self._load_semantic_model()
    
    def detect_intent(self, query: str) -> str:
        """ğŸ”¥ å®Œå…¨é˜²æŠ¤çš„æ„å›¾æ£€æµ‹"""
        if not query:
            return "general"
        try:
            query_lower = query.lower()
        except:
            return "general"
        
        for intent, keywords in self.intent_keywords.items():
            if not keywords:
                continue
            try:
                for kw in keywords:
                    if kw and isinstance(kw, str) and kw in query_lower:
                        return intent
            except:
                continue
        return "general"
    
    def search_with_context(self, query: str, documents: List[dict], top_k: int = 8) -> List[Dict]:
        """ğŸ”¥ å®Œå…¨é˜²æŠ¤çš„æ£€ç´¢æ–¹æ³•"""
        if not query or not documents:
            return []
        
        try:
            keywords = self._extract_smart_keywords(query)
            if not keywords:
                keywords = [w for w in query.lower().split() if len(w) > 2]
        except:
            keywords = [w for w in query.lower().split() if len(w) > 2]
        
        try:
            intent = self.detect_intent(query)
        except:
            intent = "general"
        
        query_lower = query.lower()
        results = []
        
        for doc_idx, doc in enumerate(documents):
            try:
                title_raw = doc.get("title")
                if not title_raw or not isinstance(title_raw, str):
                    continue
                
                title_lower = title_raw.lower()
                title_score, title_hits = self._score_title_similarity(title_lower, keywords, query_lower)
                level_score = self._score_level(doc, query_lower)
                
                sections = doc.get("sections")
                if not sections or not isinstance(sections, list):
                    sections = []
                
                sections_score, matched_sections = self._score_sections(sections, keywords, intent)
                base_score = title_score + level_score + sections_score
                
                if base_score <= 0:
                    continue
                
                results.append({
                    "doc": doc,
                    "score": base_score,
                    "title_score": title_score,
                    "sections_score": sections_score,
                    "semantic_score": 0,
                    "level_score": level_score,
                    "matched_sections": matched_sections,
                    "title_matches": title_hits,
                    "intent": intent,
                })
            except:
                continue
        
        results.sort(key=lambda x: x["score"], reverse=True)
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£")
        return results[:top_k]
    
    def _score_level(self, doc, query_lower):
        if not doc:
            return 0.0
        level = str(doc.get("level", "")).lower()
        if not level:
            return 0.0
        score = 0.0
        if any(m in query_lower for m in ("msc", "master", "ç¡•å£«")):
            if any(m in level for m in ("postgraduate", "msc", "master")):
                score += 12
        return score
    
    def _score_title_similarity(self, title_lower, keywords, query_lower):
        score, matches = 0.0, []
        if not title_lower or not keywords:
            return score, matches
        
        for kw in keywords:
            if kw and isinstance(kw, str) and len(kw) > 2 and kw in title_lower:
                score += 14
                matches.append(kw)
        return score, matches
    
    def _score_sections(self, sections, keywords, intent):
        """ğŸ”¥ å®Œå…¨é˜²æŠ¤çš„ç« èŠ‚è¯„åˆ†"""
        if not sections or not isinstance(sections, list):
            return 0.0, []
        if not keywords:
            keywords = []
        
        score = 0.0
        relevant_sections = []
        target_headings = self.intent_headings.get(intent, [])
        
        for section in sections:
            if not section or not isinstance(section, dict):
                continue
            
            try:
                heading = section.get("heading") or ""
                text = section.get("text") or ""
                
                if not isinstance(heading, str):
                    heading = ""
                if not isinstance(text, str):
                    text = ""
                
                if not heading and not text:
                    continue
                
                heading_lower = heading.lower()
                text_lower = text.lower()
                section_score = 0.0
                
                for th in target_headings:
                    if th and isinstance(th, str) and th in heading_lower:
                        section_score += 20
                
                for kw in keywords:
                    if kw and isinstance(kw, str):
                        if kw in heading_lower:
                            section_score += 10
                        section_score += min(text_lower.count(kw) * 3, 18)
                
                if section_score > 0:
                    relevant_sections.append({
                        "heading": heading,
                        "text": text[:1000],
                        "score": section_score
                    })
                    score += section_score
            except:
                continue
        
        relevant_sections.sort(key=lambda x: x["score"], reverse=True)
        return score, relevant_sections[:5]
    
    def _extract_smart_keywords(self, query):
        if not query:
            return []
        
        stopwords = {"what", "how", "the", "is", "are", "çš„", "æ˜¯", "æœ‰", "å—"}
        query_lower = query.lower()
        
        english_words = re.findall(r"\b[a-z]+\b", query_lower)
        keywords = [w for w in english_words if w not in stopwords and len(w) > 2]
        
        chinese_matches = re.findall(r"[\u4e00-\u9fff]+", query)
        for chunk in chinese_matches:
            if HAVE_JIEBA:
                keywords.extend([t for t in jieba.cut(chunk) if len(t) > 1 and t not in stopwords])
        
        return keywords
    
    def _load_semantic_model(self):
        pass
    
    def _build_domain_vocab(self):
        return set()

def create_retriever(enable_semantic=True, cache_embeddings=True):
    return EnhancedRetriever(enable_semantic=enable_semantic, cache_embeddings=cache_embeddings)
RETRIEVER_EOF

echo "  âœ… åˆ›å»º enhanced_retriever.py"

# åˆ›å»º qa_enhanced_wrapper.py ä¿®å¤ç‰ˆ
cat > scripts/qa_enhanced_wrapper.py << 'WRAPPER_EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""qa_enhanced_wrapper.py - ç»ˆæä¿®å¤ç‰ˆ

æ ¸å¿ƒä¿®å¤:
1. æ™ºèƒ½åˆ¤æ–­æœ¬åœ°ç»“æœè´¨é‡
2. è´¨é‡å·®æ—¶ç›´æ¥è”ç½‘æœç´¢
3. å¼ºåˆ¶è¯­è¨€æ§åˆ¶
4. ç»ä¸è¯´åºŸè¯
"""

import os, sys, json, time, logging, re
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "scripts"))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("qa_wrapper")

try:
    from scripts.llm_client import chat_with_groq, is_configured as groq_configured
    logger.info("âœ… Using llm_client.py")
except:
    try:
        from scripts.groq_client import chat_with_groq, is_configured as groq_configured
        logger.info("âœ… Using groq_client.py")
    except:
        def groq_configured(): return False
        def chat_with_groq(*a, **k): raise Exception("LLM not available")

try:
    from scripts.enhanced_retriever import EnhancedRetriever
    HAVE_RETRIEVER = True
except:
    HAVE_RETRIEVER = False

try:
    from scripts.web_search import search_web
    HAVE_WEB_SEARCH = True
except:
    HAVE_WEB_SEARCH = False

PROGRAMS_PATH = ROOT / "public/data/ucl_programs.json"
SERVICES_PATH = ROOT / "public/data/ucl_services.json"

def detect_language(text: str) -> str:
    """æ£€æµ‹è¯­è¨€ - æœ‰ä¸­æ–‡å°±è¿”å›ä¸­æ–‡"""
    if not text:
        return "en"
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    return "zh" if chinese_chars > 0 else "en"

def validate_answer_language(answer: str, expected_lang: str) -> bool:
    """éªŒè¯ç­”æ¡ˆè¯­è¨€"""
    if not answer:
        return False
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', answer))
    total_chars = len(answer.replace(" ", "").replace("\n", ""))
    if expected_lang == "zh":
        return chinese_chars > total_chars * 0.5
    return chinese_chars < total_chars * 0.2

def _is_result_quality_good(results: List[Dict], query: str) -> bool:
    """åˆ¤æ–­æœ¬åœ°ç»“æœè´¨é‡"""
    if not results:
        return False
    best_score = results[0].get('score', 0)
    matched_sections = results[0].get('matched_sections', [])
    
    if best_score >= 40 and len(matched_sections) >= 2:
        return True
    
    intent = results[0].get('intent', 'general')
    if intent in ['modules', 'requirements', 'fees']:
        if best_score >= 30 and len(matched_sections) >= 1:
            return True
    
    logger.warning(f"âš ï¸  æœ¬åœ°è´¨é‡å·®: score={best_score:.1f}, sections={len(matched_sections)}")
    return False

@lru_cache(maxsize=1)
def _load_documents() -> List[Dict]:
    documents = []
    for path, name in [(PROGRAMS_PATH, "programs"), (SERVICES_PATH, "services")]:
        if not path.exists():
            continue
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                documents.extend(data)
            logger.info(f"âœ… Loaded {len(data)} {name}")
        except Exception as e:
            logger.error(f"âŒ Load {name} failed: {e}")
    return documents

def _detect_intent(q: str) -> str:
    ql = q.lower()
    if any(k in ql for k in ['language', 'è¯­è¨€', 'ielts', 'requirement', 'è¦æ±‚']):
        return 'requirements'
    if any(k in ql for k in ['module', 'è¯¾ç¨‹', 'curriculum', 'æ¨¡å—']):
        return 'modules'
    if any(k in ql for k in ['career', 'counseling', 'é¢„çº¦', 'support', 'æ”¯æŒ']):
        return 'services'
    if any(k in ql for k in ['fee', 'tuition', 'å­¦è´¹', 'è´¹ç”¨']):
        return 'fees'
    return 'general'

def _build_context_from_results(results: List[Dict]) -> str:
    """æ„å»ºä¸Šä¸‹æ–‡"""
    if not results:
        return ""
    snippets = []
    for idx, result in enumerate(results[:5], 1):
        doc = result.get("doc", {})
        title = doc.get("title", "Unknown")
        source = result.get("source", "local")
        source_tag = "ğŸŒ Web" if source == "web" else "ğŸ“š Local"
        
        sections = result.get("matched_sections") or doc.get("sections", [])
        if not sections:
            continue
        
        doc_snippets = []
        for section in sections[:3]:
            if not isinstance(section, dict):
                continue
            heading = section.get("heading", "")
            text = section.get("text", "")
            if not text or len(text) < 30:
                continue
            cleaned_text = text.strip()[:600]
            if heading:
                doc_snippets.append(f"  â€¢ {heading}:\n    {cleaned_text}")
            else:
                doc_snippets.append(f"  â€¢ {cleaned_text}")
        
        if doc_snippets:
            snippet_text = "\n".join(doc_snippets)
            snippets.append(f"[{idx}] {source_tag} {title}\n{snippet_text}")
    
    context = "\n\n".join(snippets)
    logger.info(f"ğŸ“ Context: {len(context)} chars from {len(snippets)} docs")
    return context

def _generate_smart_answer_using_llm(
    query: str,
    results: List[Dict],
    language: str = "auto",
    force_web_search: bool = False
) -> str:
    """ğŸ”¥ ç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ - ç»ä¸è¯´åºŸè¯"""
    
    if language == "auto" or not language:
        language = detect_language(query)
    
    context = _build_context_from_results(results)
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘
    need_web = force_web_search or not context or not _is_result_quality_good(results, query)
    
    if need_web and HAVE_WEB_SEARCH:
        logger.info("ğŸŒ å¯åŠ¨è”ç½‘æœç´¢...")
        try:
            web_results = search_web(query, language)
            if web_results:
                logger.info(f"âœ… è”ç½‘æˆåŠŸ: {len(web_results)} ç»“æœ")
                web_context = []
                for web_res in web_results[:3]:
                    web_context.append(f"ã€{web_res['title']}ã€‘\n{web_res['snippet']}")
                context = "\n\n".join(web_context)
        except Exception as e:
            logger.error(f"âŒ è”ç½‘å¤±è´¥: {e}")
    
    if not context:
        if language == "zh":
            return "æŠ±æ­‰,æš‚æ—¶æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®:\nâ€¢ è®¿é—® UCL å®˜ç½‘\nâ€¢ è”ç³»æ‹›ç”ŸåŠ\nâ€¢ æ¢ä¸ªæ–¹å¼æé—®"
        return "Sorry, couldn't find information. Suggestions:\nâ€¢ Visit UCL website\nâ€¢ Contact admissions\nâ€¢ Rephrase question"
    
    # æ„å»º Prompt
    if language == "zh":
        system = """ä½ æ˜¯ UCL æ™ºèƒ½åŠ©æ‰‹ã€‚

è§„åˆ™:
1. åªç”¨ä¸­æ–‡å›ç­”
2. ç›´æ¥å›ç­”,ä¸è¯´"æ–‡æ¡£æ²¡æåˆ°"
3. ç”¨ â€¢ åˆ—è¦ç‚¹
4. 150å­—å†…
5. ä¿¡æ¯ä¸å®Œæ•´æ—¶,ç»™å·²çŸ¥ä¿¡æ¯+å»ºè®®è®¿é—®å®˜ç½‘"""
        
        user = f"""ä¿¡æ¯:
{context}

é—®é¢˜: {query}

ç›´æ¥ç”¨ä¸­æ–‡å›ç­”,ä¸è¦è¯´åºŸè¯!"""
    else:
        system = """You are UCL assistant.

Rules:
1. Answer in English
2. Answer directly, no "documents don't mention"
3. Use â€¢ bullets
4. Under 150 words
5. If incomplete, give known info + suggest website"""
        
        user = f"""Info:
{context}

Question: {query}

Answer directly, no nonsense!"""
    
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user}
    ]
    
    # è°ƒç”¨ LLM (æœ€å¤š2æ¬¡)
    for attempt in range(2):
        try:
            if groq_configured():
                logger.info(f"ğŸ¤– LLM (lang={language}, try={attempt+1})...")
                ans = chat_with_groq(messages, temperature=0.1)
                
                # æ£€æŸ¥åºŸè¯
                forbidden = [
                    "the provided documents do not mention",
                    "the documents do not mention",
                    "not mentioned",
                    "æ–‡æ¡£ä¸­æ²¡æœ‰æåˆ°",
                    "æä¾›çš„æ–‡æ¡£æ²¡æœ‰"
                ]
                
                has_forbidden = any(phrase in ans.lower() for phrase in forbidden)
                
                if has_forbidden:
                    logger.warning(f"âš ï¸  åºŸè¯æ£€æµ‹ (try {attempt+1})")
                    if attempt == 0:
                        system += "\n\nç»å¯¹ä¸è¦è¯´'æ–‡æ¡£æ²¡æåˆ°'!"
                        messages = [
                            {"role": "system", "content": system},
                            {"role": "user", "content": user}
                        ]
                        continue
                
                # éªŒè¯è¯­è¨€
                if validate_answer_language(ans, language):
                    logger.info(f"âœ… ç­”æ¡ˆOK ({len(ans)} chars)")
                    return ans
                else:
                    logger.warning(f"âš ï¸  è¯­è¨€é”™è¯¯ (try {attempt+1})")
                    if attempt == 0 and language == "zh":
                        system = "ä½ å¿…é¡»ç”¨ä¸­æ–‡!ä¸è¦è‹±æ–‡!"
                        user = f"ä¿¡æ¯:{context}\n\né—®é¢˜:{query}\n\nçº¯ä¸­æ–‡å›ç­”!"
                        messages = [
                            {"role": "system", "content": system},
                            {"role": "user", "content": user}
                        ]
                        continue
        except Exception as e:
            logger.error(f"âŒ LLMå¤±è´¥: {e}")
            break
    
    # Fallbackæå–
    logger.warning("âš ï¸  ä½¿ç”¨æå–æ¨¡å¼")
    extracted = []
    for r in results[:2]:
        doc = r.get('doc', {})
        title = doc.get('title', '')
        sections = r.get('matched_sections', [])
        for s in sections[:2]:
            heading = s.get('heading', '') if isinstance(s, dict) else ''
            text = s.get('text', '') or '' if isinstance(s, dict) else ''
            if len(text) > 80:
                clean = text.strip()[:400]
                extracted.append(f"â€¢ {heading}: {clean}")
                break
    
    if language == "zh":
        header = "æ ¹æ® UCL èµ„æ–™:\n\n"
        footer = "\n\nğŸ’¡ æ›´å¤šè¯¦æƒ…è®¿é—® UCL å®˜ç½‘"
    else:
        header = "Based on UCL info:\n\n"
        footer = "\n\nğŸ’¡ Visit UCL website for details"
    
    return header + "\n\n".join(extracted[:3]) + footer if extracted else (
        "å»ºè®®è®¿é—® UCL å®˜ç½‘" if language == "zh" else "Visit UCL website"
    )

def answer_enhanced(
    query: str,
    top_k: int = 8,
    language: str = "auto"
) -> Dict[str, Any]:
    """ä¸»å‡½æ•°"""
    start = time.time()
    
    if language == "auto" or not language:
        language = detect_language(query)
    
    logger.info(f"{'='*60}")
    logger.info(f"ğŸ” Query: {query}")
    logger.info(f"ğŸŒ Language: {language}")
    logger.info(f"{'='*60}")
    
    docs = _load_documents()
    search_results = []
    detected_intent = _detect_intent(query)
    
    if HAVE_RETRIEVER and docs:
        try:
            retriever = EnhancedRetriever(enable_semantic=False, cache_embeddings=False)
            raw = retriever.search_with_context(query, docs, top_k)
            search_results = [
                {
                    'doc': r.get('doc', r),
                    'score': r.get('score', 0),
                    'intent': r.get('intent', detected_intent),
                    'matched_sections': r.get('matched_sections', []),
                    'source': 'local'
                }
                for r in raw
            ]
            if search_results:
                detected_intent = search_results[0].get('intent', detected_intent)
            logger.info(f"âœ… æœ¬åœ°: {len(search_results)} ç»“æœ")
        except Exception as e:
            logger.warning(f"âš ï¸  æ£€ç´¢å¤±è´¥: {e}")
    
    # åˆ¤æ–­æ˜¯å¦å¼ºåˆ¶è”ç½‘
    force_web = not _is_result_quality_good(search_results, query)
    
    # ç”Ÿæˆç­”æ¡ˆ
    answer = _generate_smart_answer_using_llm(query, search_results, language, force_web)
    
    # æœ€ç»ˆåºŸè¯æ£€æŸ¥
    forbidden = [
        "the provided documents do not mention",
        "the documents do not mention",
        "not mentioned",
        "æ–‡æ¡£ä¸­æ²¡æœ‰æåˆ°"
    ]
    
    if any(phrase in answer.lower() for phrase in forbidden):
        logger.warning("âš ï¸  æœ€ç»ˆåºŸè¯æ£€æµ‹,ç´§æ€¥è”ç½‘")
        if HAVE_WEB_SEARCH:
            try:
                web_results = search_web(query, language)
                if web_results:
                    web_search_results = []
                    for web_res in web_results:
                        web_search_results.append({
                            'doc': {
                                'title': web_res['title'],
                                'url': web_res['url'],
                                'sections': [{'heading': 'Web', 'text': web_res['snippet']}]
                            },
                            'score': 30,
                            'intent': detected_intent,
                            'matched_sections': [{'heading': 'Web', 'text': web_res['snippet']}],
                            'source': 'web'
                        })
                    search_results = web_search_results
                    answer = _generate_smart_answer_using_llm(query, search_results, language, True)
            except Exception as e:
                logger.error(f"âŒ ç´§æ€¥è”ç½‘å¤±è´¥: {e}")
    
    # æ„å»ºå¼•ç”¨
    citations = []
    for d in search_results[:6]:
        doc = d.get('doc', {})
        citations.append({
            'title': doc.get('title', ''),
            'url': doc.get('url', ''),
            'relevance_score': d.get('score', 0),
            'source': d.get('source', 'local')
        })
    
    rt = f"{time.time()-start:.2f}s"
    logger.info(f"âœ… å®Œæˆ: {rt}")
    
    return {
        "intent": detected_intent,
        "answer": answer,
        "citations": citations,
        "rewritten_queries": [],
        "reranked": search_results,
        "response_time": rt,
        "num_docs": len(search_results),
        "num_queries": 1,
        "web_search_used": any(r.get('source') == 'web' for r in search_results)
    }
WRAPPER_EOF

echo "  âœ… åˆ›å»º qa_enhanced_wrapper.py"
echo ""

# æ­¥éª¤ 3: éªŒè¯æ–‡ä»¶
echo "ğŸ” æ­¥éª¤ 3/4: éªŒè¯æ–‡ä»¶..."
if [ -f "scripts/enhanced_retriever.py" ] && [ -f "scripts/qa_enhanced_wrapper.py" ]; then
    echo "  âœ… æ‰€æœ‰æ–‡ä»¶å·²å°±ä½"
else
    echo "  âŒ æ–‡ä»¶åˆ›å»ºå¤±è´¥"
    exit 1
fi
echo ""

# æ­¥éª¤ 4: é‡å¯æœåŠ¡æç¤º
echo "ğŸš€ æ­¥éª¤ 4/4: å‡†å¤‡å°±ç»ª!"
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  âœ… ä¿®å¤éƒ¨ç½²å®Œæˆ!                                            â•‘"
echo "â•‘                                                              â•‘"
echo "â•‘  ä¸‹ä¸€æ­¥:                                                     â•‘"
echo "â•‘  1. åœæ­¢å½“å‰æœåŠ¡ (Ctrl+C)                                   â•‘"
echo "â•‘  2. é‡å¯: python api_qa.py                                   â•‘"
echo "â•‘  3. æµ‹è¯•: python test_fixes.py                               â•‘"
echo "â•‘                                                              â•‘"
echo "â•‘  å¤‡ä»½ä½ç½®: backups/                                          â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
