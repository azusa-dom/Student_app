[1mdiff --git a/scripts/qa_enhanced_wrapper.py b/scripts/qa_enhanced_wrapper.py[m
[1mindex 6cf40e1..bf72f74 100644[m
[1m--- a/scripts/qa_enhanced_wrapper.py[m
[1m+++ b/scripts/qa_enhanced_wrapper.py[m
[36m@@ -5,6 +5,7 @@[m
 import os, sys, json, time, logging, re[m
 from pathlib import Path[m
 from typing import List, Dict, Any[m
[32m+[m[32mfrom functools import lru_cache[m
 [m
 ROOT = Path(__file__).resolve().parents[1][m
 sys.path.insert(0, str(ROOT))[m
[36m@@ -31,10 +32,31 @@[m [mtry:[m
     from scripts.enhanced_retriever import EnhancedRetriever[m
     HAVE_RETRIEVER = True[m
     logger.info("✅ Enhanced retriever loaded")[m
[32m+[m[32m    # 模块级检索器实例[m
[32m+[m[32m    retriever = EnhancedRetriever()[m
 except Exception as e:[m
     HAVE_RETRIEVER = False[m
     logger.warning(f"⚠️  Retriever not available: {e}")[m
 [m
[32m+[m[32m# ============ 导入分词工具 ============[m
[32m+[m[32mtry:[m
[32m+[m[32m    import jieba[m
[32m+[m[32m    HAVE_JIEBA = True[m
[32m+[m[32mexcept ImportError:[m
[32m+[m[32m    HAVE_JIEBA = False[m
[32m+[m[32m    logger.warning("⚠️ jieba not available, fallback to simple split for Chinese")[m
[32m+[m
[32m+[m[32mtry:[m
[32m+[m[32m    import nltk[m
[32m+[m[32m    from nltk.stem import WordNetLemmatizer[m
[32m+[m[32m    nltk.download('wordnet', quiet=True)[m
[32m+[m[32m    nltk.download('omw-1.4', quiet=True)[m
[32m+[m[32m    lemmatizer = WordNetLemmatizer()[m
[32m+[m[32m    HAVE_NLTK = True[m
[32m+[m[32mexcept ImportError:[m
[32m+[m[32m    HAVE_NLTK = False[m
[32m+[m[32m    logger.warning("⚠️ nltk not available, fallback to simple split for English")[m
[32m+[m
 PROGRAMS_PATH = ROOT / "public/data/ucl_programs.json"[m
 SERVICES_PATH = ROOT / "public/data/ucl_services.json"[m
 [m
[36m@@ -47,18 +69,33 @@[m [mdef detect_language(text: str) -> str:[m
     return "en"[m
 [m
 # ============ 文档加载 ============[m
[31m-def _load_documents() -> List[Dict]:[m
[32m+[m[32m@lru_cache(maxsize=1)[m
[32m+[m[32mdef _load_documents() -> Dict[str, Any]:[m
     documents = [][m
[32m+[m[32m    errors = [][m
     for path, name in [(PROGRAMS_PATH, "programs"), (SERVICES_PATH, "services")]:[m
[31m-        if path.exists():[m
[31m-            try:[m
[31m-                with open(path, 'r', encoding='utf-8') as f:[m
[31m-                    data = json.load(f)[m
[31m-                    documents.extend(data)[m
[31m-                logger.info(f"✅ Loaded {len(data)} {name}")[m
[31m-            except Exception as e:[m
[31m-                logger.error(f"❌ Load {name} failed: {e}")[m
[31m-    return documents[m
[32m+[m[32m        if not path.exists():[m
[32m+[m[32m            err = f"File {path} does not exist."[m
[32m+[m[32m            errors.append(err)[m
[32m+[m[32m            logger.error(f"❌ {err}")[m
[32m+[m[32m            continue[m
[32m+[m[32m        if path.stat().st_size == 0:[m
[32m+[m[32m            err = f"File {path} is empty."[m
[32m+[m[32m            errors.append(err)[m
[32m+[m[32m            logger.error(f"❌ {err}")[m
[32m+[m[32m            continue[m
[32m+[m[32m        try:[m
[32m+[m[32m            with open(path, 'r', encoding='utf-8') as f:[m
[32m+[m[32m                data = json.load(f)[m
[32m+[m[32m                documents.extend(data)[m
[32m+[m[32m            logger.info(f"✅ Loaded {len(data)} {name}")[m
[32m+[m[32m        except Exception as e:[m
[32m+[m[32m            err = f"Load {name} failed: {e}"[m
[32m+[m[32m            errors.append(err)[m
[32m+[m[32m            logger.error(f"❌ {err}")[m
[32m+[m[32m    if errors:[m
[32m+[m[32m        return {"error": True, "message": "; ".join(errors), "documents": []}[m
[32m+[m[32m    return {"error": False, "documents": documents}[m
 [m
 # ============ 意图检测 ============[m
 def _detect_intent(q: str) -> str:[m
[36m@@ -75,33 +112,72 @@[m [mdef _detect_intent(q: str) -> str:[m
 [m
 # ============ 简单搜索（fallback）============[m
 def _simple_fallback_search(query: str, documents: List[Dict], top_k: int = 8) -> List[Dict]:[m
[32m+[m[32m    lang = detect_language(query)[m
     qlower = query.lower()[m
[32m+[m[41m    [m
[32m+[m[32m    # Tokenization[m
[32m+[m[32m    if lang == "zh" and HAVE_JIEBA:[m
[32m+[m[32m        tokens = list(jieba.cut(qlower))[m
[32m+[m[32m    elif lang == "en" and HAVE_NLTK:[m
[32m+[m[32m        tokens = [lemmatizer.lemmatize(w) for w in qlower.split() if len(w) > 2][m
[32m+[m[32m    else:[m
[32m+[m[32m        tokens = [w for w in qlower.split() if len(w) > 2][m
[32m+[m[41m    [m
[32m+[m[32m    intent = _detect_intent(query)[m
[32m+[m[32m    intent_weight = {[m
[32m+[m[32m        'requirements': 1.2,[m
[32m+[m[32m        'modules': 1.1,[m
[32m+[m[32m        'services': 1.1,[m
[32m+[m[32m        'fees': 1.2,[m
[32m+[m[32m        'general': 1.0[m
[32m+[m[32m    }.get(intent, 1.0)[m
[32m+[m[41m    [m
     results = [][m
     for doc in documents:[m
         text = ' '.join([doc.get('title','')] + [[m
             f"{s.get('heading','')} {s.get('text','')}" [m
             for s in doc.get('sections',[])[:5][m
         ]).lower()[m
[31m-        score = sum(text.count(w) * 2 for w in qlower.split() if len(w) > 2)[m
[32m+[m[41m        [m
[32m+[m[32m        matched_snippets = [][m
[32m+[m[32m        score = 0[m
[32m+[m[32m        for token in tokens:[m
[32m+[m[32m            if token in text:[m
[32m+[m[32m                score += text.count(token) * 2[m
[32m+[m[32m                # Find snippet[m
[32m+[m[32m                snippet_match = re.search(r'.{0,50}' + re.escape(token) + r'.{0,50}', text, re.IGNORECASE)[m
[32m+[m[32m                if snippet_match:[m
[32m+[m[32m                    matched_snippets.append(snippet_match.group(0))[m
[32m+[m[41m        [m
         if score > 0:[m
[32m+[m[32m            score *= intent_weight[m
             results.append({[m
                 'doc': doc, [m
                 'score': score, [m
[31m-                'intent': _detect_intent(query)[m
[32m+[m[32m                'intent': intent,[m
[32m+[m[32m                'matched_snippets': matched_snippets[:3]  # Top 3 snippets[m
             })[m
     results.sort(key=lambda x: x['score'], reverse=True)[m
     return results[:top_k][m
 [m
 # ============ 构建上下文 ============[m
 def _build_context_from_results(results: List[Dict]) -> str:[m
[32m+[m[32m    if not results:[m
[32m+[m[32m        return "No relevant information found in documents."[m
[32m+[m[41m    [m
     parts = [][m
     for r in results[:3]:[m
         doc = r.get('doc', {})[m
         title = doc.get('title', 'Unknown')[m
[31m-        for s in doc.get('sections', [])[:4]:[m
[32m+[m[32m        score = r.get('score', 0)[m
[32m+[m[41m        [m
[32m+[m[32m        # Prioritize matched_sections if available[m
[32m+[m[32m        sections = r.get('matched_sections', doc.get('sections', []))[:4][m
[32m+[m[41m        [m
[32m+[m[32m        for s in sections:[m
             h, t = s.get('heading', ''), s.get('text', '')[:900][m
             if t and len(t) > 50:[m
[31m-                parts.append(f"【{title} - {h}】\n{t}")[m
[32m+[m[32m                parts.append(f"【{title} - {h} (Score: {score:.2f})】\n{t}")[m
                 break[m
     return "\n\n".join(parts)[m
 [m
[36m@@ -149,6 +225,23 @@[m [mdef _generate_smart_answer_using_llm([m
     [m
     context = _build_context_from_results(results)[m
     [m
[32m+[m[32m    if not context or context == "No relevant information found in documents.":[m
[32m+[m[32m        logger.warning("⚠️ Context is empty, falling back to extract_key_info")[m
[32m+[m[32m        return _extract_key_info(results, language)[m
[32m+[m[41m    [m
[32m+[m[32m    # Attach matched_sections summary and URLs[m
[32m+[m[32m    matched_summary = [][m
[32m+[m[32m    for r in results[:3]:[m
[32m+[m[32m        doc = r.get('doc', {})[m
[32m+[m[32m        url = doc.get('url', '')[m
[32m+[m[32m        sections = r.get('matched_sections', doc.get('sections', []))[:2][m
[32m+[m[32m        for s in sections:[m
[32m+[m[32m            h = s.get('heading', '')[m
[32m+[m[32m            t = s.get('text', '')[:200][m
[32m+[m[32m            if t:[m
[32m+[m[32m                matched_summary.append(f"From {url} - {h}: {t}")[m
[32m+[m[32m    matched_str = "\n".join(matched_summary) if matched_summary else ""[m
[32m+[m[41m    [m
     if language == "zh":[m
         # 🔥 中文 Prompt - 极度强化[m
         system = """你是 UCL 信息助手。[m
[36m@@ -157,7 +250,7 @@[m [mdef _generate_smart_answer_using_llm([m
 1. 你必须用中文回答，绝对不要使用英文[m
 2. 从文档中提取准确信息[m
 3. 使用 • 符号列出要点[m
[31m-4. 保持简洁，少于150字[m
[32m+[m[32m4. 保持简洁，100-200字[m
 5. 如果文档是英文，翻译成中文后回答[m
 [m
 记住：你的回答必须全部是中文！"""[m
[36m@@ -165,6 +258,9 @@[m [mdef _generate_smart_answer_using_llm([m
         user = f"""文档内容：[m
 {context}[m
 [m
[32m+[m[32m匹配摘要：[m
[32m+[m[32m{matched_str}[m
[32m+[m
 用户问题：{query}[m
 [m
 请用中文回答，不要使用任何英文。"""[m
[36m@@ -177,15 +273,18 @@[m [mRules:[m
 - Extract accurate information from documents[m
 - Answer in English only[m
 - Use • for bullet points[m
[31m-- Keep under 150 words[m
[32m+[m[32m- Keep 100-200 words[m
 - Be specific and factual"""[m
         [m
         user = f"""Documents:[m
 {context}[m
 [m
[32m+[m[32mMatched Summary:[m
[32m+[m[32m{matched_str}[m
[32m+[m
 User Question: {query}[m
 [m
[31m-Answer in English."""[m
[32m+[m[32mAnswer in English with bullet points."""[m
     [m
     messages = [[m
         {"role": "system", "content": system},[m
[36m@@ -195,13 +294,19 @@[m [mAnswer in English."""[m
     try:[m
         if groq_configured():[m
             logger.info(f"🤖 调用 LLM (language={language})...")[m
[31m-            ans = chat_with_groq(messages, temperature=0.1)[m
[32m+[m[32m            ans = chat_with_groq(messages, temperature=0.3)  # 🔥 提高温度以获得更多创意[m
             [m
[31m-            # 🔥 验证语言是否正确[m
[32m+[m[32m            # 🔥 简化验证 - 仅检查必要条件[m
             if language == "zh":[m
                 chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', ans))[m
                 if chinese_chars < 10:  # 中文字符太少[m
[31m-                    logger.warning("⚠️  LLM 返回了英文，使用 fallback")[m
[32m+[m[32m                    logger.warning("⚠️ LLM 返回了英文，使用 fallback")[m
[32m+[m[32m                    return _extract_key_info(results, language)[m
[32m+[m[41m            [m
[32m+[m[32m            # 🔥 英文只检查长度[m
[32m+[m[32m            if language == "en":[m
[32m+[m[32m                if len(ans) < 30:[m
[32m+[m[32m                    logger.warning("⚠️ English answer too short, using fallback")[m
                     return _extract_key_info(results, language)[m
             [m
             if ans and len(ans) > 30:[m
[36m@@ -212,7 +317,7 @@[m [mAnswer in English."""[m
         logger.error(f"❌ LLM 调用失败: {e}")[m
     [m
     # Fallback[m
[31m-    logger.info("⚠️  使用 fallback 提取")[m
[32m+[m[32m    logger.info("⚠️ 使用 fallback 提取")[m
     return _extract_key_info(results, language)[m
 [m
 # ============ 🔥 主函数 ============[m
[36m@@ -235,28 +340,32 @@[m [mdef answer_enhanced([m
     logger.info(f"{'='*60}")[m
     [m
     # 加载文档[m
[31m-    docs = _load_documents()[m
[31m-    if not docs:[m
[32m+[m[32m    load_result = _load_documents()[m
[32m+[m[32m    if load_result["error"]:[m
[32m+[m[32m        err_msg = "数据加载失败，请检查文件。" if language == "zh" else "Data loading failed, please check files."[m
         return {[m
             "intent": "error",[m
[31m-            "answer": "数据未加载" if language == "zh" else "Data not loaded",[m
[32m+[m[32m            "answer": err_msg,[m
             "citations": [],[m
             "rewritten_queries": [],[m
             "reranked": [],[m
             "response_time": f"{time.time()-start:.2f}s"[m
         }[m
[32m+[m[32m    docs = load_result["documents"][m
     [m
     # 检索[m
     search_results = [][m
[32m+[m[32m    intent = "unknown"[m
     if HAVE_RETRIEVER:[m
         try:[m
[31m-            retriever = EnhancedRetriever()[m
             raw = retriever.search_with_context(query, docs, top_k)[m
[32m+[m[32m            intent = retriever.detect_intent(query)  # 复用检索器的detect_intent，如果存在[m
             search_results = [[m
                 {[m
                     'doc': r.get('doc', r),[m
                     'score': r.get('score', 0),[m
[31m-                    'intent': _detect_intent(query)[m
[32m+[m[32m                    'intent': intent,[m
[32m+[m[32m                    'matched_sections': r.get('matched_sections', [])  # 假设检索器返回此字段[m
                 } [m
                 for r in raw[m
             ][m
[36m@@ -267,6 +376,18 @@[m [mdef answer_enhanced([m
     if not search_results:[m
         logger.info("⚠️  使用 fallback 搜索")[m
         search_results = _simple_fallback_search(query, docs, top_k)[m
[32m+[m[32m        intent = search_results[0].get('intent', 'general') if search_results else "unknown"[m
[32m+[m[41m    [m
[32m+[m[32m    if not search_results:[m
[32m+[m[32m        err_msg = "抱歉，未找到任何相关信息。请尝试其他关键词。" if language == "zh" else "Sorry, no relevant information found. Try other keywords."[m
[32m+[m[32m        return {[m
[32m+[m[32m            "intent": intent,[m
[32m+[m[32m            "answer": err_msg,[m
[32m+[m[32m            "citations": [],[m
[32m+[m[32m            "rewritten_queries": [],  # 🔥 前端需要[m
[32m+[m[32m            "reranked": [],[m
[32m+[m[32m            "response_time": f"{time.time()-start:.2f}s"[m
[32m+[m[32m        }[m
     [m
     # 生成答案[m
     answer = _generate_smart_answer_using_llm(query, search_results, language)[m
[36m@@ -285,7 +406,7 @@[m [mdef answer_enhanced([m
     logger.info(f"✅ 完成: {rt}")[m
     [m
     return {[m
[31m-        "intent": search_results[0].get('intent', 'general') if search_results else "unknown",[m
[32m+[m[32m        "intent": intent,[m
         "answer": answer,[m
         "citations": citations,[m
         "rewritten_queries": [],  # 🔥 前端需要[m
