[1mdiff --git a/scripts/qa_enhanced_wrapper.py b/scripts/qa_enhanced_wrapper.py[m
[1mindex 6cf40e1..bf72f74 100644[m
[1m--- a/scripts/qa_enhanced_wrapper.py[m
[1m+++ b/scripts/qa_enhanced_wrapper.py[m
[36m@@ -5,6 +5,7 @@[m
 import os, sys, json, time, logging, re[m
 from pathlib import Path[m
 from typing import List, Dict, Any[m
[32m+[m[32mfrom functools import lru_cache[m
 [m
 ROOT = Path(__file__).resolve().parents[1][m
 sys.path.insert(0, str(ROOT))[m
[36m@@ -31,10 +32,31 @@[m [mtry:[m
     from scripts.enhanced_retriever import EnhancedRetriever[m
     HAVE_RETRIEVER = True[m
     logger.info("âœ… Enhanced retriever loaded")[m
[32m+[m[32m    # æ¨¡å—çº§æ£€ç´¢å™¨å®ä¾‹[m
[32m+[m[32m    retriever = EnhancedRetriever()[m
 except Exception as e:[m
     HAVE_RETRIEVER = False[m
     logger.warning(f"âš ï¸  Retriever not available: {e}")[m
 [m
[32m+[m[32m# ============ å¯¼å…¥åˆ†è¯å·¥å…· ============[m
[32m+[m[32mtry:[m
[32m+[m[32m    import jieba[m
[32m+[m[32m    HAVE_JIEBA = True[m
[32m+[m[32mexcept ImportError:[m
[32m+[m[32m    HAVE_JIEBA = False[m
[32m+[m[32m    logger.warning("âš ï¸ jieba not available, fallback to simple split for Chinese")[m
[32m+[m
[32m+[m[32mtry:[m
[32m+[m[32m    import nltk[m
[32m+[m[32m    from nltk.stem import WordNetLemmatizer[m
[32m+[m[32m    nltk.download('wordnet', quiet=True)[m
[32m+[m[32m    nltk.download('omw-1.4', quiet=True)[m
[32m+[m[32m    lemmatizer = WordNetLemmatizer()[m
[32m+[m[32m    HAVE_NLTK = True[m
[32m+[m[32mexcept ImportError:[m
[32m+[m[32m    HAVE_NLTK = False[m
[32m+[m[32m    logger.warning("âš ï¸ nltk not available, fallback to simple split for English")[m
[32m+[m
 PROGRAMS_PATH = ROOT / "public/data/ucl_programs.json"[m
 SERVICES_PATH = ROOT / "public/data/ucl_services.json"[m
 [m
[36m@@ -47,18 +69,33 @@[m [mdef detect_language(text: str) -> str:[m
     return "en"[m
 [m
 # ============ æ–‡æ¡£åŠ è½½ ============[m
[31m-def _load_documents() -> List[Dict]:[m
[32m+[m[32m@lru_cache(maxsize=1)[m
[32m+[m[32mdef _load_documents() -> Dict[str, Any]:[m
     documents = [][m
[32m+[m[32m    errors = [][m
     for path, name in [(PROGRAMS_PATH, "programs"), (SERVICES_PATH, "services")]:[m
[31m-        if path.exists():[m
[31m-            try:[m
[31m-                with open(path, 'r', encoding='utf-8') as f:[m
[31m-                    data = json.load(f)[m
[31m-                    documents.extend(data)[m
[31m-                logger.info(f"âœ… Loaded {len(data)} {name}")[m
[31m-            except Exception as e:[m
[31m-                logger.error(f"âŒ Load {name} failed: {e}")[m
[31m-    return documents[m
[32m+[m[32m        if not path.exists():[m
[32m+[m[32m            err = f"File {path} does not exist."[m
[32m+[m[32m            errors.append(err)[m
[32m+[m[32m            logger.error(f"âŒ {err}")[m
[32m+[m[32m            continue[m
[32m+[m[32m        if path.stat().st_size == 0:[m
[32m+[m[32m            err = f"File {path} is empty."[m
[32m+[m[32m            errors.append(err)[m
[32m+[m[32m            logger.error(f"âŒ {err}")[m
[32m+[m[32m            continue[m
[32m+[m[32m        try:[m
[32m+[m[32m            with open(path, 'r', encoding='utf-8') as f:[m
[32m+[m[32m                data = json.load(f)[m
[32m+[m[32m                documents.extend(data)[m
[32m+[m[32m            logger.info(f"âœ… Loaded {len(data)} {name}")[m
[32m+[m[32m        except Exception as e:[m
[32m+[m[32m            err = f"Load {name} failed: {e}"[m
[32m+[m[32m            errors.append(err)[m
[32m+[m[32m            logger.error(f"âŒ {err}")[m
[32m+[m[32m    if errors:[m
[32m+[m[32m        return {"error": True, "message": "; ".join(errors), "documents": []}[m
[32m+[m[32m    return {"error": False, "documents": documents}[m
 [m
 # ============ æ„å›¾æ£€æµ‹ ============[m
 def _detect_intent(q: str) -> str:[m
[36m@@ -75,33 +112,72 @@[m [mdef _detect_intent(q: str) -> str:[m
 [m
 # ============ ç®€å•æœç´¢ï¼ˆfallbackï¼‰============[m
 def _simple_fallback_search(query: str, documents: List[Dict], top_k: int = 8) -> List[Dict]:[m
[32m+[m[32m    lang = detect_language(query)[m
     qlower = query.lower()[m
[32m+[m[41m    [m
[32m+[m[32m    # Tokenization[m
[32m+[m[32m    if lang == "zh" and HAVE_JIEBA:[m
[32m+[m[32m        tokens = list(jieba.cut(qlower))[m
[32m+[m[32m    elif lang == "en" and HAVE_NLTK:[m
[32m+[m[32m        tokens = [lemmatizer.lemmatize(w) for w in qlower.split() if len(w) > 2][m
[32m+[m[32m    else:[m
[32m+[m[32m        tokens = [w for w in qlower.split() if len(w) > 2][m
[32m+[m[41m    [m
[32m+[m[32m    intent = _detect_intent(query)[m
[32m+[m[32m    intent_weight = {[m
[32m+[m[32m        'requirements': 1.2,[m
[32m+[m[32m        'modules': 1.1,[m
[32m+[m[32m        'services': 1.1,[m
[32m+[m[32m        'fees': 1.2,[m
[32m+[m[32m        'general': 1.0[m
[32m+[m[32m    }.get(intent, 1.0)[m
[32m+[m[41m    [m
     results = [][m
     for doc in documents:[m
         text = ' '.join([doc.get('title','')] + [[m
             f"{s.get('heading','')} {s.get('text','')}" [m
             for s in doc.get('sections',[])[:5][m
         ]).lower()[m
[31m-        score = sum(text.count(w) * 2 for w in qlower.split() if len(w) > 2)[m
[32m+[m[41m        [m
[32m+[m[32m        matched_snippets = [][m
[32m+[m[32m        score = 0[m
[32m+[m[32m        for token in tokens:[m
[32m+[m[32m            if token in text:[m
[32m+[m[32m                score += text.count(token) * 2[m
[32m+[m[32m                # Find snippet[m
[32m+[m[32m                snippet_match = re.search(r'.{0,50}' + re.escape(token) + r'.{0,50}', text, re.IGNORECASE)[m
[32m+[m[32m                if snippet_match:[m
[32m+[m[32m                    matched_snippets.append(snippet_match.group(0))[m
[32m+[m[41m        [m
         if score > 0:[m
[32m+[m[32m            score *= intent_weight[m
             results.append({[m
                 'doc': doc, [m
                 'score': score, [m
[31m-                'intent': _detect_intent(query)[m
[32m+[m[32m                'intent': intent,[m
[32m+[m[32m                'matched_snippets': matched_snippets[:3]  # Top 3 snippets[m
             })[m
     results.sort(key=lambda x: x['score'], reverse=True)[m
     return results[:top_k][m
 [m
 # ============ æ„å»ºä¸Šä¸‹æ–‡ ============[m
 def _build_context_from_results(results: List[Dict]) -> str:[m
[32m+[m[32m    if not results:[m
[32m+[m[32m        return "No relevant information found in documents."[m
[32m+[m[41m    [m
     parts = [][m
     for r in results[:3]:[m
         doc = r.get('doc', {})[m
         title = doc.get('title', 'Unknown')[m
[31m-        for s in doc.get('sections', [])[:4]:[m
[32m+[m[32m        score = r.get('score', 0)[m
[32m+[m[41m        [m
[32m+[m[32m        # Prioritize matched_sections if available[m
[32m+[m[32m        sections = r.get('matched_sections', doc.get('sections', []))[:4][m
[32m+[m[41m        [m
[32m+[m[32m        for s in sections:[m
             h, t = s.get('heading', ''), s.get('text', '')[:900][m
             if t and len(t) > 50:[m
[31m-                parts.append(f"ã€{title} - {h}ã€‘\n{t}")[m
[32m+[m[32m                parts.append(f"ã€{title} - {h} (Score: {score:.2f})ã€‘\n{t}")[m
                 break[m
     return "\n\n".join(parts)[m
 [m
[36m@@ -149,6 +225,23 @@[m [mdef _generate_smart_answer_using_llm([m
     [m
     context = _build_context_from_results(results)[m
     [m
[32m+[m[32m    if not context or context == "No relevant information found in documents.":[m
[32m+[m[32m        logger.warning("âš ï¸ Context is empty, falling back to extract_key_info")[m
[32m+[m[32m        return _extract_key_info(results, language)[m
[32m+[m[41m    [m
[32m+[m[32m    # Attach matched_sections summary and URLs[m
[32m+[m[32m    matched_summary = [][m
[32m+[m[32m    for r in results[:3]:[m
[32m+[m[32m        doc = r.get('doc', {})[m
[32m+[m[32m        url = doc.get('url', '')[m
[32m+[m[32m        sections = r.get('matched_sections', doc.get('sections', []))[:2][m
[32m+[m[32m        for s in sections:[m
[32m+[m[32m            h = s.get('heading', '')[m
[32m+[m[32m            t = s.get('text', '')[:200][m
[32m+[m[32m            if t:[m
[32m+[m[32m                matched_summary.append(f"From {url} - {h}: {t}")[m
[32m+[m[32m    matched_str = "\n".join(matched_summary) if matched_summary else ""[m
[32m+[m[41m    [m
     if language == "zh":[m
         # ğŸ”¥ ä¸­æ–‡ Prompt - æåº¦å¼ºåŒ–[m
         system = """ä½ æ˜¯ UCL ä¿¡æ¯åŠ©æ‰‹ã€‚[m
[36m@@ -157,7 +250,7 @@[m [mdef _generate_smart_answer_using_llm([m
 1. ä½ å¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼Œç»å¯¹ä¸è¦ä½¿ç”¨è‹±æ–‡[m
 2. ä»æ–‡æ¡£ä¸­æå–å‡†ç¡®ä¿¡æ¯[m
 3. ä½¿ç”¨ â€¢ ç¬¦å·åˆ—å‡ºè¦ç‚¹[m
[31m-4. ä¿æŒç®€æ´ï¼Œå°‘äº150å­—[m
[32m+[m[32m4. ä¿æŒç®€æ´ï¼Œ100-200å­—[m
 5. å¦‚æœæ–‡æ¡£æ˜¯è‹±æ–‡ï¼Œç¿»è¯‘æˆä¸­æ–‡åå›ç­”[m
 [m
 è®°ä½ï¼šä½ çš„å›ç­”å¿…é¡»å…¨éƒ¨æ˜¯ä¸­æ–‡ï¼"""[m
[36m@@ -165,6 +258,9 @@[m [mdef _generate_smart_answer_using_llm([m
         user = f"""æ–‡æ¡£å†…å®¹ï¼š[m
 {context}[m
 [m
[32m+[m[32måŒ¹é…æ‘˜è¦ï¼š[m
[32m+[m[32m{matched_str}[m
[32m+[m
 ç”¨æˆ·é—®é¢˜ï¼š{query}[m
 [m
 è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•è‹±æ–‡ã€‚"""[m
[36m@@ -177,15 +273,18 @@[m [mRules:[m
 - Extract accurate information from documents[m
 - Answer in English only[m
 - Use â€¢ for bullet points[m
[31m-- Keep under 150 words[m
[32m+[m[32m- Keep 100-200 words[m
 - Be specific and factual"""[m
         [m
         user = f"""Documents:[m
 {context}[m
 [m
[32m+[m[32mMatched Summary:[m
[32m+[m[32m{matched_str}[m
[32m+[m
 User Question: {query}[m
 [m
[31m-Answer in English."""[m
[32m+[m[32mAnswer in English with bullet points."""[m
     [m
     messages = [[m
         {"role": "system", "content": system},[m
[36m@@ -195,13 +294,19 @@[m [mAnswer in English."""[m
     try:[m
         if groq_configured():[m
             logger.info(f"ğŸ¤– è°ƒç”¨ LLM (language={language})...")[m
[31m-            ans = chat_with_groq(messages, temperature=0.1)[m
[32m+[m[32m            ans = chat_with_groq(messages, temperature=0.3)  # ğŸ”¥ æé«˜æ¸©åº¦ä»¥è·å¾—æ›´å¤šåˆ›æ„[m
             [m
[31m-            # ğŸ”¥ éªŒè¯è¯­è¨€æ˜¯å¦æ­£ç¡®[m
[32m+[m[32m            # ğŸ”¥ ç®€åŒ–éªŒè¯ - ä»…æ£€æŸ¥å¿…è¦æ¡ä»¶[m
             if language == "zh":[m
                 chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', ans))[m
                 if chinese_chars < 10:  # ä¸­æ–‡å­—ç¬¦å¤ªå°‘[m
[31m-                    logger.warning("âš ï¸  LLM è¿”å›äº†è‹±æ–‡ï¼Œä½¿ç”¨ fallback")[m
[32m+[m[32m                    logger.warning("âš ï¸ LLM è¿”å›äº†è‹±æ–‡ï¼Œä½¿ç”¨ fallback")[m
[32m+[m[32m                    return _extract_key_info(results, language)[m
[32m+[m[41m            [m
[32m+[m[32m            # ğŸ”¥ è‹±æ–‡åªæ£€æŸ¥é•¿åº¦[m
[32m+[m[32m            if language == "en":[m
[32m+[m[32m                if len(ans) < 30:[m
[32m+[m[32m                    logger.warning("âš ï¸ English answer too short, using fallback")[m
                     return _extract_key_info(results, language)[m
             [m
             if ans and len(ans) > 30:[m
[36m@@ -212,7 +317,7 @@[m [mAnswer in English."""[m
         logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")[m
     [m
     # Fallback[m
[31m-    logger.info("âš ï¸  ä½¿ç”¨ fallback æå–")[m
[32m+[m[32m    logger.info("âš ï¸ ä½¿ç”¨ fallback æå–")[m
     return _extract_key_info(results, language)[m
 [m
 # ============ ğŸ”¥ ä¸»å‡½æ•° ============[m
[36m@@ -235,28 +340,32 @@[m [mdef answer_enhanced([m
     logger.info(f"{'='*60}")[m
     [m
     # åŠ è½½æ–‡æ¡£[m
[31m-    docs = _load_documents()[m
[31m-    if not docs:[m
[32m+[m[32m    load_result = _load_documents()[m
[32m+[m[32m    if load_result["error"]:[m
[32m+[m[32m        err_msg = "æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ã€‚" if language == "zh" else "Data loading failed, please check files."[m
         return {[m
             "intent": "error",[m
[31m-            "answer": "æ•°æ®æœªåŠ è½½" if language == "zh" else "Data not loaded",[m
[32m+[m[32m            "answer": err_msg,[m
             "citations": [],[m
             "rewritten_queries": [],[m
             "reranked": [],[m
             "response_time": f"{time.time()-start:.2f}s"[m
         }[m
[32m+[m[32m    docs = load_result["documents"][m
     [m
     # æ£€ç´¢[m
     search_results = [][m
[32m+[m[32m    intent = "unknown"[m
     if HAVE_RETRIEVER:[m
         try:[m
[31m-            retriever = EnhancedRetriever()[m
             raw = retriever.search_with_context(query, docs, top_k)[m
[32m+[m[32m            intent = retriever.detect_intent(query)  # å¤ç”¨æ£€ç´¢å™¨çš„detect_intentï¼Œå¦‚æœå­˜åœ¨[m
             search_results = [[m
                 {[m
                     'doc': r.get('doc', r),[m
                     'score': r.get('score', 0),[m
[31m-                    'intent': _detect_intent(query)[m
[32m+[m[32m                    'intent': intent,[m
[32m+[m[32m                    'matched_sections': r.get('matched_sections', [])  # å‡è®¾æ£€ç´¢å™¨è¿”å›æ­¤å­—æ®µ[m
                 } [m
                 for r in raw[m
             ][m
[36m@@ -267,6 +376,18 @@[m [mdef answer_enhanced([m
     if not search_results:[m
         logger.info("âš ï¸  ä½¿ç”¨ fallback æœç´¢")[m
         search_results = _simple_fallback_search(query, docs, top_k)[m
[32m+[m[32m        intent = search_results[0].get('intent', 'general') if search_results else "unknown"[m
[32m+[m[41m    [m
[32m+[m[32m    if not search_results:[m
[32m+[m[32m        err_msg = "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ä»»ä½•ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•å…¶ä»–å…³é”®è¯ã€‚" if language == "zh" else "Sorry, no relevant information found. Try other keywords."[m
[32m+[m[32m        return {[m
[32m+[m[32m            "intent": intent,[m
[32m+[m[32m            "answer": err_msg,[m
[32m+[m[32m            "citations": [],[m
[32m+[m[32m            "rewritten_queries": [],  # ğŸ”¥ å‰ç«¯éœ€è¦[m
[32m+[m[32m            "reranked": [],[m
[32m+[m[32m            "response_time": f"{time.time()-start:.2f}s"[m
[32m+[m[32m        }[m
     [m
     # ç”Ÿæˆç­”æ¡ˆ[m
     answer = _generate_smart_answer_using_llm(query, search_results, language)[m
[36m@@ -285,7 +406,7 @@[m [mdef answer_enhanced([m
     logger.info(f"âœ… å®Œæˆ: {rt}")[m
     [m
     return {[m
[31m-        "intent": search_results[0].get('intent', 'general') if search_results else "unknown",[m
[32m+[m[32m        "intent": intent,[m
         "answer": answer,[m
         "citations": citations,[m
         "rewritten_queries": [],  # ğŸ”¥ å‰ç«¯éœ€è¦[m
